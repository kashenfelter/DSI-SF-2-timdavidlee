{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1: LINEAR REGRESSION --- DIRECTLY SOLVE\n",
    "# WEEK 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### Simple linear regression equation\n",
    "\n",
    "Our equation for the predicted values of our target variable is:\n",
    "\n",
    "### $$\\hat{y} = \\beta_0 + \\beta_1*x$$\n",
    "\n",
    "$\\hat{y}$ is the commonly used notation for the _predicted_ value of $y$.\n",
    "\n",
    "(The $\\epsilon$ error term is gone - it is an unknowable variable (if we knew what it was, we could perfectly model our target variable).\n",
    "\n",
    "Write a function that will calculate a predicted $y$ ($\\hat{y}$) from your $x$ variable and $\\beta$ coefficients. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Residuals\n",
    "\n",
    "The definition of \"residuals\" in linear regression is:\n",
    "\n",
    "### $$ residual = y - \\hat{y}$$\n",
    "\n",
    "Where $y$ is the true value of our target at this observation, and $\\hat{y}$ is the predicted value of our target. Simple enough. \n",
    "\n",
    "#### Write a function to calculate residuals below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Solving directly -- >Minimizing the sum of squared errors\n",
    "\n",
    "Deriving the equation that minimizes the sum of squared errors in simple linear regression can be done using calculus. [See here](http://web.cocc.edu/srule/MTH244/other/LRJ.PDF) or [here](https://en.wikipedia.org/wiki/Simple_linear_regression) for descriptions of the derivation.\n",
    "\n",
    "Skipping the partial derivitaves, the formulas for the $\\beta_0$ and $\\beta_1$ that minimize the sum of squares are:\n",
    "\n",
    "### $$ \\beta_1 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y} ) (x_i - \\bar{x} )}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n",
    "\n",
    "and\n",
    "\n",
    "### $$ \\beta_0 = \\bar{y} - \\beta_1\\bar{x} $$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the mean of $x$ and $y$, respectively.\n",
    "\n",
    "#### Write functions below to calculate $\\beta_0$ and $\\beta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\beta_1$ is in fact also equivalent to:\n",
    "\n",
    "### $$ \\beta_1 = \\frac{cov(x, y)}{var(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Multiple linear regression\n",
    "\n",
    "It is of course rare in regression that you will only want to predict $y$ from a single predictor $x$. Multiple linear regression predicts $y$ from more than 1 $x$ variable.\n",
    "\n",
    "The formula for computing the $\\beta$ values in multiple regression is best done with linear algebra. I'm not going to go into the details of the _reason_ that this works, but if you want to see the explanation [these slides are a great resource](http://statweb.stanford.edu/~nzhang/191_web/lecture4_handout.pdf).\n",
    "\n",
    "The linear algebra formula is as follows, where $X$ is a _matrix_ of predictors $x_1$ through $x_i$ (with each column a predictor), and $y$ are the true values of $y$. There is still only 1 predicted variable:\n",
    "\n",
    "### $$ \\hat{y} = \\beta X $$\n",
    "\n",
    "_The intercept term is part of $X$! It is a column of all ones added to the columns of predictors._\n",
    "\n",
    "Written out more simply, without the linear algebra, $\\hat{y}$ is calculated:\n",
    "\n",
    "### $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n",
    "\n",
    "The calculation of the $\\beta$ values is done with the linear algebra matrix multiplication formula:\n",
    "\n",
    "### $$ \\beta = (X'X)^{-1}X'Y $$\n",
    "\n",
    "Where $X'$ is the _transposed matrix of original matrix $X$_ and $(X'X)^-1$ is the _inverted matrix_ of $X'X$.\n",
    "\n",
    "Don't worry about the linear algebra here if you don't have experience with it. It's not so important to understand. Just remember that $y$ is estimated with an intercept term and each $x$ predictor multiplied by it's own $\\beta$ value.\n",
    "\n",
    "#### Create a \"design matrix\" with the first column a column of all 1s (intercept column) and the other columns the variables in the dataset that are not the target variable\n",
    "\n",
    "This is easiest to do with pandas: add a column for the intercept first, then extract the matrix with `.values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Modeling - this is where you decide which model to use\n",
    "# The parameters found in GridSearch can be used for regression analysis\n",
    "#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assign lm as the LinearRegression function\n",
    "lm = LinearRegression()\n",
    "# Fits trainX and trainY with the lm model and assigns to a variable\n",
    "model = lm.fit(trainX, trainY)\n",
    "# Returns Predicted Y values\n",
    "predictions = model.predict(testX)\n",
    "# Plots your True Y with Predicted Y\n",
    "plt.scatter(testY, predictions)\n",
    "# R-squared\n",
    "score = model.score(testX, testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: REGULARIZATION\n",
    "\n",
    "The concept of regularization is adding an additional \"penalty\" on the size of coefficients to the minimization of sum of squared errors in standard regression.\n",
    "\n",
    "In other words, there are additional components to the loss function, so the minimization becomes a balance between these components. \n",
    "\n",
    "The two most common types of regularization are the **\"Lasso\"**, **\"Ridge\"**, and the **Elastic Net**. We will be examining the math behind how they work and the effect they have on model fits.\n",
    "\n",
    "---\n",
    "\n",
    "### Refresher: the least squares loss function\n",
    "\n",
    "You've become familiar at this point with the least squares loss function. Vanilla regression minimizes the residual sum of squares (RSS) to fit the data:\n",
    "\n",
    "### $$ \\text{minimize}\\; RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i) = \\sum_{i=1}^n \\left(y_i - (\\beta_0 + \\beta_i x_i)\\right) $$\n",
    "\n",
    "Where our model predictions for $y$ are based on the sum of the $beta_0$ intercept and the products of $\\beta_i$ with $x_i$.\n",
    "\n",
    "---\n",
    "\n",
    "### Ridge regression\n",
    "\n",
    "The Ridge regression adds an additional thing to the loss function: the sum of the squared (non-intercept!) $\\beta$ values:\n",
    "\n",
    "### $$ \\text{minimize}\\; RSS = \\sum_{i=1}^n \\left(y_i - (\\beta_0 + \\beta_i x_i)\\right) + \\lambda_2\\sum_{i=1}^n \\beta_i^2$$\n",
    "\n",
    "What are these new components?\n",
    "\n",
    "$\\beta_i^2$ is the squared coefficient for variable $x_i$.\n",
    "\n",
    "$\\sum_{i=1}^n \\beta_i^2$ is the sum of these squared coefficients for every variable we have in our model. This does **not** include the intercept $\\beta_0$.\n",
    "\n",
    "$\\lambda_2$ is a constant for the _strength_ of the regularization parameter. The higher this value, the greater the impact of this new component in the loss function. If this were zero, then we would revert back to just the least squares loss function. If this were, say, a billion, then the residual sum of squares component would have a much smaller effect on the loss/cost than the regularization term.\n",
    "\n",
    "---\n",
    "\n",
    "### Lasso regression\n",
    "\n",
    "The Lasso regression takes a different approach. Instead of adding the sum of _squared_ $\\beta$ coefficients to the RSS, it adds the sum of the _absolute value_ of the $\\beta$ coefficients:\n",
    "\n",
    "### $$ \\text{minimize}\\; RSS = \\sum_{i=1}^n \\left(y_i - (\\beta_0 + \\beta_i x_i)\\right) + \\lambda_1\\sum_{i=1}^n |\\beta_i|$$\n",
    "\n",
    "$|\\beta_i|$ is the absolute value of the $\\beta$ coefficient for variable $x_i$\n",
    "\n",
    "$\\lambda_1$ is again the strength of the regularization penalty component in the loss function. In lasso the lambda is denoted with a 1, in ridge the lambda is denoted with a 2. \n",
    "\n",
    "---\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "Elastic Net is a combination of both the Lasso and the Ridge regularizations. It adds both penalties to the loss function:\n",
    "\n",
    "### $$ \\text{minimize}\\; RSS = \\sum_{i=1}^n \\left(y_i - (\\beta_0 + \\beta_i x_i)\\right) + \\lambda_1\\sum_{i=1}^n |\\beta_i| + \\lambda_2\\sum_{i=1}^n \\beta_i^2$$\n",
    "\n",
    "In the elastic net, the effect of the Ridge vs. the Lasso is balanced by the two lambda parameters. \n",
    "\n",
    "---\n",
    "\n",
    "### So when do you use each? What is the effect of regularization?\n",
    "\n",
    "This is what we will investigate in this lesson. We will be using a dataset on wine quality.\n",
    "\n",
    "The important aspect of this data, which is a reason why we might choose to use regularization, is that there is **multicollinearity** in the data.\n",
    "\n",
    "The term multicollinearity means that there are high correlations between predictor variables in your model. This is a problem because the regression has no good way to distinguish between the effect of either on the target variable. You can end up with meaningless or absurd coefficients if you don't address this problem.\n",
    "\n",
    "The Lasso and Elastic Net are also very useful for when you have redundant or unimportant variables. If you have 1000 variables in a dataset the Lasso can perform \"feature selection\" automatically for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Normalize the predictor columns\n",
    "\n",
    "With the Lasso and Ridge it is neccessary to normalize the predictor columns before constructing the models, even the dummy coded categorical variables. \n",
    "\n",
    "Below we define our target variable and then normalize the columns that are not the target.\n",
    "\n",
    "### Why is normalization of predictors required?\n",
    "\n",
    "Recall the equations for the Ridge and Lasso penalties:\n",
    "\n",
    "### $$ \\text{Ridge penalty}\\; = \\lambda_2\\sum_{i=1}^n \\beta_i^2$$\n",
    "\n",
    "### $$ \\text{Lasso penalty}\\; = \\lambda_2\\sum_{i=1}^n |\\beta_i|$$\n",
    "\n",
    "**How are the $\\beta$ coefficients affected by the mean and variance of your variables?**\n",
    "\n",
    "If the mean and variance of your $x$ predictors are different, their respective $\\beta$ coefficients _scale with the mean and variance of the predictors **regardless of their explanatory power.**_\n",
    "\n",
    "This means that if one of your $x$ variables, for example the price of a home, will have a much smaller $\\beta$ value than say the number of bedrooms in a house – just because the scale of the two variables are so different.\n",
    "\n",
    "The Ridge and Lasso penalties are agnostic to the mean and variance of your predictors. All they \"care about\" are the values of the coefficients. If one of your coefficients is much larger than any of the others, it will dominate the effect of the penalty on your minimization!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-11-e09d5d6fe548>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-e09d5d6fe548>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    (alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "\n",
    "\n",
    "# Lasso (Complete with GridSearch)\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n",
    "parameters\n",
    "\t(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, \n",
    "\tmax_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "parameters\n",
    "\t(eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute='auto', \n",
    "\tmax_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, positive=False, \n",
    "\trandom_state=None, selection='cyclic')\n",
    "\n",
    "\n",
    "#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "\n",
    "\n",
    "# Ridge (Complete with GridSearch)\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge()\n",
    "parameters\n",
    "\t(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, \n",
    "\ttol=0.001, solver='auto', random_state=None\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "parameters\n",
    "\t(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, \n",
    "\tcv=None, gcv_mode=None, store_cv_values=False)\n",
    "\n",
    "\n",
    "#<><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><><>\n",
    "\n",
    "\n",
    "# Elastic Net (Complete with GridSearch)\n",
    "from sklearn.linear_model import ElasticNet\n",
    "en = ElasticNet()\n",
    "parameters\n",
    "\t(alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, \n",
    "\tcopy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "parameters\t\n",
    "\t(l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, \n",
    "\tprecompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=1, \n",
    "\tpositive=False, random_state=None, selection='cyclic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3: K NEAREST NEIGHBORS CLASSIFICATION \n",
    "# Week 4: (1.1)\n",
    "\n",
    "In this notebook we are going to look at how the kNN algorithm classifies malignant vs. benign tumor category in the Wisconsin breast cancer dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## kNN\n",
    "\n",
    "The pseudocode algorithm for kNN is as follows:\n",
    "\n",
    "```\n",
    "for unclassified_point in sample:\n",
    "    for known_point in known_class_points:\n",
    "        calculate distances (euclidean or other) between known_point and unclassified_point\n",
    "    for k in range of specified_neighbors_number:\n",
    "        find k_nearest_points in known_class_points to unclassified_point\n",
    "    assign class to unclassified_point using \"votes\" from k_nearest_points\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Assign knn as a KNeighborsClassifier function\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform') # uniform or distance\n",
    "# Fits trainX and trainY with the knn model and assigns to a variable\n",
    "model = knn.fit(trainX, trainY)\n",
    "# ?\n",
    "predictions = model.predict(testX)\n",
    "# ?\n",
    "score = model.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4: LOGISTIC REGRESSION\n",
    "# Week 4 (2.1)\n",
    "\n",
    "Logistic regression is arguably the most famous and well used classifier. It _is_ a regression, but don't let that confuse you: it estimates probabilities of class membership.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. A (brief) review of regression models\n",
    "\n",
    "To understand how logistic regression works, we need to understand least squares regression. \n",
    "\n",
    "As you are all familiar with, a regression with variable(s) matrix $X$ predicting target $y$ is formulated as:\n",
    "\n",
    "### $$E(y|X) = \\beta_0 + \\sum_{j}^p\\beta_jx_j$$\n",
    "\n",
    "Where:\n",
    "- $E(y|X)$ is the expected value (mean) of y given variable matrix $X$\n",
    "- $\\sum_{j}^p$ are the predictors $j$ thru $p$ (columns) of the $X$ matrix\n",
    "- $beta_0$ is the intercept\n",
    "- $beta_j$ is the coefficient for the predictor $x_j$, the $j$th column in variable matrix $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Binary classes case\n",
    "\n",
    "Logistic regression can solve multi-class problems – we will look at this another day – but the basic classification problem is binary. \n",
    "\n",
    "In our case, `1=admitted` and `0=rejected`.\n",
    "\n",
    "The logistic regression is still solving for an expected value. In the binary classification case this expected value is the probability of one class:\n",
    "\n",
    "### $$E[y \\in {0,1}] = P(y = 1)$$\n",
    "\n",
    "In regression syntax we would have:\n",
    "\n",
    "### $$P(y = 1) = \\beta_0 + \\sum_{j}^p\\beta_jx_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. The dilemma: probability estimation using regression\n",
    "\n",
    "### $$P(y = 1) = \\beta_0 + \\sum_{j}^p\\beta_jx_j$$\n",
    "\n",
    "There is an important problem with this new equation: we want to estimate a probability instead of a real number.\n",
    "\n",
    "Why is this a problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. The logit \"link function\"\n",
    "\n",
    "As the name implies, logistic regression is still a regression. It can still be solved by minimization of the sum of squared errors, and there is still an intercept and coefficients.\n",
    "\n",
    "Logistic regression is a twist on regression for categorical/class target variables. Instead of solving for the _mean_ of $y$, logistic regression solves for the _probability of class membership_ of $y$.\n",
    "\n",
    "How does it do this? Regressions can be generalized to $y$ targets that do not fall between `[-infinity, infinity]` through the use of **link functions**.\n",
    "\n",
    "A link function is simply a function of the expected value of the target variable:\n",
    "\n",
    "### $$logit(E(y | X)) = \\beta_0 + \\sum_{j}^p\\beta_jx_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Step 1: Odds ratios\n",
    "\n",
    "We have to modify the regression for it to work for predicting probabilities. The initial step in the solution depends on the use of **odds ratios**. Before we get into _why_, it's important to understand what an odds ratio is.\n",
    "\n",
    "Probabilities and odds ratios represent the same thing in different ways. Probabilities can be alternatively expressed as odds ratios. The odds ratio for probability **p** is defined:\n",
    "\n",
    "### $$\\text{odds ratio}(p) = \\frac{p}{1-p}$$\n",
    "\n",
    "The odds ratio of a probability is a measure of how many times more likely it is than the inverse case.\n",
    "\n",
    "For example:\n",
    "\n",
    "- When **`p = 0.5`**: **`odds ratio = 1`**\n",
    "    - it is equally likely to happen as it is to not happen.\n",
    "- When **`p = 0.75`**: **`odds ratio = 3`**\n",
    "    - it is 3 times more likely to happen than not happen.\n",
    "- When **`p = 0.40`**: **`odds ratio = 0.666..`**\n",
    "    - it is 2/3rds as likely to happen than not happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8: Odds ratio in place of probability\n",
    "\n",
    "What happens if we put the odds ratio in place of the probability in the regression equation?\n",
    "\n",
    "Put the odds ratio in place of the probability on the left side of the regression equation.\n",
    "\n",
    "### $$ \\frac{P(y = 1)}{1-P(y = 1)} = \\beta_0 + \\sum_{j}^p\\beta_jx_j$$\n",
    "\n",
    "The range of odds ratio, our predicted value, is now restricted to be in the range **`[0, infinity]`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 9. Step 2: Log odds (natural logarithm of the odds ratio)\n",
    "\n",
    "If we take the natural logarithm of a variable that falls between 0 and infinity, we can actually transform it into a variable that falls between the range negative infinity and infinity.\n",
    "\n",
    "This is because taking the logarithm of fractions results in negative numbers.\n",
    "\n",
    "The regression can now predict any negative or positive number, and we can convert it back into the odds ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. The logit link function\n",
    "\n",
    "The combination of converting the probability to an odds ratio and taking the logarithm of that is called the **logit link function**, and is what regression uses to estimate probability:\n",
    "\n",
    "\n",
    "### $$\\text{logit}\\big(E[y]\\big) = \\text{logit}\\big(P(y=1)\\big) = log\\bigg(\\frac{P(y=1)}{1-P(y=1)}\\bigg) =  \\beta_0 + \\sum_{j}^p\\beta_jx_j$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meaning of the betas in log odds\n",
    "\n",
    "Remember that our values are in terms of log-odds. \n",
    "\n",
    "> If $\\beta_1$ is 0, then $\\beta_0$ represents the log odds of admittance for a student with an average gpa.\n",
    "\n",
    "> $\\beta_1$ is the effect of a unit increase in gpa on the log odds of admittance. \n",
    "\n",
    "This sucks because log odds are hard to interpret. Luckily though, we can apply the logistic transform to get the probability of admittance at different $\\beta$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "logreg = LogisticRegressionCV(cv=5)\n",
    "\n",
    "# Fit the data points into the LogisticRegression model\n",
    "model = logreg.fit(trainX, trainY)\n",
    "\n",
    "# Predict Probability\n",
    "probabilities = model.predict_proba(testX)\n",
    "\n",
    "# Score the model\n",
    "score = model.score(testX, testY)\n",
    "print 'Model Score: ', score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5: GRADIENT DESCENT\n",
    "# Week 5 (2.1)\n",
    "\n",
    "Gradient descent is in essence an algorithm designed to minimize functions. It is popular in machine learning and statistics for use in minimizing loss functions such as least squares.\n",
    "\n",
    "The gradient descent algorithim uses the derivative of the loss function to move in the direction where the loss function is \"descending\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Derivatives\n",
    "\n",
    "The derivative of a function measures the **rate of change** of the values of the function with respect to another quantity. \n",
    "\n",
    "We are not going to cover the calculus of derivatives today, but will give examples through explaining their use in gradient descent.\n",
    "\n",
    "Imagine the derivative as a tangent line on the edge of another function. For example, in the image below, if the black curve was the velocity of a car, the red tangent would represent the derivative of velocity at that point, which is the acceleration of the car.\n",
    "\n",
    "![derivative](https://camo.githubusercontent.com/2f70b084174b825e3ad88564301f9aaf46997fd3/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30662f54616e67656e745f746f5f615f63757276652e737667)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A derivative of a function indicates whether the function is **increasing or decreasing** based on the value of the derivative. \n",
    "\n",
    "* If the function is not changing (the tangent line is flat), **the derivative is 0**\n",
    "* If the function is increasing (the tangent slope is positive), **the derivative is positive**\n",
    "* If the function is decreasing (the tangent slope is negative), **the derivative is negative**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The least squares loss and regression\n",
    "\n",
    "Recall the least squares loss from yesterday:\n",
    "\n",
    "### $$\\frac{1}{N}\\sum_{i=1}^N{\\left(y_i - \\hat{y}_i\\right)^2}$$\n",
    "\n",
    "As well as the formula for a linear regression with a single predictor variable:\n",
    "\n",
    "### $$y = \\beta_0 + \\beta_1x_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can redefine the loss function, inserting the regression formula:\n",
    "\n",
    "### $$\\frac{1}{N}\\sum_{i=1}^N{\\left(y_i - (\\beta_0 + \\beta_1x_i)\\right)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partial derivatives of the loss functions\n",
    "\n",
    "We are going to calculate the two **partial derivatives** of the loss function. Partial derivatives are derivatives with respect to one variable while keeping the other variables constant. Our partial derivatives will be:\n",
    "\n",
    "* The derivative of the loss function with respect to beta0 (the intercept)\n",
    "* The derivative of the loss function with respect to beta1 (the slope/coefficient for x1)\n",
    "\n",
    "This is because the error function is defined by these two parameters. In other words, the value of the error function depends on the changes in beta0 and beta1. \n",
    "\n",
    "What about x and y? Those variables affect the calculation of the loss, but they are not changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I've basically forgotten my calculus and differentiation, but I looked up the partial derivatives.)\n",
    "\n",
    "**The partial derivative with respect to beta0:**\n",
    "\n",
    "### $$\\frac{\\delta}{\\delta\\beta_0} = \\frac{2}{N}\\sum_{i=1}^N{-\\left(y_i - (\\beta_0 + \\beta_1x_1)\\right)}$$\n",
    "\n",
    "**The partial derivative with respect to beta1:**\n",
    "\n",
    "### $$\\frac{\\delta}{\\delta\\beta_1} = \\frac{2}{N}\\sum_{i=1}^N{-x_i\\left(y_i - (\\beta_0 + \\beta_1x_1)\\right)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are we going to do with these partial derivatives?\n",
    "\n",
    "Recall that a positive derivative indicates an increasing function and a negative derivative indicates a decreasing function. \n",
    "\n",
    "If we subtract a fraction of the partial derivative of beta1 from beta1, and subtract a fraction of the partial derivative of beta0 from beta0, we will modify beta1 and beta0 such that the value of the error function shrinks!\n",
    "\n",
    "We can repeat this incremental process until we reach the minimum of the function.\n",
    "\n",
    "This is called gradient descent because **we are iteratively moving down the gradient of the error function to its minimum.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/7/79/Gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  Gradient descent can fail\n",
    "\n",
    "One of the most fickle things about gradient descent is the step size (also known as learning rate). If this is not tuned properly, the algorithm may never converge and in fact explode into extreme values.\n",
    "\n",
    "Gradient descent also only works where there is a gradient to follow. Here is a toy example of a function where gradient descent will fail:\n",
    "\n",
    "$$f(x, y) = \\begin{cases}\n",
    "2 x^2 & \\quad \\text{if $x \\leq 1$}\\\\\n",
    "2  & \\quad \\text{else}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A6: SGD - STOCHASTIC GRADIENT DESCENT \n",
    "\n",
    "What is the difference between gradient descent and stochastic gradient descent? It's actually a very small difference, but has big implications.\n",
    "\n",
    "Instead of **all** the samples updating the gradient at a time, **only one** sample updates the gradient (iterating over all the observations, though this can change based on specification) within each overall iteration.\n",
    "\n",
    "Stochastic gradient descent has some nice properties over gradient descent:\n",
    "\n",
    "- It solves faster since it immediately starts to update the gradient.\n",
    "- It can handle much, much larger datasets since it only needs to calculate a single row or small batch of rows of the entire dataset.\n",
    "\n",
    "The downside is that the MSE may not converge to an optimal value as well, since local minima become more likely. However, it typically _does_ converge to an optimal, so this risk is a small one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (LinearRegression, LogisticRegression, \n",
    "                                  Lasso, Ridge,\n",
    "                                  SGDRegressor, SGDClassifier)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sgd_reg = SGDRegressor()\n",
    "\n",
    "print sgd_reg_gs.best_params_\n",
    "print sgd_reg_gs.best_score_\n",
    "sgd_reg = sgd_reg_gs.best_estimator_\n",
    "sgd_reg_gs_params = sgd_reg_gs.get_params()\n",
    "sgd_reg_gs_params\n",
    "\n",
    "sgd_cls = SGDClassifier()\n",
    "sgd_cls_gs = RandomizedSearchCV(sgd_cls, sgd_clf_params, cv=5, verbose=2, n_iter=100)\n",
    "print sgd_cls_gs.best_params_\n",
    "print sgd_cls_gs.best_score_\n",
    "sgd_cls = sgd_cls_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A7: SUPPORT VECTOR MACHINES\n",
    "# Week 5 (4.1)\n",
    "Today we'll be learning about a different approach to classification called Support Vector Machines (SVM).\n",
    "\n",
    "These fit a decision boundary similarly to a regression, but uses a different loss function called the \"hinge loss\" (as opposed to the log loss in logistic regression).\n",
    "\n",
    "SVMs are notorious for being less intuitive than other classifiers such as kNN and Logistic regression, but hopefully you will have a feel for it by the end of the lecture.\n",
    "\n",
    "[For a really great resource check out these slides (many of whch I've taken to put in this lecture).](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n",
    "\n",
    "[This website is also a great resource, on a slightly more technical level.](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the SVM classify?\n",
    "\n",
    "#### It's important to start with the intuition for the special _linearly separable_ classification case.\n",
    "\n",
    "\n",
    "If classification of observations is \"linearly separable\", SVM fits the **\"decision boundary\"** that is defined by the largest margin between the closest points for each class. This is commonly called the **\"maximum margin hyperplane (MMH)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linearly separable SVM](./images/linear_separability_vs_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition behind the SVM decision boundary\n",
    "\n",
    "SVM's criterion for a decision surface is one that is _maximally far away from any data point between classes_. The distance from the decision boundary to the closest data point determines the \"margin\" of the classifier.\n",
    "\n",
    "The points SVM uses to fit the decision boundary are called \"support vectors\". This term comes from linear algebra: in a vector space, points can be defined as a vector between the origin and that point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition cont.\n",
    "\n",
    "Why maximize the margin? **SVM solves for the decision boundary that minimizes generalization error.** \n",
    "\n",
    "Observations that are near the decision boundary between the classes are the most ambiguous observations. They are the observations that are approaching equal chance to be one class or the other.\n",
    "\n",
    "SVM, instead of considering all the observations \"equally\" in the loss function it minimizes, defines it's fit using the most ambiguous points. It's decision boundary is _safe_ in that errors in new measured observations are not likely to cause the SVM to mis-classify.\n",
    "\n",
    "The SVM is concerned with generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Maximum margin hyperplane function\n",
    "\n",
    "The decision boundary (which is equivalent to the MMH) is derived by the [discriminant function](https://en.wikipedia.org/wiki/Discriminant_function_analysis#Discriminant_functions). a linear combination of predictors that maximizes the difference between groups. \n",
    "\n",
    "You are already familiar with the discriminant function for logistic regression, represented by the sigmmoid curve solved by the log loss. \n",
    "\n",
    "In an SVM, the discriminant function is:\n",
    "\n",
    "### $$ f(x) = sign(w^T x + b) $$\n",
    "\n",
    "where **$w$** is the normalized weight vector, perpendicular to the decision boundary.\n",
    "\n",
    "$b$ is the \"bias\", which is the corollary to the intercept in regression.\n",
    "\n",
    "(Note that the $bias$ term is sometimes called the intercept. Don't confuse it with bias-variance tradeoff we discussed earlier.)\n",
    "\n",
    "The solution to $f(x)$ for an $x$ observation is a sign (-1, or 1), and this determines the (binary-case) class label of the observation $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hinge loss function\n",
    "\n",
    "The function that the SVM minimizes to find the boundary is:\n",
    "\n",
    "### $$  minimize \\;\\; ||w||^2 + C\\sum_{i=1}^N max\\left(0, 1 - y_i \\: f(x_i)\\right) $$\n",
    "\n",
    "where $||w||^2$ is the magnitude of the weight vector squared\n",
    "\n",
    "$C$ is a regularization term, which we will get to soon. Small $C$ creates a wider margin, and large $C$ creates a tighter margin.\n",
    "\n",
    "### $$\\sum_{i=1}^N max\\left(0, 1 - y_i \\: f(x_i)\\right)$$ \n",
    "\n",
    "is the sum of the errors. \n",
    "\n",
    "If $f(x_i) > 1$, the point lies _outside_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $f(x_i) = 1$ the point is _on_ the margin and does not contribute to the loss.\n",
    "\n",
    "If $f(x_i) < 1$ the point lies _inside_ the margin and **does** contribute to the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss cont.\n",
    "\n",
    "### $$  minimize \\;\\; ||w||^2 + C\\sum_{i=1}^N max\\left(0, 1 - y_i \\: f(x_i)\\right) $$\n",
    "\n",
    "\n",
    "The right hand side of the function to be optimized is saying:\n",
    "\n",
    "> Every point should be on the correct side of the margins for both classes.\n",
    "    \n",
    "The left hand side is saying:\n",
    "\n",
    "> Maximize the distance between the margins by modifying the weight vector.\n",
    "\n",
    "It seems in the above equation like we are minimizing the margin due to the $||w||^2$ term. This is in fact maximizing the margin, but the math behind it is more involved. [This page, which I linked to before, does a good job explaining it in my opinion.](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter $C$\n",
    "\n",
    "The hyper-parameter $C$ controls the extent to which the SVM is tolerant to errors. It is sometimes called the \"soft-margin constant\". \n",
    "\n",
    "$C$ affects the strength of the \"penalty\" similar to the lambda terms in the Ridge and Lasso. \n",
    "\n",
    "By multiplying the sum of the errors, which are the distances from the margins to the points inside of the margin, it allows the SVM to classify non-linearly separable problems by allowing errors to occur. \n",
    "\n",
    "The lower the value of $C$, the more misclassified observations errors are allowed. These misclassified points are known as \"slack variables\". Reducing the effect of errors on the loss function puts the emphasis on widening the margin.\n",
    "\n",
    "For those interested in exporing the math more, [there is a good tutorial here.](http://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/#more-457)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving non-linearly separable problems with the \"kernel trick\"\n",
    "\n",
    "The math behind the \"kernel trick\" is beyond the scope of this lecture, but it allows an SVM to classify non-linearly separable problems. It is a big reason why SVMs are so popular.\n",
    "\n",
    "All you need to understand for now is that with an SVM **you can arbitrarily transform your observations that _have no linear seperability_ by putting them into a different \"dimensional space\".** \n",
    "\n",
    "This is done by \"wrapping\" your predictors in a kernel function that transforms them to a different dimensional space. At the end, they can be transformed back, along with the decision boundary, to their original dimensional space.\n",
    "\n",
    "[Check out these lecture slides for a good overview.](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)\n",
    "\n",
    "The following pictures should give you at least a general intuition of what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "lin_model = SVC(kernel='linear')\n",
    "\n",
    "scores = cross_val_score(lin_model, Xn, y, cv=5)\n",
    "sm = scores.mean()\n",
    "ss = scores.std()\n",
    "\n",
    "rbf_model = SVC(kernel='rbf')\n",
    "\n",
    "scores = cross_val_score(rbf_model, Xn, y, cv=5)\n",
    "sm = scores.mean()\n",
    "ss = scores.std()\n",
    "print \"Average score: {:0.3} +/- {:0.3}\".format(sm, ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A8: Classification and regression trees (CARTs)\n",
    "# Week 6 (1.1)\n",
    "\n",
    "Decision trees are a widely popular and powerful machine learning technique for both classification and regression problems.\n",
    "\n",
    "To perform classification or regression, decision trees make sequential, hierarchical decisions about the outcome variable based on the predictor data.\n",
    "\n",
    "---\n",
    "\n",
    "[Classification CART documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "\n",
    "[Regression CART documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)\n",
    "\n",
    "[Decision tree user guide](http://scikit-learn.org/stable/modules/tree.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of decision trees\n",
    "\n",
    "Decision tree models are **hierarchical** and **non-parametric**.\n",
    "\n",
    "**Hierarchical** means that the model is definied by a sequence of questions which yield a class label or value when applied to any observation. Once trained, the model behaves like a recipe, a series of \"if this then that\" conditions that yields a specific result for our input data.\n",
    "\n",
    "[**Non-parametric** methods](https://en.wikipedia.org/wiki/Nonparametric_statistics) stand in contrast to models like logistic regression or ordinary least squares regression. There are no underlying assumptions about the distribution of the data or the errors. Non-parametric models essentially start with no _assumed_ parameters about the data and construct them based on the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed Acyclic Graphs (DAG)\n",
    "\n",
    "CART models are in fact a special case of [Directed Acyclic Graphs (DAG).](https://en.wikipedia.org/wiki/Directed_acyclic_graph) \n",
    "\n",
    "DAGs have **nodes** and **edges**. In the golf example above, the nodes represent the decision points about the output variable given the predictors, and the edges are the \"paths\" between nodes that represent answers to the questions.\n",
    "\n",
    "The **acyclic** part of DAGs means that the edges do not cycle back on themselves.\n",
    "\n",
    "- The top node is called the **root node**. It has 0 incoming edges, and 2+ outgoing edges. \n",
    "- Internal nodes test a condition on a specific feature. They have 1 incoming edge, and 2+ outgoing edges. \n",
    "- A **leaf node** contains a class label (or regression value). It has 1 incoming edge and 0 outgoing edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a decision tree\n",
    "\n",
    "Building decision trees requires algorithms capable of determining an optimal choice at each node. \n",
    "\n",
    "One such algorithm is [**Hunt's algorithm**](http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/lguo/decisionTree.html). This is a greedy, recursive algorithm that leads to a local optimum:\n",
    "\n",
    "- [**Greedy:**](https://en.wikipedia.org/wiki/Greedy_algorithm) the algorithm makes the most optimal decision it can at each step.\n",
    "- [**Recursive:**](https://en.wikipedia.org/wiki/Recursion) the algorithm splits task into subtasks and solves each the same way.\n",
    "- [**Local optimum:**](https://en.wikipedia.org/wiki/Local_optimum) the algorithm finds a solution just for the given neighborhood of points.\n",
    "\n",
    "The algorithm works by recursively partitioning records into smaller and smaller subsets. The partitioning decision is made at each node according to a metric called **purity.** A node is said to be 100% pure when all of its records belong to a single class (or have the same value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode classification decision tree algorithm\n",
    "\n",
    "---\n",
    "\n",
    "    Given a set of records Dt at node t:\n",
    "        If all records in Dt belong to class A: \n",
    "            t is a leaf node corresponding to class (Base case)\n",
    "        Else if Dt contains records from both A and B:\n",
    "            Create test condition to partition the observations\n",
    "            Define t as an internal node, with outgoing edges to child nodes\n",
    "            partition records in Dt with conditional test logic to child nodes\n",
    "            Recursively apply steps at each child node.\n",
    "\n",
    "- Splits can be binary way or multi-way. \n",
    "- Features can be categorical or continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and \"purity\"\n",
    "\n",
    "Recall from the algorithm above we iteratively create test conditions to split the data. \n",
    "\n",
    "---\n",
    "\n",
    "In a binary classification task, a maximum impurity partition is given by the distribution (classification):\n",
    "\n",
    "### $$ p(0|t) = p(1|t) = 0.5 $$\n",
    "\n",
    "where both classes are equally present in the partition distribution $t$.\n",
    "\n",
    "Maximum purity, on the other hand, is when only one class is present, i.e: \n",
    "\n",
    "### $$ p(0|t) = 1 – p(1|t) = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purity objective function\n",
    "\n",
    "To achieve maximum purity we need an **objective function** to optimize. \n",
    "\n",
    "We want our objective function to measure the **gain in purity** from a particular split. Therefore it depends on the class distribution over the nodes (before and after the split). \n",
    "\n",
    "For example, let \n",
    "\n",
    "### $$p(i|t)$$ \n",
    "\n",
    "be the probability of **`class i`** in the data at **`node t`** (e.g., the fraction of records labeled **`i`** at node **`t`**) \n",
    "\n",
    "We then define an impurity function that will smoothly vary between the two extreme cases of minimum impurity (one class or the other only) and the maximum impurity case cas as an equal mix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common purity functions (classification)\n",
    "\n",
    "---\n",
    "\n",
    "## $$ \\text{Entropy} = - \\sum_{i=1}^{classes} p(i\\;|\\;t)\\;log_2\\; p(i\\;|\\;t) $$\n",
    "\n",
    "## $$ \\text{Gini} = 1 - \\sum_{i=1}^{classes} p(i\\;|\\;t)^2 $$\n",
    "\n",
    "The Gini inpurity is primarily used in the CART algorithm, but both Gini and Entropy are available in sklearn's classification and decision tree models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purity functions](./images/measures.png)\n",
    "\n",
    "---\n",
    "\n",
    "Impurity measures on their own they are not enough to tell us how a split will do. We need to look at impurity **before & after** the split. We can make this comparison using what is called the **gain**: \n",
    "\n",
    "## $$ \\Delta = I(\\text{parent}) - \\sum_{\\text{children}}\\frac{N_j}{N}I(\\text{child}_j) $$\n",
    "\n",
    "Where $I$ is the impurity measure, $N_j$ denotes the number of records at child node $j$, and $N$ denotes the number of records at the parent node. When $I$ is the [**entropy function**](https://en.wikipedia.org/wiki/Binary_entropy_function), this quantity is called the [**information gain**](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees).\n",
    "\n",
    "**[Nice example of how misclassification error can break branching.](http://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing overfitting\n",
    "\n",
    "A stopping criterion determines when to no longer construct further nodes. \n",
    "\n",
    "We can stop when all records belong to the same class, or when all records have the same attributes. This [**maximizes variance at the expense of bias**](http://blog.fliptop.com/blog/2015/03/02/bias-variance-and-overfitting-machine-learning-overview/), leading to overfitting. \n",
    "\n",
    "**Setting a maximum depth:**\n",
    "\n",
    "A simple way to prevent overfitting is to set a hard limit on the \"depth\" of the decision tree.\n",
    "\n",
    "**Minimum observations to make a split:**\n",
    "\n",
    "An alternative to maximum depth (and can be used at the same time), is to specify the minimum number of datapoints reqired to make a split at a node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART advantages\n",
    "\n",
    "- Simple to understand and interpret. People are able to understand decision tree models after a brief explanation.\n",
    "    - Useful to work with non technical departments (marketing/sales).\n",
    "- Requires little data preparation. \n",
    "    - Other techniques often require data normalization, dummy variables need to be created and blank values to be removed.\n",
    "- Able to handle both numerical and categorical data. \n",
    "    - Other techniques are usually specialized in analyzing datasets that have only one type of variable.\n",
    "- Uses a **white box** model.\n",
    "    - If a given situation is observable in a model the explanation for the condition is easily explained by boolean logic.\n",
    "    - By contrast, in a **black box** model, the explanation for the results is typically difficult to understand, for example in neural networks.\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "- Robust. Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "- Performs well with large datasets. Large amounts of data can be analyzed using standard computing resources in reasonable time.\n",
    "- Once trained can be implemented on hardware and has extremely fast execution.\n",
    "    - Real-time applications like trading, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART disadvantages\n",
    "\n",
    "- Locally-optimal.\n",
    "    - Practical decision-tree learning algorithms are based on heuristics such as the greedy algorithm where locally-optimal decisions are made at each node. \n",
    "    - Such algorithms cannot guarantee to return the globally-optimal decision tree.\n",
    "- Overfitting.\n",
    "    - Decision-tree learners can create over-complex trees that do not generalize well from the training data.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems. In such cases, the decision tree becomes prohibitively large.\n",
    "    - Neural networks, for example, are superior for these problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#regression\n",
    "dtr1 = DecisionTreeRegressor(max_depth=1)\n",
    "dtr1.fit(Xr, yr)\n",
    "dtr1_scores = cross_val_score(dtr1, Xr, yr, cv=4)\n",
    "\n",
    "#classification\n",
    "dtc1 = DecisionTreeClassifier(max_depth=1)\n",
    "dtc1.fit(Xr, yr)\n",
    "\n",
    "dtc1_scores = cross_val_score(dtr1, Xr, yr, cv=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A9-11: Intro to ensemble methods\n",
    "# Week 6 (2.3)\n",
    "\n",
    "**Ensemble methods** are supervized learning models which combine the predictions of multiple smaller models to improve predictive power and generalization.\n",
    "\n",
    "The smaller models that combine to make the ensemble model are referred to as **base models**. Ensemble methods often result in considerably higher performance than any of the individual base models could achieve.\n",
    "\n",
    "![ensemble](./images/Ensemble.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use ensembles\n",
    "\n",
    "    - Medical diagnoses\n",
    "    - Predicting disease outbreak, natrual disasters\n",
    "    - Stock market predictions\n",
    "    - AI\n",
    "\n",
    "Or any case where the highest performance is desired at the expense of model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two popular families of ensemble methods\n",
    "\n",
    "---\n",
    "\n",
    "**BAGGING**\n",
    "\n",
    "Several estimators are built independently on subsets of the data and their predictions are averaged. Typically the combined estimator is usually better than any of the single base estimator.\n",
    "\n",
    "**Bagging can reduce variance with little to no effect on bias.**\n",
    "\n",
    "    ex: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**BOOSTING**\n",
    "\n",
    "Base estimators are built sequentially. Each subsequent estimator focuses on the weaknesses of the previous estimators. In essence several weak models \"team up\" to produce a powerful ensemble model. (We will discuss these later this week.)\n",
    "\n",
    "**Boosting can reduce bias without incurring higher variance.**\n",
    "\n",
    "    ex: Gradient Boosted Trees, AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential deficiencies of base models\n",
    "\n",
    "There are three categories of weaknesses in which \"base models\" can fail or produce poor results:\n",
    "\n",
    "1. Statistical problems\n",
    "2. Computational problems\n",
    "3. \"Representational\" problems\n",
    "\n",
    "Ensemble methods are designed to address any or all three.\n",
    "\n",
    "---\n",
    "\n",
    "Let\n",
    "\n",
    "### $$ \\begin{aligned} \\text{true function of data} &= f() \\\\ \\text{model function of data} &= h() \\end{aligned}$$\n",
    "\n",
    "Where $h()$ can be a classifier or a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical problem\n",
    "\n",
    "**The amount of training data available is small**. A single base classifier will have difficulty converging to $h()$.\n",
    "\n",
    "![statistical](./images/statistical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "A bagging ensemble model, for example, mitigates this problem by \"averaging out\" base classifier predictions to improve convergence on the true function.\n",
    "\n",
    "[Paper describing in-depth reason for this.](http://web.cs.iastate.edu/~jtian/cs573/Papers/Dietterich-ensemble-00.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational problem\n",
    "\n",
    "There is sufficient training data, but it is computationally intractable to find the best model $h()$.\n",
    "\n",
    "For example, if a base classifier is a decision tree, an exhaustive search of the hypothesis space of all possible classifiers is extremely complex (NP-complete).\n",
    "\n",
    "This is, for example, why decision trees use heuristic algorithms at nodes (greedy search).\n",
    "\n",
    "![computational](./images/computational.png)\n",
    "\n",
    "---\n",
    "\n",
    "Ensembles composed of several, simpler base models using different starting points can converge faster to a good approximation of $f()$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representational problem\n",
    "\n",
    "Suppose we use a decision tree as a base classifier. A decision tree works by forming a \"rectilinear\" partition of the feature space, **i.e it always cuts at a fixed value along a feature.**\n",
    "\n",
    "But what if $f()$ is best modeled by diagonal line?\n",
    "\n",
    "It cannot be represented by a finite number of rectilinear segments, and the true decision boundary cannot be obtained by the decision tree classifier.\n",
    "\n",
    "![dtcut](./images/dtcut.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A representational problem occurs when $f()$ cannot be expressed in terms of our hypothesis at all.** \n",
    "\n",
    "Yet, it may be still be possible to approximate $f()$ by expanding the space of representable functions using ensemble methods!\n",
    "\n",
    "![representational](./images/representational.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions for ensembles to outperform base models\n",
    "\n",
    "For an ensemble method to perform better than a base classifier, it must meet these two criteria:\n",
    "\n",
    "1. **Accuracy:** the combination of base classifiers must outperform random guessing. \n",
    "2. **Diversity:** base models must not be identical in classification/regression estimates.\n",
    "    - [Description of diversity.](https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume11/opitz99a-html/node2.html)\n",
    "    - [Paper on measures of diversity.](http://staff.ustc.edu.cn/~ketang/papers/TangSuganYao_ML06.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "The ensemble method we will be using today is called **bagging**, which is short for **bootstrap aggregating**.\n",
    "\n",
    "Bagging builds multiple base models with **resampled training data with replacement.** We train $k$ base classifiers on $k$ different samples of training data. Using random subsets of the data to train base models promotes more differences between the base models.\n",
    "\n",
    "Random Forests, which \"bag\" decision trees, can achieve very high classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging's magic decrease of model variance \n",
    "\n",
    "One of the biggest advantages of Random Forests is that they **decrease variance without increasing bias**. Essentially you can get a better model without having to trade off between bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "**VARIANCE DECREASE**\n",
    "\n",
    "Base model estimates are averaged together, so variability of model predictions (across hypothetical samples) is lower.\n",
    "\n",
    "---\n",
    "\n",
    "**NO/LITTLE BIAS INCREASE**\n",
    "\n",
    "The bias remains the same as the bias of the individual base models. The model is still able to model the \"true function\" since the  base models' complexity is unrestricted (low bias).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A9: Building a basic Random Forest\n",
    "# Week 6 (2.3)\n",
    "\n",
    "The Random Forest model is a popular bagging ensemble method. It combines many decision tree classifiers or regressors as the \"base models\" to make predictions.\n",
    "\n",
    "By building this ourselves we will get to see the internals of exactly what is going on in a bagging ensemble model.\n",
    "\n",
    "---\n",
    "\n",
    "### Construction of the RF\n",
    "\n",
    "The Random Forest classifier is built such that:\n",
    "\n",
    "1. Multiple internal decision tree classifiers will be built as the base models\n",
    "- For each base model, the data will be resampled like in bootstrapping.\n",
    "- Each decision tree will be fit on the bootstrapped sample of the data.\n",
    "- To predict, each internal base model will be passed the new data and make their predictions. The final output will be a vote across the base models for the class.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular bagging methods and Boosting\n",
    "\n",
    "---\n",
    "\n",
    "- Random Forests\n",
    "- Extremely Randomized Trees\n",
    "- Intro to \"Boosting\" ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rf](./images/randomforests_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Bagging\n",
    "\n",
    "---\n",
    "\n",
    "In a bagging ensemble of decision trees where each tree gets all the predictors, **decision tree base estimators can end up correlated**. \n",
    "\n",
    "For example, if one or a few features are very strong predictors for the dependent variable, these features will be selected in many or most of the bagging base trees. This makes the decisions of the base estimator trees correlated.\n",
    "\n",
    "By selecting a random subset of the features at each split (feature bagging), we reduce the amount of correlation between the base models, which strengthens the predictions of the overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature bagging heuristics\n",
    "\n",
    "---\n",
    "\n",
    "Let $p$ be the number of features.\n",
    "\n",
    "**For classification problems**, $\\sqrt{p}$ (rounded down) is the recommended number of features to randomly subset for each split. \n",
    "\n",
    "**For regression problems** $p/3$ (rounded down) is the recommended feature subset size with a minimum node depth of 5 as the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=100,max_depth=5)\n",
    "score = cross_val_score(rfr,Xtrain,ytrain,cv=5,verbose=1)\n",
    "print np.mean(score), score\n",
    "\n",
    "rf = RandomForest(n_estimators=1000, max_depth=None, max_features='auto')\n",
    "rf.fit(Xtrain, ytrain)\n",
    "yhat = rf.predict(Xtest)\n",
    "print 'rf acc:', accuracy_score(ytest, yhat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A10: Extremely Randomized Trees\n",
    "# Week 6 (3.2)\n",
    "---\n",
    "\n",
    "**Extremely Randomized Trees** (in sklearn: **`ExtraTrees`**) are different from Random Forests in two ways:\n",
    "\n",
    "1. The data sent to each base estimator is no longer a bootstrapped sample.\n",
    "2. Within each base estimator, the node splits are no longer optimized based on purity, but instead split criteria is determined at random.\n",
    "\n",
    "---\n",
    "\n",
    "In other words, **the top-down splitting in each base tree learner is randomized.**\n",
    "\n",
    "The split values at nodes are selected from the feature's possible range in the data to ensure the split actually splits up the data at each node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extremely Randomized Trees vs. Random Forests\n",
    "\n",
    "---\n",
    "\n",
    "1. **Reduces variance more than RF:** greater variety of leaf nodes.\n",
    "2. **Bias is increased more than RF**: each base estimator tree has greater expected error between observations and decision boundary.\n",
    "3. **Computationally faster to train**: no optimal split calculations at nodes.\n",
    "4. **Less correlation between base estimator decisions.**\n",
    "5. **Potentially better performance than RF.**\n",
    "\n",
    "---\n",
    "\n",
    "They are not guaranteed to perform better than Random Forests. It depends on the data and the problem. \n",
    "\n",
    "[For a full overview of differences see this paper.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7485&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "etr = ExtraTreesRegressor(n_estimators=100,max_depth=5)\n",
    "score = cross_val_score(etr,Xtrain,ytrain,cv=5,verbose=1)\n",
    "print np.mean(score), score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is another ensemble method with a theoretically different approach than bagging.\n",
    "\n",
    "---\n",
    "\n",
    "**BOOSTING**\n",
    "\n",
    "1. **Base model fitting an iterative procedure**: it cannot be run in parallel.\n",
    "- **Weights assigned to observations indicating their \"importance\"**: samples with higher weights are given higher influence on the total error of the next model, prioritizing those observations.\n",
    "- **Weights change at each iteration with the goal of correcting the errors/misclassifications of the previous iteration**: the first base estimator is fit with uniform weights on the observations.\n",
    "- **Final prediction is typically constructed by a weighted vote**: weights for each base model depends on their training errors or misclassification rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and cons\n",
    "\n",
    "---\n",
    "\n",
    "**PROS**\n",
    "\n",
    "- Achieves higher performance than bagging when hyper-parameters tuned properly.\n",
    "- Can be used for classification and regression equally well.\n",
    "- Easily handles mixed data types.\n",
    "- Can use \"robust\" loss functions that make the model resistant to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "**CONS**\n",
    "\n",
    "- Difficult and time consuming to properly tune hyper-parameters.\n",
    "- Cannot be parallelized like bagging (bad scalability when huge amounts of data).\n",
    "- More risk of overfitting compared to bagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![boostvsbag](./images/BoostingVSBagging.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting and bias-variance \n",
    "\n",
    "---\n",
    "\n",
    "Recall that **bagging aims to reduce variance**.\n",
    "\n",
    "**Boosting aims to reduce bias!** (and can reduce variance a bit as well).\n",
    "\n",
    "---\n",
    "\n",
    "#### Why?\n",
    "\n",
    "The rationale/theory behind Boosting is to combines **many weak learners into a single strong learner.**\n",
    "\n",
    "Instead of deep/full decision trees like in bagging, **Boosting uses shallow/high bias base estimators.**\n",
    "\n",
    "Thus, each weak learner has:\n",
    "\n",
    "- Low variance\n",
    "- High bias\n",
    "\n",
    "And the iterative fitting to explain error/misclassification unexplained by the previous base models reduces bias without increasing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A11: AdaBoost\n",
    "# Week 6 (3.2)\n",
    "\n",
    "---\n",
    "\n",
    "AdaBoost is the original boosting algorithm. Predictions from AdaBoost follow the formula:\n",
    "\n",
    "\n",
    "### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$\n",
    "\n",
    "where\n",
    "\n",
    "$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\n",
    "\n",
    "$T$ is the set of \"weak learners\"\n",
    "\n",
    "$\\alpha_t$ is the contribution weight for weak learner $t$\n",
    "\n",
    "$h_t(X)$ is the prediction of weak learner $t$\n",
    "\n",
    "and $y$ is binary **with values -1 and 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weak learner classifiers are trained sequentially.  After each fit, the importance weights on each observation need to be updated. AdaBoost, like all boosting ensemble methods, focuses the next model's fit on the misclassifications/weaknesses of the prior models.\n",
    "\n",
    "All training examples start with equal importance weighting. When we finish training a classifier, we update the importance weighting of the classifier itself represented by alpha $\\alpha$.\n",
    "\n",
    "### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)$$\n",
    "\n",
    "Where $\\epsilon_t$ is the misclassification rate for the current classifier:\n",
    "\n",
    "### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training sample importance weights are adjusted according to this alpha parameter. First update the weights like so:\n",
    "\n",
    "### $$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n",
    "\n",
    "And then each by the sum of weights to normalize them, ensuring that they sum to 1:\n",
    "\n",
    "### $$ D_{t+1}(i) = \\frac{D_{t+1}(i)}{\\sum_{i=1}^N D_{t+1}(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "abr = AdaBoostRegressor(n_estimators=100)\n",
    "score = cross_val_score(abr,Xtrain,ytrain,cv=5,verbose=1)\n",
    "print np.mean(score), score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A12: Gradient Boosting Models\n",
    "# Week 6 (3.2)\n",
    "\n",
    "---\n",
    "\n",
    "A Gradient Boosting Model (GBM) is a generalization of boosting that is essentially an **approximation of gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why?\n",
    "\n",
    "**In gradient descent, we minimized prediction error with regard to the coefficients $b_1 ... b_i$**\n",
    "\n",
    "**GBM minimizes with respect to the function defining prediction errors $f(x)$**\n",
    "\n",
    "More intuitively, at each step in the GBM:\n",
    "- A model $h(x)$ is constructed to further reduce overall error defined by $f(x)$\n",
    "- The model $h(x)$ therefore _emulates a step descending the gradient of the total error space._ \n",
    "\n",
    "By minimizing the error with respect to the function we can perform the \"gradient descent\" down a loss function like least-squares loss for non-parametric models!\n",
    "\n",
    "---\n",
    "\n",
    "_For more math-y explanations (as if this wasn't bad enough) see [here](https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting) and [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=100)\n",
    "score = cross_val_score(gbr,Xtrain,ytrain,cv=5,verbose=1)\n",
    "print np.mean(score), score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Unsupervised Learning \n",
    "\n",
    "<img src=\"https://snag.gy/BdfATE.jpg\" style=\"width: 500px\">\n",
    "\n",
    "Clustering is one of the most ubiquitous and widespread methods for understanding a dataset. In clustering, we group points in a dataset together so that the members of that group are more similar to each other than they are to members of other groups. In this sense, we're creating groups to understand our data. \n",
    "\n",
    "- No \"true\" target / response to compare\n",
    "- We apply structure to data, rather than assume some structure (ie: no true value / class / label)\n",
    "- Predictions based on observed characteristics within data\n",
    "\n",
    "For instance; Your employer gives you a dataset of voter preferences from a local poll and they want you to figure out just exactly how these voters are grouping based on their preferences. The answer: clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is Clustering Different from Classification? \n",
    "\n",
    "You may be thinking: How is clustering different from classification? If we're just creating groups, aren't the two one and the same?\n",
    "There exists an important distinction between classification and clustering: In classification, we are grouping data according to a set of predefined groups; We know what the characteristics of a mammal are, and humans have the characteristics of that predefined group. In clustering, however, we set out to figure out *if* the points in our dataset have relationships with each other, and we group those with similar characteristics in a cluster. In other words, we need to discover the classes themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Review\n",
    "\n",
    "KNN is a supervised classification method, with some important differences.  We will review.\n",
    "\n",
    "![](https://snag.gy/WPF4ZS.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## How Does Clustering Work? - Demo (10 mins)\n",
    "\n",
    "The are numerous algorithms for clustering a dataset:\n",
    "\n",
    "- **K-Means** (mean centroids)\n",
    "- **Heirarchical** (nested clusters by merging or splitting successively)\n",
    "- **DBSCAN** (density based)\n",
    "- Affinity Propagation (graph based approach to let points 'vote' on their preferred 'exemplar')\n",
    "- Mean Shift (can find number of clusters)\n",
    "- Spectral Clustering\n",
    "- Agglomerative Clustering (suite of algorithms all based on applying the same criteria/characteristics of one cluster to others)\n",
    "\n",
    "\n",
    "\n",
    "Today we're going to look at one of the most commonly used algorithms: k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A13: K-Means Clustering\n",
    "# Week 7 (1.1)\n",
    "\n",
    "### K-Means is the most popular clustering algorithm\n",
    "\n",
    "It's one of the easier methods to understand and other clustering techniques will be based on some of the assumptions that K-Means is based on.\n",
    "\n",
    "- # $K = Clusters$\n",
    "- # $Means = Mean\\ of\\ cluster\\ points$\n",
    "\n",
    "in which the number of clusters k is chosen in advance, after which the goal is to partition the inputs into sets  in a way that minimizes the total sum of squared distances from each point to the mean of its assigned cluster.”\n",
    "\n",
    "\n",
    "K-Means is a clustering algorithm that assumes *k* clusters, and then computes these clusters based on the attributes of the available data. The algorithm takes your entire dataset, let's call it *df*, and iterates over its attributes to determine clusters based around centers, known as **centroids**. Unlike many statistical methods, there is no finite way to determine what \"k\" is; for our methods, we're going to approximate *k* based on distribution of our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Refresher\n",
    "\n",
    "_We are going to leave this here for those of us who want a refresher, and move on..._\n",
    "\n",
    "![](https://snag.gy/5v9dLl.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Step by Step\n",
    "\n",
    "<table width=\"500\" cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/7haoS3.jpg\" style=\"width: 150px\"></td>\n",
    "   <td style=\"vertical-align: top; width: 400px;\"><br><b>Step 1.</b><br>We have data in a N-Dimensional feature space (2D for example).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/DaIVgk.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><br><b>Step 2.</b><br>Intialize K centroid (2 here).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/DaIVgk.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 3.</b><br>Assign points to *closest* cluster based on _euclidean distance_.<br><br>$\\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$\n",
    "\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/NY1EeT.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 4.</b><br>Calculate mean of points assigned to centroid (2 here).  Update new centroid positions to mean (ie: geometric center).<br><br>$new\\ centroid\\ position= \\bar{x}, \\bar{y}$\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/tSfDZs.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 5.</b><br>Repeat step 3-4, updating class membership based on centroid distance.\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/BbIicn.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Fin.</b><br>Convergence is met once all points no longer change to a new class (defined by closest centroid distance).\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example\n",
    "\n",
    "![](https://snag.gy/5hFXUA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means is sensitive to outliers\n",
    "\n",
    "![](https://snag.gy/WFNMQY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means is also sensitive to centroid initialization\n",
    "\n",
    "![](https://snag.gy/5sigCD.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 mins) How Many K?\n",
    "\n",
    "Sometimes it's obvious, and sometimes it's not!  What do you think?\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/4rU39.png\"><br>1</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/gq28F.png\"><br>2</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"https://snag.gy/cWPgno.jpg\"><br>3</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing K\n",
    "\n",
    "You may want to select an appropriate method of initializing your centroids, for instance:\n",
    "\n",
    "- Randomly\n",
    "- Manually\n",
    "- Special KMeans++ method in Sklearn (_This initializes the centroids to be generally distant from each other_)\n",
    "\n",
    "**Depending on your problem, you may find some of these are better than others.**\n",
    "\n",
    "_Manual is recommended if you know your data well enough to see the clusters without much help._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note about K-Means convergence\n",
    "\n",
    "In general, k-means will converge to a solution and return a partition of k clusters, even if no natural clusters exist in the data.  It's entirely possible the clusters may not mean anything at all. **Knowing your domain and dataset is essential.**\n",
    "\n",
    "_\"Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the init='kmeans++' parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.\"_ [sklearn Clustering Guide](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "![](http://www.datamilk.com/kmeans_animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test the _fit_ of our centroids and clusters by computing the **Silhouette Coefficient**, a metric to test the cohesion of local points to their clusters and the seperation to other clusters.\n",
    "\n",
    "**We will talk about the Silhouette Coefficient in more depth in our next lesson**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "There are tons of problems with K-Means.  Largely, it's possible to get widely different results especially if we use random initialization.\n",
    "\n",
    "- Different solutions can converge with random initialization\n",
    "- Not everything can be reflected as a \"glob\" of points\n",
    "   - Irregular shapes\n",
    "   - Different densities\n",
    "   - Inconsistent cluster spacing\n",
    "- Interpretation of results dificult without subject matter expertise\n",
    "   - Accuracy is highly subjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.aishack.in/static/img/tut/kmeans-bad-initial-guess.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why would we learn this? \n",
    "\n",
    "** Basic Cluster Analysis **\n",
    "\n",
    "- Easily visualize our data\n",
    "- Find subsets of common characteristics\n",
    "- Understand dissimilar clusters\n",
    "- A stand-alone tool to get insight into data distribution\n",
    "- As a preprocessing step for other algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Evalutation\n",
    "\n",
    "The key to understanding your clustering analysis are the visual evaluation of your clusters, the measurement of their characteristics, and the computation of metrics that can measure how good your analysis is and how to interpret it.\n",
    "\n",
    "If we were to make some basic assumptions about cluster quality, they would be:\n",
    "\n",
    "- high intra-class similarity (within clusters)\n",
    "- low inter-class similarity  (to other clusters)\n",
    "\n",
    "With K-Means, the **silhouette coefficient** is one such measure we will explore.  There are other measures of cluster quality worth mentioning, that aren't implemented in sklearn, here is some additional material to explore:\n",
    "\n",
    "- [Davies Bouldin Index (DBI)](https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index) _- The Davies-Bouldin index is the sum of the worst intra-to-inter cluster distance ratios over all kk clusters_\n",
    "- [Dunn index](https://en.wikipedia.org/wiki/Dunn_index) _- Ratio between the smallest distance between observations not in the same cluster to the largest intra-cluster distance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Techniques to Evaluate Clusters - Demo (15 mins)\n",
    "\n",
    "From visual methods to metrics to algorithms, there are many methods that we can use to evaluate clusters and our clustering algorithms. Today, we're going to look at a few of the more common ones. \n",
    "\n",
    "![](http://www.turingfinance.com/wp-content/uploads/2015/02/K-Means-Clustering-Gif.gif.pagespeed.ce.KrcZK0xYgT.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "\n",
    "When evaluating clusters, the first and easiest method is to visually examine the output of the clustering algorithm. After we run the algorithm and calculate the centroids as we did in the previous lesson, we can plot the resulting clusters to see where the centroids are based and how the clusters are grouping. \n",
    "\n",
    "![plot](../assets/images/plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhoutte Scores\n",
    "\n",
    "The silhouette score, or silhouette coefficient, is the measure of how closely related a point is to members of its cluster rather than members of other clusters. If the resulting score is high **1**, then the clustering analysis has an appropriate number of clusters. If the score is low **-1**, there are either too many or too few clusters.\n",
    "\n",
    "```python\n",
    "metrics.silhouette_score(y, predicted, metric='euclidean')\n",
    "```\n",
    "\n",
    "** But what does it mean!?  Great question!**\n",
    "\n",
    "We need to understand the idea of **chehesion** and **seperation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohesion\n",
    "\n",
    "**Cohesion** measures clustering effectiveness within a cluster.\n",
    "\n",
    "\n",
    "![](https://snag.gy/lIyfsF.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation\n",
    "\n",
    "**Separation** measures clustering effectiveness between clusters.\n",
    "\n",
    "![](https://snag.gy/W5o7nh.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://snag.gy/FLwIql.jpg)\n",
    "\n",
    "One useful measure than combines the ideas of cohesion and separation is the **silhouette coefficient**. For point $x_i$, this is given by: \n",
    "\n",
    "![](https://snag.gy/vmAExH.jpg)\n",
    "\n",
    "such that:\n",
    "- $a_i$ = average in-cluster of $x_i$ to all other points in it's cluster\n",
    "- $b_i$ = the smallest average of point $x_i$ to all other cluster points (by cluster) _$min(b_i)$ of points to clusters_\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://snag.gy/3eCGRi.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretting Silhouette Coefficient\n",
    "\n",
    "- The silhouette of the entire dataset is the average of the silhouette scores of all the individual records.\n",
    "- The silhouette coefficient can take values between -1 and 1.\n",
    "\n",
    "In general, we want separation to be high and cohesion to be low. This corresponds to a value of **SC** close to +1.\n",
    "\n",
    "A **negative silhouette coefficient** means the cluster radius is larger than the space between clusters, and thus clusters overlap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette for Optimal K\n",
    "\n",
    "One of the things we can use the silhouette score for, is finding optimal number of K.  **Keep in mind, this is still a subjective measure and it's not guaranteed to be an official measure of quality.  It can help you but it's no substitute for knowing your data**.  Visually inspecting your data, in addition to looking at how the silhouette changes over time, can give you a rough sense of the quality of your clusters in terms in **cohesion** and **seperation**, but nothing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Silhouette SSE average over K\n",
    "\n",
    "Using the \"elbow\" technique / trick, which may sound really disreputable, but is actually a great way to evaluate the optimal number of K and **intertia** (i.e. the sum of distances to the nearest cluster center).  Basically, we look for the elbow in our scores over a range of K, and chose the value before it levels off.\n",
    "\n",
    ">\"More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified.\" [Elbow Method](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method)\n",
    "\n",
    "Ideally, the elbow would look like this:\n",
    "\n",
    "![](http://i.stack.imgur.com/BzwBY.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "model = KMeans(n_clusters = k)\n",
    "model.fit(df)\n",
    "\n",
    "labels = model.labels_\n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "core = silhouette_score(df, labels, metric='euclidean')\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A14: Principal Components Analysis (PCA)\n",
    "# Week 7 (3.1)\n",
    "\n",
    "---\n",
    "\n",
    "PCA is the quintessential \"dimensionality reduction\" algorithm. \n",
    "\n",
    "Dimensionality reduction is the process of combining or collapsing your existing features (columns in X) into new features that not only retain the signal in the original data in fewer variables while ideally reducing noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA?\n",
    "\n",
    "---\n",
    "\n",
    "PCA finds the linear combinations of your current predictor variables that create new \"principal components\". The principal components explain (in order) the maximum possible amount of variance in your predictors.\n",
    "\n",
    "A more natural way of thinking about PCA is that **it transforms the coordinate system so that the axes become the  most concise, informative descriptors of our data as a whole.**\n",
    "\n",
    "Your old axes are your origina, variables, and your new axes are the principal components from PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Process of PCA \n",
    "\n",
    "---\n",
    "\n",
    "Say we have a matrix $X$ of predictor variables. PCA will give us the ability to transform our $X$ matrix into a new matrix $Z$. \n",
    "\n",
    "First we will derive a **weighting matrix** $W$ from the correlational/covariance structure of $X$ that allows us to perform the transformation.\n",
    "\n",
    "Each successive dimension (column) in $Z$ will be rank-ordered according to variance in it's values!\n",
    "\n",
    "**There are 3 assumptions that PCA makes:**\n",
    "1. Linearity: Our data does not hold nonlinear relationships.\n",
    "2. Large variances define importance: our dimensions are constructed to maximize remaining variance.\n",
    "3. Principal components are orthogonal: each component (columns of $Z$) is completely un-correlated with the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "---\n",
    "\n",
    "![eigenvalue](./assets/images/eigenvalue.png)\n",
    "![orthogonal eigenvectors](./assets/images/eigenvectors_orthogonal.png)\n",
    "![transformed XY](./assets/images/transformed_xy.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EIGENVECTORS**\n",
    "\n",
    "An eigenvector specifies a direction through the original coordinate space. The eigenvector with the highest correspoding eigenvalue is the first principal component.\n",
    "\n",
    "---\n",
    "\n",
    "**EIGENVALUES**\n",
    "\n",
    "Eigenvalues indicate the amount of variance in the direction of it's corresponding eigenvector.\n",
    "\n",
    "---\n",
    "\n",
    "**Every eigenvector has a corresponding eigenvalue!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PCA coord transform](./assets/images/pca_coordinate_transformation.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Principal Components\"\n",
    "\n",
    "---\n",
    "\n",
    "What is a principal component? **Principal components are the vectors that define the new coordinate system for your data.** Transforming your original data columns onto the principal component axes constructs new variables that are optimized to explain as much variance as possible and to be independent (uncorrelated).\n",
    "\n",
    "Creating these variables is a well-defined mathematical process, but in essence **each component is created as a weighted sum of your original columns, such that all components are orthogonal (perpendicular) to each other**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRINCIPAL COMPONENT TRANSFORMATION OF DATA: PC1 VS PC2**\n",
    "\n",
    "[setosa.io has an extremely nice interactive visualization for PCA](http://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why would we want to do PCA?\n",
    "\n",
    "---\n",
    "\n",
    "- We can reduce the number of dimensions (remove bottom number of components) and lose the least possible amount of variance information in our data.\n",
    "- Since we are assuming our variables are interrelated (at least in the sense that they together explain a dependent variable), the information of interest should exist along directions with largest variance.\n",
    "- The directions of largest variance should have the highest Signal to Noise ratio.\n",
    "- Correlated predictor variables (also referred to as \"redundancy\" of information) are combined into independent variables. Our predictors from PCA are guaranteed to be independent.\n",
    "\n",
    "---\n",
    "\n",
    "[Good paper on PCA](http://arxiv.org/pdf/1404.1100.pdf)\n",
    "\n",
    "[Nice site on performing PCA](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual PCA Codealong\n",
    "\n",
    "---\n",
    "\n",
    "**MANUAL PCA STEPS:**\n",
    "\n",
    "1. Standardize data: centering is required, but full normalization is nice for visuals later.\n",
    "2. Calculate eigenvectors and eigenvalues from correlation or covariance matrix.\n",
    "3. Sort eigenvalues and choose eigenvectors that correspond to the largest eigenvalues. The number you choose is up to you, but we will take 2 for the sake of visualization here.\n",
    "4. Construct the projection weighting matrix $W$ from the eigenvectors.\n",
    "5. Transform the original dataset $X$ with $W$ to obtain the new 2-dimensional transformed matrix $Z$.\n",
    "\n",
    "---\n",
    "\n",
    "**DATA**\n",
    "\n",
    "We are going to be using a simple 75-row, 4-column dataset with demographic information. It contains:\n",
    "\n",
    "    age (limited to 20-65)\n",
    "    income\n",
    "    health (a rating on a scale of 1-100, where 100 is the best health)\n",
    "    stress (a rating on a scale of 1-100, where 100 is the most stressed)\n",
    "    \n",
    "All of the variables are continuous.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "hep_pca = PCA()\n",
    "hep_pca.fit(stats_n)\n",
    "stats_pcs = hep_pca.transform(stats_n)\n",
    "\n",
    "stats_pcs = pd.DataFrame(stats_pcs, columns=['PC'+str(i) for i in range(1,8)])\n",
    "\n",
    "hep_pca.explained_variance_ratio_\n",
    "\n",
    "for col, comp in zip(stats.columns, hep_pca.components_[0]):\n",
    "    print col, comp\n",
    "\n",
    "for col, comp in zip(stats.columns, hep_pca.components_[1]):\n",
    "    print col, comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "# A15: Hierarchical Clustering (10 mins)\n",
    "\n",
    "#### What is Hierarchical Clustering? \n",
    "\n",
    "Hierarchical clustering, like k-means clustering, is another common form of clustering analysis. With this type of clustering - we seek to do exactly what the name suggests: \n",
    "\n",
    "- Build hierarchies of links\n",
    "- Form clusters\n",
    "\n",
    "Once these links are determined, they are displayed in what is called a **dendrogram** - a graph that displays all of these links in a hierarchical manner.\n",
    "\n",
    "![denex](../assets/images/denex.png)\n",
    "\n",
    "To find clusters in a dendrogram, we can cut the graph to find the clusters - we'll go over this later in the lesson. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is Hierarchical Clustering Different from K-Means Clustering?\n",
    "\n",
    "![](https://snag.gy/tfzWw6.jpg)\n",
    "\n",
    "Much like we learned about k-means clustering, hierarchical clustering is another method for classifying our data. If you recall, in k-means clustering, the algorithm groups data into a pre-defined set of clusters based on assigning the closest points to centroids, calculating the geometric mean of classified points, then moving the centroid until no points change class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However in the case of hierarchical clustering, the algorithm builds classifications trees of the data that merges groups of similar data points.**\n",
    "\n",
    "With k-means, the boundaries between the various clusters are distinct and independent (see graph), whereas in hierarchical clustering, there are shared similarities between those groups that are represented by the classification tree.  Going further - unlike with k-means, hierarchical clustering does not require you to define \"k\" as an input.\n",
    "\n",
    "![kmeans](../assets/images/kmeans.png)\n",
    "\n",
    "All of these attributes can lend themselves to certain clustering situations - for instance, hierarchical clustering is more beneficial for smaller datasets - think about the complexity of a dendrogram from a 1000 point dataset! Likewise, this form of clustering works better when we have binary data or dummy variables: as k-means computes *means* in forming clusters, performing k-means on a dataset with a significant amount of variables would skew the resulting clusters and distributions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference?\n",
    "\n",
    "**Generally, K-Means looks to achieve seperation**\n",
    "- Partitions are independent of each other\n",
    "\n",
    "**Hierachical Clustering**\n",
    "- Partitions can be visualized using a tree structure (a dendrogram)\n",
    "- Does not need the number of clusters as input (no random initialization)\n",
    "- Possible to view partitions at different levels of granularities (i.e., can refine/coarsen clusters) using different K\n",
    "- Will converge at the same solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## How Does Hierarchical Clustering Work? - Demo (10 mins)\n",
    "\n",
    "In hierarchical clustering, instead of clustering in one step, the clusters are determined in the a varying number of partitions. At each step, it makes the best choice based on the surrounding datapoints, with the ultimate goal that these best choices will lead to the best choice of clusters overall. Given the algorithm's method of calculating linkages based on immediate datapoints, it's known as a **greedy algorithm**.\n",
    "\n",
    "There are two forms of hierarchical clustering; **agglomerative hierarchical clustering** and **divisive hierarchical clustering**.\n",
    "\n",
    "![](../assets/images/hier.png)\n",
    "\n",
    "Today, we're going to look at one of the most fundamental methods for agglomerative hierarchical cluster, known as **linkage clustering**. Linkage clustering iterates through datapoints and computes the distance between groups by computing the distance between two neighboring datapoints, using the **nearest neighbor** technique similar to KNN.\n",
    "\n",
    "### Divisive\n",
    "- Start at the trunk of the tree and build the leaves\n",
    "\n",
    "### Agglomerative\n",
    "- Start with the leaves of the tree and build the trunk\n",
    "\n",
    "_A **greedy algorithm** is an algorithm that follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Single Linkage\" Step by Step\n",
    "(Aggomerative)\n",
    "\n",
    "\n",
    "Also known as \"single linkage\", minimum distance clustering or nearest neighbor clustering.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>![](https://snag.gy/SDZyGz.jpg)</td>\n",
    "        <td>Distance between two clusters is defined by the minimum distance between objects of the two clusters. \n",
    "</td>\n",
    "    </tr>\n",
    "        \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### First let's consider a single series of distances between X/Y points in 2D space, represented as a matrix.\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>![](https://snag.gy/EcRNns.jpg)</td>\n",
    "        <td>\n",
    "        <ul><li>_Each feature, A-F, would be considered a \"cluster\".  All points are clusters._</li>\n",
    "        <li>In each step of the iteration, we find the closest pair clusters.</li>\n",
    "        <li>Our end goal is to ultimately cluster all of these to one single cluster.</li>\n",
    "        <li>**In this case, the closest cluster is between cluster F and D with shortest distance of 0.5.**</li>\n",
    "        </ul>\n",
    "        <br>\n",
    "        **Thus, we group cluster D and F into cluster (D, F)**\n",
    "        </td>\n",
    "    </tr>\n",
    "        \n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"350\">![](https://snag.gy/siCURp.jpg)</td>\n",
    "        <td>\n",
    "        <li>D and F turn into a cluster</li>\n",
    "        <li>Distance matrix is updated (distance between ungrouped clusters do not change)</li>\n",
    "        <br>\n",
    "        **Now the problem is how to calculate distance between newly grouped clusters (D, F) and other clusters?**\n",
    "        </td>\n",
    "    </tr>\n",
    "        \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"350\">![](https://snag.gy/syM7BH.jpg)</td>\n",
    "        <td>**Cluster B and cluster A is now 0.71, wich creats cluster name \"(A, B)\"**\n",
    "        <br>\n",
    "        <ul>\n",
    "            <li>Now we update the distance matrix. </li>\n",
    "            <li>Using the **original** input distance matrix (size 6 by 6), distance between cluster C and cluster (D, F) is computed as </li>\n",
    "            <ol>$d_{(c)\\rightarrow (a,b)} = min(d_{CA}, d_{CB}) = min(5.66, 4.95) = 4.95$<br><br></ol>\n",
    "            <li>Distance between cluster (D, F) and cluster (A, B) is the minimum distance between all objects involves in the two clusters </li>\n",
    "            <ol>$d_{(d,f)\\rightarrow (a,b)} = min(d_{DA}, d_{DB}, d_{FA}, d_{FB}) = min(2.61, 2.92, 3.20, 2.50) = 2.50$<br><br></ol>\n",
    "            <li>Now we compute $e$ and $(a,b)$</li>\n",
    "            <ol>$d(e)\\rightarrow (a,b) = min(d_{E}, d_{AB}) = min(4.24, 3.54) = 3.54$</ol>\n",
    "        </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"350\">![](https://snag.gy/Tg4V6J.jpg)</td>\n",
    "        <td>**Here's our updated distance matrix**\n",
    "        <ol>$d_{((D,F),E)\\rightarrow(AB)} = MIN(d_{DA},d_{DB},d_{FA},d_{FB},d_{EB}) = MIN(3.61, 2.92, 3.20, 2.50, 4.24, 3.54) = 2.50$</ol>\n",
    "        <ol>$d_{(D,F), E)\\rightarrow C} = MIN(d_{DF}, d_{FC}, d_{EC}) = MIN(2.24,2.50,1.41) = 1.41$</ol>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://snag.gy/lrdR8b.jpg)\n",
    "<ol>$d_{(((D,F), E),C)\\rightarrow(A, B)} = MIN(d_{DA},d_{DB},d_{FA}, d_{FB}, d_{EA}, d_{EB}, d_{CA}, d_{CB})$</ol>\n",
    "<ol>$d_{(((D,F), E),C)\\rightarrow(A, B)} = MIN(3.61,2.92,3.20,2.50,4.24,3.54, 5.66, 4.95) = 2.50$</ol>\n",
    "\n",
    "<ol>\n",
    "\n",
    "          <li>In the beginning we have 6 clusters: A, B, C, D, E and F </li>\n",
    "\n",
    "          <li>We merge cluster D and F into cluster (D, F) at distance<strong> 0.50</strong> </li>\n",
    "\n",
    "          <li>We merge cluster A and cluster B into (A, B) at distance <strong>0.71</strong> </li>\n",
    "\n",
    "          <li>We merge cluster E and (D, F) into ((D, F), E) at distance <strong>1.00</strong> </li>\n",
    "\n",
    "          <li>We merge cluster ((D, F), E) and C into (((D, F), E), C) at distance <strong>1.41</strong> </li>\n",
    "\n",
    "          <li>We merge cluster (((D, F), E), C) and (A, B) into ((((D, F), E), C), (A, B)) at distance <strong>2.50</strong> </li>\n",
    "\n",
    "          <li>The last cluster contain all the objects, thus conclude the computation </li>\n",
    "\n",
    "</ol>\n",
    "\n",
    "<br>\n",
    "<center>**Our final result, can be represented in terms of a dendogram such as:**</center>\n",
    "\n",
    "![](https://snag.gy/NJ2lxe.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering in Python\n",
    "\n",
    "Implementing hierarchical clustering in python is as simple as calling a function from the SciPy toolbox:\n",
    "\n",
    "```python\n",
    "Z = linkage(X, 'ward')\n",
    "```\n",
    "\n",
    "Here, \"X\" represents the matrix of data that we are clustering, and \"ward\" tells our algorithm which method to use to calculate distance between our newly formed clusters - in this case **Ward's Method** which seeks to minimize the variance when forming clusters. When calculating distance, the default is **Euclidean distance**.\n",
    "\n",
    "After we cluster, we can calculate the dendrogram using a simple ```dendrogram()``` function from SciPy, which we can then draw using our handy  ```plt``` from matplotlib. \n",
    "\n",
    "To check how well our algorithm has measured distance, we can calculate the **cophenetic correlation coefficient**. This metric, which measures the height of the dendrogram at the point where two branches merge, can tell us how well the dendrogram has measured the distance between data points in the original dataset and is a helpful measure to see how well our clustering test has run. \n",
    "\n",
    "```python\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "```\n",
    "\n",
    "Here, we call the cophenetic function using ```cophenet``` from SciPy and apply it to our clustered set, Z, and the distance of our original set, X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Briefly:  Cophenetic Coefficient Intuition\n",
    "\n",
    "$$\n",
    "c = \\frac {\\sum_{i<j} (x(i,j) - \\bar{x})(t(i,j) - \\bar{t})}{\\sqrt{[\\sum_{i<j}(x(i,j)-\\bar{x})^2] [\\sum_{i<j}(t(i,j)-\\bar{t})^2]}}.\n",
    "$$[Detailed Cophenetic Coefficient Calculation](https://en.wikipedia.org/wiki/Cophenetic_correlation#Calculating_the_cophenetic_correlation_coefficient)\n",
    "\n",
    "- Based on interpoint distance within clusters\n",
    "- Considers $MIN(C_i)$ when looking at distance between clusters (product moment correlation)\n",
    "- Values closer to $1$ are considered good in terms of \"fusion\" (how well clusters sit with each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "X = df.as_matrix(columns=None)\n",
    "Z = linkage(X, 'ward')\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "\n",
    "def plot_dendogram(df):\n",
    "    \n",
    "    # Data prep\n",
    "    X = df.as_matrix(columns=None)\n",
    "    Z = linkage(X, 'ward')\n",
    "    \n",
    "    # plotting\n",
    "    plt.title('Dendrogram')\n",
    "    plt.xlabel('Index Numbers')\n",
    "    plt.ylabel('Distance')\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        leaf_rotation=90.,  \n",
    "        leaf_font_size=8.,\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plot_dendogram(lang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A16: Density-based spatial clustering of applications with noise (DBSCAN)\n",
    "# Week 8 (1.3)\n",
    "\n",
    "---\n",
    "\n",
    "DBSCAN is a clustering algorithm that groups datapoints together based on \"density\". Nearby points get assigned to a common cluster, and outlier points get assigned to their own clusters. DBSCAN is effective and attractive for its simplicity and minimal pre-specified parameters.\n",
    "\n",
    "There are only two parameters that need to be specified for DBSCAN:\n",
    "\n",
    "    eps : a minimum distance between points that can define a \"connection\"\n",
    "    \n",
    "    min_samples : minimum number of points that a point needs to have \n",
    "                  as neighbors to define it as a \"core sample\"\n",
    "    \n",
    "**Core samples** are by design the points that lie internally within a cluster. \n",
    "\n",
    "**Non-core samples** do not meet the minimum required neighboring points, but are still connected to a cluster defined by a core sample or samples. Hence these points lie on the edges of a cluster.\n",
    "\n",
    "**Outliers** are points that do not meet the distance criteria to a cluster nor minimum neighbors to form a new cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## DBSCAN algorithm\n",
    "\n",
    "The DBSCAN algorithm proceeds iteratively through the points, determining via the distance measure and minimum samples specified whether points are core samples, edge samples, or outliers.\n",
    "\n",
    "Here is the pseudocode algorithm below, which we will be coding up ourselves:\n",
    "\n",
    "\n",
    "```\n",
    "DBSCAN(D, eps, MinPts) {\n",
    "   C = 0\n",
    "   for each point P in dataset D {\n",
    "      if P is visited\n",
    "         continue next point\n",
    "      mark P as visited\n",
    "      NeighborPts = regionQuery(P, eps)\n",
    "      if sizeof(NeighborPts) < MinPts\n",
    "         mark P as NOISE\n",
    "      else {\n",
    "         C = next cluster\n",
    "         expandCluster(P, NeighborPts, C, eps, MinPts)\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "expandCluster(P, NeighborPts, C, eps, MinPts) {\n",
    "   add P to cluster C\n",
    "   for each point P' in NeighborPts { \n",
    "      if P' is not visited {\n",
    "         mark P' as visited\n",
    "         NeighborPts' = regionQuery(P', eps)\n",
    "         if sizeof(NeighborPts') >= MinPts\n",
    "            NeighborPts = NeighborPts joined with NeighborPts'\n",
    "      }\n",
    "      if P' is not yet member of any cluster\n",
    "         add P' to cluster C\n",
    "   }\n",
    "}\n",
    "\n",
    "regionQuery(P, eps)\n",
    "   return all points within P's eps-neighborhood (including P)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## DBSCAN in parts\n",
    "\n",
    "We can roll our own DBSCAN following the pseudcode above. Doing it piece by piece in parts will make it clear how the algorithm works.\n",
    "\n",
    "### 1. Create some clustered data\n",
    "\n",
    "sklearn has some nice data generation functions in its `sklearn.datasets` module. I've loaded a handful of them below. You can use them to create clustered data easily to test clustering algorithms.\n",
    "\n",
    "Generate clustered data and plot it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Make the skeleton of the DBSCAN class\n",
    "\n",
    "Start laying out the blueprint for how DBSCAN will work. We'll need to start out:\n",
    "\n",
    "1. An `__init__` function to initialize the class with the `eps` and `min_samples` arguments.\n",
    "- A (for now empty) `fit` function that will run DBSCAN on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Writing the `fit` function (equivalent to the `DBSCAN` function in pseudocode)\n",
    "\n",
    "Our `fit` function will follow the logic of the `DBSCAN` function in the pseudocode above (re-pasted here). In general, when building classes, think about what variables are best suited to be class attributes. It takes practice to get a feel for it.\n",
    "\n",
    "```\n",
    "DBSCAN(D, eps, MinPts) {\n",
    "   C = 0\n",
    "   for each point P in dataset D {\n",
    "      if P is visited\n",
    "         continue next point\n",
    "      mark P as visited\n",
    "      NeighborPts = regionQuery(P, eps)\n",
    "      if sizeof(NeighborPts) < MinPts\n",
    "         mark P as NOISE\n",
    "      else {\n",
    "         C = next cluster\n",
    "         expandCluster(P, NeighborPts, C, eps, MinPts)\n",
    "      }\n",
    "   }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Write the function to find neighbors\n",
    "\n",
    "We need to convert this function in the pseudocode to a class function:\n",
    "\n",
    "```\n",
    "regionQuery(P, eps)\n",
    "   return all points within P's eps-neighborhood (including P)\n",
    "```\n",
    "\n",
    "I've already named this `self.find_region_points(i)` so far, so we'll call it that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Write the function to expand the clusters.\n",
    "\n",
    "The final function (and the one that actually assigns our clusters) is defined by the pseudocode as:\n",
    "\n",
    "```\n",
    "expandCluster(P, NeighborPts, C, eps, MinPts) {\n",
    "   add P to cluster C\n",
    "   for each point P' in NeighborPts { \n",
    "      if P' is not visited {\n",
    "         mark P' as visited\n",
    "         NeighborPts' = regionQuery(P', eps)\n",
    "         if sizeof(NeighborPts') >= MinPts\n",
    "            NeighborPts = NeighborPts joined with NeighborPts'\n",
    "      }\n",
    "      if P' is not yet member of any cluster\n",
    "         add P' to cluster C\n",
    "   }\n",
    "}\n",
    "```\n",
    "\n",
    "Essentially the function takes a point id, the neighboring point ids, the cluster number, minimum distance, and minimum points, and figures out based on those components what cluster a point should be in. \n",
    "\n",
    "I've already pre-named this function in the `fit` function to be `expand_cluster(i, neighbors)`. We only need to pass in the current point and neighboring points because we're storing all of the other information the function needs as class attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "db =DBSCAN(eps=eps, min_samples=min_samples)\n",
    "db.fit(X)\n",
    "plot_db_clusters(X, db.fit_predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A17: AgglomerativeClustering\n",
    "\n",
    "Bottoms up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "ac =AgglomerativeClustering(n_clusters=n_clusters)\n",
    "ac.fit(X)\n",
    "plot_db_clusters(X, ac.fit_predict(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A18: Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "MCMC is a way to find the posterior distribution through Bayes Theorem without needing to solve the denominator of the equation, $P(data)$.\n",
    "\n",
    "To review, Bayes Theorem states:\n",
    "\n",
    "### $$ P(model\\;|\\;data) = \\frac{P(data\\;|\\;model)}{P(data)}\\;P(model) $$\n",
    "\n",
    "Where $P(model\\;|\\;data)$ is the **posterior distribution of the model**\n",
    "\n",
    "$P(data\\;|\\;model)$ is the **likelihood of the data given the model**\n",
    "\n",
    "$P(data)$ is the **total probability of the data**, or the probability of the data occurring across all models\n",
    "\n",
    "and $P(model)$ is the **prior distribution of the model**\n",
    "\n",
    "---\n",
    "\n",
    "### When solving Bayes Theorem is intractable...\n",
    "\n",
    "Bayes Theorem is designed to \"update\" our prior distribution to a posterior distribution, and both of those distributions are probability density functions.\n",
    "\n",
    "We've been looking at situations like coin flips where solving the theorem actually has an equation for the different components, but there are many situations where the posterior cannot be directly solved.\n",
    "\n",
    "With $P(data)$, for example, we need to get the likelihood of the data across _all possible models_. With a coin flip, we can integrate between 0 and 1 for all possible weightings of the coin, but what about something more complicated like the number of minutes a flight is delayed?\n",
    "\n",
    "MCMC is designed to let us still get the posterior distribution estimate without needing to actually solve the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Log likelihood of the data\n",
    "\n",
    "Recall that we have a component of Bayes Theorem that represents the likelihood of the data given our model:\n",
    "\n",
    "### $$ P(data\\;|\\;model) $$\n",
    "\n",
    "What is this representing? **Our \"model\" in this case is the distribution of the possible mean delays.** We are modeling delay times with a distribution of possible mean delay times and nothing more. It is not as complicated a model as a regression with multiple predictors, but a model nonetheless.\n",
    "\n",
    "To calculate the likelihood of the data given our model we need to ask: how likely would a datapoint (delay) be to occur according to our model?\n",
    "\n",
    "Below I'll write a function to calculate the sum of the log of probability densities at each of our delay data points. This will be the aggregate likelihood across all the delays.\n",
    "\n",
    "Why the log of the probability densities? Often likelihoods are so tiny that the computer has a difficult time working with the numbers. Converting these to log format puts them in terms of orders of magnitude and makes sure the computer doesn't break doing the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Checking proposals about different delays\n",
    "\n",
    "MCMC at the core is all about checking different likelihoods of the data (and your prior) for different versions of your model. \n",
    "\n",
    "Right now we believe that the mean delay is 20. How much would the likelihood $P(data\\;|\\;model)$ change if that mean were instead 30? Would this likelihood be larger or smaller?\n",
    "\n",
    "We can use the likelihood function we wrote above to check any new parameter, but we need a system for making new proposals. In MCMC the parameter is moved at random in small increments. In this case we will draw a new number from a normal distribution with a mean that is our current value and small standard deviation. This will either move the distribution slightly to the left or to the right.\n",
    "\n",
    "In this example we're only going to explore different possibilities for the mean and leave the standard deviation of the delay fixed, but really any parameter can be allowed to vary except for the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Metropolis-Hastings sampler\n",
    "\n",
    "Now we can actually start to do the core MCMC process. You've seen in pymc3 the NUTS sampler, which is a more advanced way of doing MCMC. We are going to use the original sampling process called Metropolis-Hastings.\n",
    "\n",
    "How do we get rid of the denominator in Bayes equation, $P(data)$, that makes problems impossible to compute?\n",
    "\n",
    "Let's consider two different specific proposals for the mean delay, model1 and model2. We can write out the equations for the posterior probability of both when we consider those proposals against the data:\n",
    "\n",
    "### $$ P(model_1\\;|\\;data) = \\frac{P(data\\;|\\;model_1)}{P(data)}\\;P(model_1) $$\n",
    "\n",
    "### $$ P(model_2\\;|\\;data) = \\frac{P(data\\;|\\;model_2)}{P(data)}\\;P(model_2) $$\n",
    "\n",
    "MCMC uses the idea that we can compare the _relative likelihoods_ of each model by dividing one by the other:\n",
    "\n",
    "### $$ \\frac{\\frac{P(data\\;|\\;model_1)\\;P(model_1)}{P(data)}}{\\frac{P(data\\;|\\;model_2)\\;P(model_2)}{P(data)}}  = \\frac{P(data\\;|\\;model_1)\\;P(model_1)}{P(data\\;|\\;model_2)\\;P(model_2)} $$\n",
    "\n",
    "Which is the same as saying we are dividing the posterior of one of the models by the posterior of the other model. This cancels out the $P(data)$ component that we didn't have any equation to compute before.\n",
    "\n",
    "But this doesn't get us all of the way there yet. The part that makes MCMC work for finding the posterior density is that it uses this equation above to _visit models with relatively higher likelihood more often._ \n",
    "\n",
    "**The acceptance criterion:**\n",
    "\n",
    "Once you have the relative likelihood of each model given the data, you can compare that to a random number between 0 and 1:\n",
    "\n",
    "    acceptance_criterion = (proposal_likelihood / current_likelihood)\n",
    "    or\n",
    "    acceptance_criterion = e^(proposal_log_likelihood - current_log_likelihood)\n",
    "    \n",
    "    if acceptance_criterion > 1:\n",
    "        accept the proposed model\n",
    "    else:\n",
    "    \n",
    "        random_fraction = np.random.rand()  # lies between 0.0 and 1.0\n",
    "\n",
    "        if acceptance_criterion > random_fraction:\n",
    "            accept the proposed model\n",
    "        else:\n",
    "            accept the current model\n",
    "        \n",
    "If the proposed model has a _higher_ likelihood than the current model, the acceptance criterion will always be above one and it will always accept the proposed model as the new current model.\n",
    "\n",
    "If the proposed model has a _lower_ likelihood than the current model, then it has a chance of becoming the new current model with a probability proportional to relatively how much lower its likelihood is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### MCMC\n",
    "\n",
    "For the final MCMC function, we just need to supply:\n",
    "\n",
    "1. the data\n",
    "2. the initial information about our model (in this case, the initial mean delay proposal)\n",
    "3. the information about the prior distribution\n",
    "4. the number of iterations\n",
    "\n",
    "It will wrap the Metropolis-Hastings sampler and run it on each iteration. Recall that the sampler will return the current accepted model (mean). We append this to the list of posterior means and use it as the new current mean for the next iteration of the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
