{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from lda2vec import preprocess, Corpus\n",
    "from topicModelingClass import topicModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>company</th>\n",
       "      <th>jobdesc</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>title</th>\n",
       "      <th>sourcesite</th>\n",
       "      <th>views</th>\n",
       "      <th>days_posted</th>\n",
       "      <th>post_start_date</th>\n",
       "      <th>link</th>\n",
       "      <th>base_title</th>\n",
       "      <th>parsed_title</th>\n",
       "      <th>parsed_title_i</th>\n",
       "      <th>expanded_title</th>\n",
       "      <th>prefix_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Advocate Health Care</td>\n",
       "      <td>\\nAs part of Advocate Health Care, Advocate Ch...</td>\n",
       "      <td>Oak Lawn</td>\n",
       "      <td>IL</td>\n",
       "      <td>Clinical Practice Specialist - 4 Hope</td>\n",
       "      <td>ind</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.indeed.com/viewjob?jk=6244f7f3a4861...</td>\n",
       "      <td>specialist</td>\n",
       "      <td>[clinical, practice, specialist, , , 4, hope]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>practice specialist</td>\n",
       "      <td>practice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>University of Washington Medical Center</td>\n",
       "      <td>\\nThe University of Washington (UW) is proud t...</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>WEB DEVELOPER</td>\n",
       "      <td>ind</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.indeed.com/viewjob?jk=3d6a9f18f5301...</td>\n",
       "      <td>developer</td>\n",
       "      <td>[web, developer]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>web developer</td>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                  company  \\\n",
       "0      0                     Advocate Health Care   \n",
       "1      1  University of Washington Medical Center   \n",
       "\n",
       "                                             jobdesc      city state  \\\n",
       "0  \\nAs part of Advocate Health Care, Advocate Ch...  Oak Lawn    IL   \n",
       "1  \\nThe University of Washington (UW) is proud t...   Seattle    WA   \n",
       "\n",
       "                                   title sourcesite  views days_posted  \\\n",
       "0  Clinical Practice Specialist - 4 Hope        ind      0         NaN   \n",
       "1                          WEB DEVELOPER        ind      0         NaN   \n",
       "\n",
       "  post_start_date                                               link  \\\n",
       "0             NaN  http://www.indeed.com/viewjob?jk=6244f7f3a4861...   \n",
       "1             NaN  http://www.indeed.com/viewjob?jk=3d6a9f18f5301...   \n",
       "\n",
       "   base_title                                   parsed_title parsed_title_i  \\\n",
       "0  specialist  [clinical, practice, specialist, , , 4, hope]            [2]   \n",
       "1   developer                               [web, developer]            [1]   \n",
       "\n",
       "        expanded_title prefix_title  \n",
       "0  practice specialist     practice  \n",
       "1        web developer          web  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('master_total_df.p','rb') as f:\n",
    "    master_total_df = pickle.load(f)\n",
    "master_total_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alltext = master_total_df['jobdesc'].values\n",
    "def uniuncode(x):\n",
    "    try:\n",
    "        return unicode(x.decode('utf-8')).replace('\\n',' -').lower()\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            return unicode(x).replace('\\n',' -').lower()\n",
    "        except:\n",
    "            print x\n",
    "            return x \n",
    "sometext = [uniuncode(x) for x in alltext[:100]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. tokens made\n",
      "2. corpus updated\n",
      "3. corpus compacted\n",
      "4. bag of words (BOW) made\n",
      "5. corpus flattened\n",
      "2 <SKIP>  -->  SKIP\n",
      "3 ,  -->  上\n",
      "5 -  -->  上\n",
      "6 .  -->  上\n",
      "15 :  -->  上\n",
      "24 )  -->  上\n",
      "26 (  -->  上\n",
      "35 -•  -->  -4\n",
      "36   -->  上\n",
      "40 ’s  -->  qs\n",
      "50 ;  -->  上\n",
      "79 -experience  -->  experience\n",
      "82 -the  -->  dthe\n",
      "112 /  -->  上\n",
      "115 -ability  -->  ability\n",
      "133 -strong  -->  strong\n",
      "143 and/or  -->  andor\n",
      "151 ?  -->  上\n",
      "162 -30+  -->  -0\n",
      "203 –  -->  上\n",
      "245 -we  -->  iwe\n",
      "257 -work  -->  work\n",
      "263 -job  -->  job\n",
      "266 --  -->  -4\n",
      "295 !  -->  上\n",
      "303 -a  -->  qa\n",
      "304 -excellent  -->  excellent\n",
      "307 ’  -->  上\n",
      "321 you’ll  -->  youll\n",
      "327 -this  -->  Tthis\n",
      "334 -you  -->  Ryou\n",
      "354 -bachelor  -->  bachelor\n",
      "368 -what  -->  whatā\n",
      "383 '  -->  上\n",
      "386 -qualifications  -->  qualifications\n",
      "387 -responsibilities  -->  responsibilities\n",
      "393 -about  -->  about\n",
      "394 -at  -->  qat\n",
      "456 -as  -->  Das\n",
      "463 \"  -->  上\n",
      "492 -  -->  -4\n",
      "497 -develop  -->  develop\n",
      "502 -knowledge  -->  aknowledge\n",
      "523 -working  -->  working\n",
      "526 -our  -->  Touré\n",
      "546 -responsible  -->  responsible\n",
      "547 -provide  -->  provide\n",
      "548 e.g.  -->  eg.\n",
      "554 -·  -->  -4\n",
      "557 -to  -->  eto\n",
      "560 -required  -->  required\n",
      "561 -must  -->  must\n",
      "593 -5+  -->  -5\n",
      "596 -requirements  -->  requirements\n",
      "605 10  -->  -0\n",
      "630 we’re  -->  werre\n",
      "634 -understanding  -->  understanding\n",
      "635 -design  -->  design\n",
      "657 “  -->  上\n",
      "678 -preferred  -->  preferred\n",
      "683 -proficiency  -->  proficiency\n",
      "711 psv  -->  pov\n",
      "724 -solid  -->  solid\n",
      "729 -lead  -->  lead\n",
      "731 -demonstrated  -->  demonstrated\n",
      "745 100  -->  cw0\n",
      "748 -manage  -->  manage\n",
      "755 ”  -->  上\n",
      "758 -education  -->  education\n",
      "782 tmx  -->  tsx\n",
      "805 -assist  -->  bassist\n",
      "821 2015  -->  Y1A\n",
      "834 -drive  -->  Gdrive\n",
      "843 -data  -->  Xdata\n",
      "850 -support  -->  support\n",
      "881 -provides  -->  provides\n",
      "923 -3+  -->  -3\n",
      "924 -use  -->  Fuse\n",
      "925 -advanced  -->  advanced\n",
      "927 -an  -->  Dan\n",
      "932 cacs  -->  calcs\n",
      "936 2016  -->  Y1A\n",
      "993 -collaborate  -->  collaborate\n",
      "1035 -location  -->  location\n",
      "1040 -who  -->  who\n",
      "1041 -good  -->  good\n",
      "1045 -basic  -->  basic\n",
      "1051 -1+  -->  -1\n",
      "1054 -maintain  -->  maintain\n",
      "1057 -deloitte  -->  Deloitte\n",
      "1058 -technical  -->  technical\n",
      "1097 -key  -->  Ikey\n",
      "1101 i.e.  -->  ie.\n",
      "1104 2014  -->  Y1A\n",
      "1105 -perform  -->  perform\n",
      "1106 -comfortable  -->  comfortable\n",
      "1107 -primary  -->  primary\n",
      "1108 -minimum  -->  minimum\n",
      "1110 -desired  -->  desired\n",
      "1112 -proven  -->  proven\n",
      "1115 -additional  -->  additional\n",
      "1141 -position  -->  position\n",
      "1162 dmi  -->  dm1\n",
      "1172 c#  -->  clé\n",
      "1178 -full  -->  full\n",
      "1188 -create  -->  create\n",
      "1196 -microsoft  -->  microsoft\n",
      "1204 -execute  -->  execute\n",
      "1206 -new  -->  knew\n",
      "1212 20  -->  2_\n",
      "1252 -highly  -->  highly\n",
      "1254 -participate  -->  participate\n",
      "1256 jibestream  -->  lifestream\n",
      "1263 -microstrategy  -->  Microstrategy\n",
      "1272 -2+  -->  -2\n",
      "1276 -ensure  -->  ensure\n",
      "1322 kpis  -->  kips\n",
      "1341 -of  -->  Hof\n",
      "1344 -technology  -->  technology\n",
      "1345 -description  -->  description\n",
      "1349 -fluency  -->  fluency\n",
      "1350 -build  -->  ebuild\n",
      "1357 resolvit  -->  resolvin\n",
      "1388 -familiarity  -->  familiarity\n",
      "1402 40  -->  -0\n",
      "1422 -why  -->  why\n",
      "1423 [  -->  上\n",
      "1461 mongodb  -->  Tongod\n",
      "1469 -benefits  -->  benefits\n",
      "1473 -analytical  -->  analytical\n",
      "1477 -if  -->  fif\n",
      "1479 education:bachelor'srequired  -->  Education_Teachers_Kuppet\n",
      "1480 timerequired  -->  unrequired\n",
      "1482 -passion  -->  passion\n",
      "1486 -business  -->  business\n",
      "1491 -salary  -->  salary\n",
      "1497 loyaltyone  -->  LoyaltyOne\n",
      "1509 -and  -->  andÂ\n",
      "1510 -self  -->  self\n",
      "1517 -are  -->  Care\n",
      "1529 -physical  -->  physical\n",
      "1541 -join  -->  join\n",
      "1549 you’re  -->  youíre\n",
      "1559 -high  -->  ahigh\n",
      "1567 -project  -->  project\n",
      "1569 -team  -->  Ateam\n",
      "1572 -12  -->  -1\n",
      "1574 e.g  -->  edg\n",
      "1579 -help  -->  Phelp\n",
      "1581 fixstream  -->  Bitstream\n",
      "1582 hcm  -->  1cm\n",
      "1586 he/she  -->  Deshe\n",
      "1591 hdfs  -->  dfs\n",
      "1593 -have  -->  Shave\n",
      "1602 206  -->  2_\n",
      "1606 -system  -->  system\n",
      "1615 5+  -->  +\n",
      "1626 -sap  -->  sap\n",
      "1628 -willingness  -->  willingness\n",
      "1630 -all  -->  Gall\n",
      "1690 -health  -->  mhealth\n",
      "1696 50  -->  -0\n",
      "1720 nosql  -->  nobel\n",
      "1723 -consultants  -->  consultants\n",
      "1726 -content  -->  econtent\n",
      "1743 543  -->  IR3\n",
      "1750 -training  -->  training\n",
      "1752 -skill  -->  Askill\n",
      "1753 -works  -->  nworks\n",
      "1755 -chegg  -->  Tcheng\n",
      "1756 -serve  -->  serve\n",
      "1758 -information  -->  information\n",
      "1759 -essential  -->  essential\n",
      "1760 -document  -->  document\n",
      "1765 -analytics  -->  analytics\n",
      "1766 -application  -->  application\n",
      "1771 -demonstrates  -->  demonstrates\n",
      "1772 -developing  -->  developing\n",
      "1773 -email  -->  Femail\n",
      "1775 -skills  -->  skills\n",
      "1776 -database  -->  database\n",
      "1782 -graduate  -->  graduate\n",
      "1784 mvc  -->  moc\n",
      "1802 -in  -->  Nin\n",
      "1817 s/he  -->  sche\n",
      "1820 hbase  -->  base\n",
      "1824 -o  -->  Oo\n",
      "1830 -one  -->  gone\n",
      "1833 170  -->  cw0\n",
      "1843 eeo  -->  eet\n",
      "1853 ‘  -->  上\n",
      "1906 spss  -->  spys\n",
      "1907 ‑  -->  上\n",
      "1912 devops  -->  Devons\n",
      "1916 -willis  -->  willis\n",
      "1923   -->  上\n",
      "1932 -hands  -->  Shands\n",
      "1933 -organizational  -->  organizational\n",
      "1942 -20  -->  -0\n",
      "1943 -your  -->  your\n",
      "1946 -please  -->  please\n",
      "1947 -able  -->  sable\n",
      "1948 -cision  -->  Décision\n",
      "1952 -note  -->  Anote\n",
      "1954   -->  上\n",
      "1957 hl7  -->  hlp\n",
      "1959 -11  -->  -1\n",
      "1960 -other  -->  Rother\n",
      "1961 spha  -->  saha\n",
      "1962 -establish  -->  restablish\n",
      "1963 -bring  -->  bring\n",
      "1974 -communication  -->  communication\n",
      "1980 -computer  -->  computer\n",
      "1984 -do  -->  do\n",
      "1991 365  -->  6d\n",
      "2003 -open  -->  open\n",
      "2005 -competitive  -->  competitive\n",
      "2013 -assists  -->  Bassists\n",
      "2015 -extensive  -->  extensive\n",
      "2017 -command  -->  command\n",
      "2020 -you’ll  -->  youll\n",
      "2022 -general  -->  general\n",
      "2024 timejob  -->  thejob\n",
      "2025 60  -->  6d\n",
      "2038 25  -->  2_\n",
      "2056 -flexible  -->  flexible\n",
      "2057 -employee  -->  employee\n",
      "2058 excel/sas  -->  excelsis\n",
      "2059 objects/ms  -->  objects\n",
      "2060 sql/business  -->  smallbusiness\n",
      "2063 -effective  -->  effective\n",
      "2071 2012  -->  Y1A\n",
      "2131 15  -->  O5\n",
      "2145 24]7  -->  2_\n",
      "2150 -updates  -->  updates\n",
      "2154 -creates  -->  creates\n",
      "2155 angularjs  -->  angular\n",
      "2157 -factset  -->  Factset\n",
      "2159 dm&a  -->  dma\n",
      "2163 ts/sci  -->  Casci\n",
      "2165 -trimble  -->  tribble\n",
      "2168 -successful  -->  successful\n",
      "2172 -duties  -->  duties\n",
      "2183 don’t  -->  donÕt\n",
      "2202 -how  -->  show\n",
      "2206 -monitor  -->  monitor\n",
      "2208 -process  -->  process\n",
      "2209 -act  -->  act\n",
      "2211 -weekly  -->  weekly\n",
      "2214 -senior  -->  senior\n",
      "2215 -oracle  -->  coracle\n",
      "2218 -communicate  -->  communicate\n",
      "2220 -define  -->  define\n",
      "2221 -equal  -->  sequal\n",
      "2222 -dignity  -->  dignity\n",
      "2224 -candidates  -->  candidates\n",
      "2228 -reporting  -->  reporting\n",
      "2229 -icf  -->  fif\n",
      "2231 cargurus  -->  CarGurus\n",
      "2234 -schedule  -->  schedule\n",
      "2238 -expert  -->  expert\n",
      "2239 -identify  -->  identify\n",
      "2241 -401(k  -->  -4_°_C\n",
      "2243 -sotera  -->  Kotera\n",
      "2244 -halfaker  -->  Halfaker\n",
      "2245 -jobs  -->  jobs\n",
      "2246 -»  -->  -4\n",
      "2247 eeod  -->  eed\n",
      "2250 -maintains  -->  maintains\n",
      "2251 -city  -->  cityâ\n",
      "2252 -you’re  -->  youíre\n",
      "2253 -significant  -->  asignificant\n",
      "2254 -exceptional  -->  exceptional\n",
      "2256 -we’re  -->  werre\n",
      "2257 -newyork  -->  newyork\n",
      "2264 30+  -->  3Rd\n",
      "2268 -customer  -->  customer\n",
      "2270 -while  -->  while\n",
      "2275 -amazon  -->  amazon\n",
      "2282 -big  -->  Ibig\n",
      "2292 -windows  -->  windows\n",
      "2294 -top  -->  atop\n",
      "2311 -regular  -->  regular\n",
      "2315 -bonus  -->  bonus\n",
      "2325 google+  -->  google\n",
      "2329 dbas  -->  obas\n",
      "2339 ultipro  -->  Altiero\n",
      "2351 -stay  -->  Estay\n",
      "2361 icf  -->  iff\n",
      "2365 5,000  -->  ,6_0\n",
      "2366 -8+  -->  -8\n",
      "2373 -40  -->  -4\n",
      "2377 -.  -->  O.\n",
      "2388 education/experience  -->  Medication_adherence\n",
      "2397 sccm  -->  scam\n",
      "2400 -miles  -->  Qmiles\n",
      "2421 -very  -->  Avery\n",
      "2456 5000  -->  0_0\n",
      "2473 -prepare  -->  prepare\n",
      "2479 -benefit  -->  benefit\n",
      "2480 2017  -->  Y1A\n",
      "2482 -*li  -->  cli\n",
      "2486 -managing  -->  managing\n",
      "2487 -ims  -->  aims\n",
      "2488 -manages  -->  manages\n",
      "2494 bs/ba  -->  bo_ba\n",
      "2498 -digital  -->  digital\n",
      "2503 -consistent  -->  consistent\n",
      "2504 -follow  -->  follow\n",
      "2505 -corporate  -->  corporate\n",
      "2506 jquery  -->  jQuery\n",
      "2510 -here  -->  where\n",
      "2511 -supporting  -->  supporting\n",
      "2513 -paid  -->  paid\n",
      "2514 -collect  -->  collect\n",
      "2515 -track  -->  Strack\n",
      "2516 -proficient  -->  proficient\n",
      "2520 -10+  -->  -1\n",
      "2522 presbyterian/weill  -->  Presbyterianism\n",
      "2525 6450  -->  6'_0\n",
      "2528 -lawson  -->  Clawson\n",
      "2529 -implementation  -->  implementation\n",
      "2535 -contax  -->  conta\n",
      "2536 -imagine  -->  imagine\n",
      "2541 -efi  -->  Refi\n",
      "2543 -functions  -->  functions\n",
      "2546 r&d  -->  Árd\n",
      "2547 srm  -->  arm\n",
      "2553 -standard  -->  standard\n",
      "2556 -performs  -->  performs\n",
      "2558 -video  -->  video\n",
      "2568 -role  -->  role\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-91d2beb7bac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mn_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mfn_wordvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../08-wordvectors-pretrained/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompact_word_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn_wordvc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'6. pretrained words loaded'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/lda2vec/corpus.py\u001b[0m in \u001b[0;36mcompact_word_vectors\u001b[0;34m(self, vocab, filename, array, top)\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0midx\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                     \u001b[0msel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdamerau_levenshtein_distance_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                     \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0;31m# choice = difflib.get_close_matches(word, choices)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpyxdameraulevenshtein.pyx\u001b[0m in \u001b[0;36mpyxdameraulevenshtein.damerau_levenshtein_distance_ndarray (pyxdameraulevenshtein/pyxdameraulevenshtein.c:2549)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpyxdameraulevenshtein.pyx\u001b[0m in \u001b[0;36mpyxdameraulevenshtein.damerau_levenshtein_distance_ndarray (pyxdameraulevenshtein/pyxdameraulevenshtein.c:2446)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2216\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2283\u001b[0m             \u001b[0;31m# Convert args to object arrays first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m             inputs = [array(_a, copy=False, subok=True, dtype=object)\n\u001b[0;32m-> 2285\u001b[0;31m                       for _a in args]\n\u001b[0m\u001b[1;32m   2286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2287\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# max words grabbed per document\n",
    "max_words = 10000\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "# convert text to unicode (if not already)\n",
    "# in my case text is already in unicode\n",
    "# tokenize uses spacy under the hood\n",
    "tokens, vocab = preprocess.tokenize(sometext, max_words, merge=False, n_threads=4)\n",
    "\n",
    "print '1. tokens made'\n",
    "# he made a generic corpus based on default dictionary\n",
    "# see documentation\n",
    "corpus = Corpus()\n",
    "\n",
    "# Make a ranked list of rare vs frequent words\n",
    "corpus.update_word_count(tokens)\n",
    "corpus.finalize()\n",
    "\n",
    "print '2. corpus updated'\n",
    "\n",
    "# The tokenization uses spaCy indices, and so may have gaps\n",
    "# between indices for words that aren't present in our dataset.\n",
    "# This builds a new compact index\n",
    "compact = corpus.to_compact(tokens)\n",
    "\n",
    "print '3. corpus compacted'\n",
    "\n",
    "\n",
    "# Remove extremely rare words\n",
    "pruned = corpus.filter_count(compact, min_count=10)\n",
    "\n",
    "# Convert the compactified arrays into bag of words arrays\n",
    "bow = corpus.compact_to_bow(pruned)\n",
    "\n",
    "print '4. bag of words (BOW) made'\n",
    "\n",
    "# Words tend to have power law frequency, so selectively\n",
    "# downsample the most prevalent words\n",
    "clean = corpus.subsample_frequent(pruned)\n",
    "doc_ids = np.arange(pruned.shape[0])\n",
    "flattened, (doc_ids,) = corpus.compact_to_flat(pruned, doc_ids)\n",
    "assert flattened.min() >= 0\n",
    "\n",
    "print '5. corpus flattened'\n",
    "\n",
    "# Fill in the pretrained word vectors\n",
    "n_dim = 300\n",
    "fn_wordvc = '../08-wordvectors-pretrained/GoogleNews-vectors-negative300.bin'\n",
    "vectors, s, f = corpus.compact_word_vectors(vocab, filename=fn_wordvc)\n",
    "\n",
    "print '6. pretrained words loaded'\n",
    "\n",
    "# Save all of the preprocessed files\n",
    "pickle.dump(vocab, open('vocab.pkl', 'w'))\n",
    "pickle.dump(corpus, open('corpus.pkl', 'w'))\n",
    "np.save(\"flattened\", flattened)\n",
    "np.save(\"doc_ids\", doc_ids)\n",
    "np.save(\"pruned\", pruned)\n",
    "np.save(\"bow\", bow)\n",
    "np.save(\"vectors\", vectors)\n",
    "\n",
    "print '7. files saved'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
