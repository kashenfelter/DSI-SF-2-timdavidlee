{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>company</th>\n",
       "      <th>jobdesc</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>title</th>\n",
       "      <th>sourcesite</th>\n",
       "      <th>views</th>\n",
       "      <th>days_posted</th>\n",
       "      <th>post_start_date</th>\n",
       "      <th>link</th>\n",
       "      <th>base_title</th>\n",
       "      <th>parsed_title</th>\n",
       "      <th>parsed_title_i</th>\n",
       "      <th>expanded_title</th>\n",
       "      <th>prefix_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Advocate Health Care</td>\n",
       "      <td>\\nAs part of Advocate Health Care, Advocate Ch...</td>\n",
       "      <td>Oak Lawn</td>\n",
       "      <td>IL</td>\n",
       "      <td>Clinical Practice Specialist - 4 Hope</td>\n",
       "      <td>ind</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.indeed.com/viewjob?jk=6244f7f3a4861...</td>\n",
       "      <td>specialist</td>\n",
       "      <td>[clinical, practice, specialist, , , 4, hope]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>practice specialist</td>\n",
       "      <td>practice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>University of Washington Medical Center</td>\n",
       "      <td>\\nThe University of Washington (UW) is proud t...</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>WEB DEVELOPER</td>\n",
       "      <td>ind</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.indeed.com/viewjob?jk=3d6a9f18f5301...</td>\n",
       "      <td>developer</td>\n",
       "      <td>[web, developer]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>web developer</td>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                  company  \\\n",
       "0      0                     Advocate Health Care   \n",
       "1      1  University of Washington Medical Center   \n",
       "\n",
       "                                             jobdesc      city state  \\\n",
       "0  \\nAs part of Advocate Health Care, Advocate Ch...  Oak Lawn    IL   \n",
       "1  \\nThe University of Washington (UW) is proud t...   Seattle    WA   \n",
       "\n",
       "                                   title sourcesite  views days_posted  \\\n",
       "0  Clinical Practice Specialist - 4 Hope        ind      0         NaN   \n",
       "1                          WEB DEVELOPER        ind      0         NaN   \n",
       "\n",
       "  post_start_date                                               link  \\\n",
       "0             NaN  http://www.indeed.com/viewjob?jk=6244f7f3a4861...   \n",
       "1             NaN  http://www.indeed.com/viewjob?jk=3d6a9f18f5301...   \n",
       "\n",
       "   base_title                                   parsed_title parsed_title_i  \\\n",
       "0  specialist  [clinical, practice, specialist, , , 4, hope]            [2]   \n",
       "1   developer                               [web, developer]            [1]   \n",
       "\n",
       "        expanded_title prefix_title  \n",
       "0  practice specialist     practice  \n",
       "1        web developer          web  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('master_total_df.p','rb') as f:\n",
    "    master_total_df = pickle.load(f)\n",
    "master_total_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolate the Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alltext = master_total_df['jobdesc'].values\n",
    "\n",
    "# sample the entire text body \n",
    "sometext = alltext[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "#vectorizer = TFIDFVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(sometext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the summary total counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data           478\n",
      "experience     406\n",
      "business       322\n",
      "work           238\n",
      "team           227\n",
      "skills         224\n",
      "ability        172\n",
      "analytics      158\n",
      "development    157\n",
      "management     152\n",
      "marketing      150\n",
      "solutions      148\n",
      "support        145\n",
      "years          132\n",
      "strong         128\n",
      "technical      125\n",
      "systems        122\n",
      "knowledge      119\n",
      "new            118\n",
      "design         113\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_X = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "summary = df_X.sum().sort_values(ascending = False)\n",
    "print summary[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Prep - setup vocab, and make Corpus from Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.003603\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "corpus = matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "vocab = {v: k for k, v in vectorizer.vocabulary_.iteritems()}\n",
    "vocab.items()[:25]\n",
    "\n",
    "print datetime.datetime.now()-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: u'00',\n",
       " 1: u'000',\n",
       " 2: u'02138',\n",
       " 3: u'10',\n",
       " 4: u'100',\n",
       " 5: u'1000',\n",
       " 6: u'11',\n",
       " 7: u'12',\n",
       " 8: u'120',\n",
       " 9: u'13',\n",
       " 10: u'139',\n",
       " 11: u'14',\n",
       " 12: u'15',\n",
       " 13: u'150',\n",
       " 14: u'1575',\n",
       " 15: u'16',\n",
       " 16: u'1606420',\n",
       " 17: u'17',\n",
       " 18: u'170',\n",
       " 19: u'18',\n",
       " 20: u'1828',\n",
       " 21: u'18m',\n",
       " 22: u'19',\n",
       " 23: u'1969',\n",
       " 24: u'1976',\n",
       " 25: u'19801required',\n",
       " 26: u'1st',\n",
       " 27: u'20',\n",
       " 28: u'200',\n",
       " 29: u'2008',\n",
       " 30: u'201',\n",
       " 31: u'2010',\n",
       " 32: u'2011',\n",
       " 33: u'2012',\n",
       " 34: u'2013',\n",
       " 35: u'2014',\n",
       " 36: u'2015',\n",
       " 37: u'2016',\n",
       " 38: u'2017',\n",
       " 39: u'2020',\n",
       " 40: u'206',\n",
       " 41: u'21',\n",
       " 42: u'210',\n",
       " 43: u'215',\n",
       " 44: u'21st',\n",
       " 45: u'23',\n",
       " 46: u'230',\n",
       " 47: u'24',\n",
       " 48: u'24x7',\n",
       " 49: u'25',\n",
       " 50: u'250',\n",
       " 51: u'26',\n",
       " 52: u'28',\n",
       " 53: u'29',\n",
       " 54: u'2b',\n",
       " 55: u'2d',\n",
       " 56: u'2nd',\n",
       " 57: u'30',\n",
       " 58: u'300',\n",
       " 59: u'30m',\n",
       " 60: u'31',\n",
       " 61: u'320',\n",
       " 62: u'35',\n",
       " 63: u'360',\n",
       " 64: u'365',\n",
       " 65: u'380',\n",
       " 66: u'39',\n",
       " 67: u'3rd',\n",
       " 68: u'40',\n",
       " 69: u'400',\n",
       " 70: u'401',\n",
       " 71: u'401k',\n",
       " 72: u'409',\n",
       " 73: u'41',\n",
       " 74: u'45',\n",
       " 75: u'47',\n",
       " 76: u'50',\n",
       " 77: u'500',\n",
       " 78: u'5000',\n",
       " 79: u'51',\n",
       " 80: u'5257',\n",
       " 81: u'53r3',\n",
       " 82: u'543',\n",
       " 83: u'55',\n",
       " 84: u'550',\n",
       " 85: u'5er',\n",
       " 86: u'5th',\n",
       " 87: u'60',\n",
       " 88: u'62',\n",
       " 89: u'6450',\n",
       " 90: u'6452',\n",
       " 91: u'648',\n",
       " 92: u'65',\n",
       " 93: u'70',\n",
       " 94: u'703',\n",
       " 95: u'723014br',\n",
       " 96: u'729168',\n",
       " 97: u'75',\n",
       " 98: u'7552',\n",
       " 99: u'80',\n",
       " 100: u'800',\n",
       " 101: u'821',\n",
       " 102: u'848',\n",
       " 103: u'8600',\n",
       " 104: u'870',\n",
       " 105: u'89',\n",
       " 106: u'92',\n",
       " 107: u'945',\n",
       " 108: u'__________________',\n",
       " 109: u'aa',\n",
       " 110: u'aaai',\n",
       " 111: u'abap',\n",
       " 112: u'abilities',\n",
       " 113: u'abilitiesbs',\n",
       " 114: u'ability',\n",
       " 115: u'able',\n",
       " 116: u'abroad',\n",
       " 117: u'absence',\n",
       " 118: u'abstractions',\n",
       " 119: u'abusive',\n",
       " 120: u'aca',\n",
       " 121: u'academic',\n",
       " 122: u'accelerate',\n",
       " 123: u'accelerated',\n",
       " 124: u'accept',\n",
       " 125: u'acceptance',\n",
       " 126: u'accepting',\n",
       " 127: u'accepts',\n",
       " 128: u'access',\n",
       " 129: u'accessibility',\n",
       " 130: u'accessible',\n",
       " 131: u'accommodate',\n",
       " 132: u'accommodation',\n",
       " 133: u'accommodations',\n",
       " 134: u'accomplish',\n",
       " 135: u'accomplished',\n",
       " 136: u'accomplishes',\n",
       " 137: u'accomplishing',\n",
       " 138: u'accomplishments',\n",
       " 139: u'accordance',\n",
       " 140: u'according',\n",
       " 141: u'accordingly',\n",
       " 142: u'account',\n",
       " 143: u'accountabilities',\n",
       " 144: u'accountability',\n",
       " 145: u'accountable',\n",
       " 146: u'accounting',\n",
       " 147: u'accounts',\n",
       " 148: u'accreditation',\n",
       " 149: u'accredited',\n",
       " 150: u'accuracy',\n",
       " 151: u'accuracyexperience',\n",
       " 152: u'accuragely',\n",
       " 153: u'accurate',\n",
       " 154: u'accurately',\n",
       " 155: u'achievable',\n",
       " 156: u'achieve',\n",
       " 157: u'achieved',\n",
       " 158: u'achieving',\n",
       " 159: u'acknowledged',\n",
       " 160: u'acquire',\n",
       " 161: u'acquires',\n",
       " 162: u'acquisition',\n",
       " 163: u'acquisitions',\n",
       " 164: u'act',\n",
       " 165: u'acting',\n",
       " 166: u'action',\n",
       " 167: u'actionable',\n",
       " 168: u'actions',\n",
       " 169: u'active',\n",
       " 170: u'actively',\n",
       " 171: u'activities',\n",
       " 172: u'activity',\n",
       " 173: u'activitycreating',\n",
       " 174: u'acts',\n",
       " 175: u'acumen',\n",
       " 176: u'acute',\n",
       " 177: u'ad',\n",
       " 178: u'ada',\n",
       " 179: u'adapt',\n",
       " 180: u'adaptable',\n",
       " 181: u'adapting',\n",
       " 182: u'add',\n",
       " 183: u'added',\n",
       " 184: u'addition',\n",
       " 185: u'additional',\n",
       " 186: u'additionally',\n",
       " 187: u'address',\n",
       " 188: u'addressed',\n",
       " 189: u'addresses',\n",
       " 190: u'addressing',\n",
       " 191: u'adds',\n",
       " 192: u'adequacy',\n",
       " 193: u'adhere',\n",
       " 194: u'adhered',\n",
       " 195: u'adherence',\n",
       " 196: u'adheres',\n",
       " 197: u'adhering',\n",
       " 198: u'adjust',\n",
       " 199: u'adjusted',\n",
       " 200: u'adjustment',\n",
       " 201: u'adm',\n",
       " 202: u'administer',\n",
       " 203: u'administering',\n",
       " 204: u'administers',\n",
       " 205: u'administration',\n",
       " 206: u'administrative',\n",
       " 207: u'administrator',\n",
       " 208: u'admired',\n",
       " 209: u'adobe',\n",
       " 210: u'adopted',\n",
       " 211: u'adopting',\n",
       " 212: u'adoption',\n",
       " 213: u'ads',\n",
       " 214: u'adsense',\n",
       " 215: u'adult',\n",
       " 216: u'advance',\n",
       " 217: u'advanced',\n",
       " 218: u'advancement',\n",
       " 219: u'advancing',\n",
       " 220: u'advantage',\n",
       " 221: u'advantageous',\n",
       " 222: u'advantages',\n",
       " 223: u'adventure',\n",
       " 224: u'advertise',\n",
       " 225: u'advertising',\n",
       " 226: u'advice',\n",
       " 227: u'advise',\n",
       " 228: u'advising',\n",
       " 229: u'advisor',\n",
       " 230: u'advisory',\n",
       " 231: u'advocacy',\n",
       " 232: u'advocate',\n",
       " 233: u'adwords',\n",
       " 234: u'aec',\n",
       " 235: u'aeronautics',\n",
       " 236: u'affecting',\n",
       " 237: u'affiliated',\n",
       " 238: u'affiliation',\n",
       " 239: u'affirmative',\n",
       " 240: u'affordable',\n",
       " 241: u'afraid',\n",
       " 242: u'age',\n",
       " 243: u'agencies',\n",
       " 244: u'agency',\n",
       " 245: u'agenda',\n",
       " 246: u'agent',\n",
       " 247: u'agents',\n",
       " 248: u'aggregate',\n",
       " 249: u'aggregation',\n",
       " 250: u'aggressively',\n",
       " 251: u'agile',\n",
       " 252: u'agility',\n",
       " 253: u'aging',\n",
       " 254: u'ago',\n",
       " 255: u'agreed',\n",
       " 256: u'agreement',\n",
       " 257: u'agreements',\n",
       " 258: u'agriclear',\n",
       " 259: u'agriculture',\n",
       " 260: u'ahead',\n",
       " 261: u'aids',\n",
       " 262: u'aim',\n",
       " 263: u'air',\n",
       " 264: u'airbnb',\n",
       " 265: u'ais',\n",
       " 266: u'ajax',\n",
       " 267: u'aka',\n",
       " 268: u'akamai',\n",
       " 269: u'al',\n",
       " 270: u'albans',\n",
       " 271: u'alexa',\n",
       " 272: u'algebra',\n",
       " 273: u'algorithms',\n",
       " 274: u'alienage',\n",
       " 275: u'align',\n",
       " 276: u'aligned',\n",
       " 277: u'alignment',\n",
       " 278: u'alike',\n",
       " 279: u'alkon',\n",
       " 280: u'alliance',\n",
       " 281: u'allocation',\n",
       " 282: u'allow',\n",
       " 283: u'allowing',\n",
       " 284: u'allows',\n",
       " 285: u'allscripts',\n",
       " 286: u'alongside',\n",
       " 287: u'alpha',\n",
       " 288: u'alphasights',\n",
       " 289: u'altegra',\n",
       " 290: u'alterex',\n",
       " 291: u'alternate',\n",
       " 292: u'alternatives',\n",
       " 293: u'alteryx',\n",
       " 294: u'alto',\n",
       " 295: u'amazing',\n",
       " 296: u'amazon',\n",
       " 297: u'ambari',\n",
       " 298: u'ambiguity',\n",
       " 299: u'ambiguous',\n",
       " 300: u'ambition',\n",
       " 301: u'ambitious',\n",
       " 302: u'ambulatory',\n",
       " 303: u'amend',\n",
       " 304: u'america',\n",
       " 305: u'amex',\n",
       " 306: u'aml',\n",
       " 307: u'amounts',\n",
       " 308: u'amrd598',\n",
       " 309: u'analogy',\n",
       " 310: u'analyse',\n",
       " 311: u'analyses',\n",
       " 312: u'analysesability',\n",
       " 313: u'analysing',\n",
       " 314: u'analysis',\n",
       " 315: u'analyst',\n",
       " 316: u'analysts',\n",
       " 317: u'analytic',\n",
       " 318: u'analytical',\n",
       " 319: u'analytically',\n",
       " 320: u'analytics',\n",
       " 321: u'analyticspull',\n",
       " 322: u'analyticsseo',\n",
       " 323: u'analyze',\n",
       " 324: u'analyzing',\n",
       " 325: u'ancestry',\n",
       " 326: u'ancillary',\n",
       " 327: u'andanalysis',\n",
       " 328: u'andapplying',\n",
       " 329: u'anddeliverables',\n",
       " 330: u'andinsightful',\n",
       " 331: u'android',\n",
       " 332: u'angeles',\n",
       " 333: u'angular',\n",
       " 334: u'angularjs',\n",
       " 335: u'annual',\n",
       " 336: u'annually',\n",
       " 337: u'annuities',\n",
       " 338: u'ansible',\n",
       " 339: u'answer',\n",
       " 340: u'answered',\n",
       " 341: u'answering',\n",
       " 342: u'anticipating',\n",
       " 343: u'antivirus',\n",
       " 344: u'anymore',\n",
       " 345: u'aol',\n",
       " 346: u'apache',\n",
       " 347: u'apart',\n",
       " 348: u'apartwe',\n",
       " 349: u'api',\n",
       " 350: u'apis',\n",
       " 351: u'apm',\n",
       " 352: u'app',\n",
       " 353: u'appear',\n",
       " 354: u'appears',\n",
       " 355: u'appetite',\n",
       " 356: u'applicable',\n",
       " 357: u'applicant',\n",
       " 358: u'applicants',\n",
       " 359: u'application',\n",
       " 360: u'applications',\n",
       " 361: u'applicationsproficiency',\n",
       " 362: u'applied',\n",
       " 363: u'applies',\n",
       " 364: u'apply',\n",
       " 365: u'applying',\n",
       " 366: u'appointment',\n",
       " 367: u'appreciate',\n",
       " 368: u'appreciated',\n",
       " 369: u'apprise',\n",
       " 370: u'approach',\n",
       " 371: u'approaches',\n",
       " 372: u'appropriate',\n",
       " 373: u'appropriately',\n",
       " 374: u'approval',\n",
       " 375: u'approvals',\n",
       " 376: u'approved',\n",
       " 377: u'approximately',\n",
       " 378: u'apps',\n",
       " 379: u'aptitude',\n",
       " 380: u'architect',\n",
       " 381: u'architectfairfax',\n",
       " 382: u'architecting',\n",
       " 383: u'architects',\n",
       " 384: u'architectural',\n",
       " 385: u'architecture',\n",
       " 386: u'architectures',\n",
       " 387: u'area',\n",
       " 388: u'areadecision',\n",
       " 389: u'arealphasights',\n",
       " 390: u'areas',\n",
       " 391: u'aredigital',\n",
       " 392: u'aredo',\n",
       " 393: u'areyou',\n",
       " 394: u'arlington',\n",
       " 395: u'arms',\n",
       " 396: u'arrangements',\n",
       " 397: u'array',\n",
       " 398: u'art',\n",
       " 399: u'articles',\n",
       " 400: u'articulate',\n",
       " 401: u'articulates',\n",
       " 402: u'artifactory',\n",
       " 403: u'artificial',\n",
       " 404: u'artistic',\n",
       " 405: u'arts',\n",
       " 406: u'asap',\n",
       " 407: u'asia',\n",
       " 408: u'ask',\n",
       " 409: u'asked',\n",
       " 410: u'asking',\n",
       " 411: u'asp',\n",
       " 412: u'aspect',\n",
       " 413: u'aspects',\n",
       " 414: u'aspiration',\n",
       " 415: u'assassins',\n",
       " 416: u'assembled',\n",
       " 417: u'assembly',\n",
       " 418: u'assertive',\n",
       " 419: u'assess',\n",
       " 420: u'assesses',\n",
       " 421: u'assessment',\n",
       " 422: u'assessments',\n",
       " 423: u'asset',\n",
       " 424: u'assetexperience',\n",
       " 425: u'assets',\n",
       " 426: u'assetsalary',\n",
       " 427: u'assigned',\n",
       " 428: u'assignedqualifications',\n",
       " 429: u'assignment',\n",
       " 430: u'assignments',\n",
       " 431: u'assimilate',\n",
       " 432: u'assist',\n",
       " 433: u'assistance',\n",
       " 434: u'assistant',\n",
       " 435: u'assistants',\n",
       " 436: u'assisting',\n",
       " 437: u'assists',\n",
       " 438: u'associate',\n",
       " 439: u'associated',\n",
       " 440: u'associates',\n",
       " 441: u'association',\n",
       " 442: u'assortment',\n",
       " 443: u'assumptions',\n",
       " 444: u'assurance',\n",
       " 445: u'assure',\n",
       " 446: u'atlanta',\n",
       " 447: u'atlantic',\n",
       " 448: u'atmosphere',\n",
       " 449: u'atrium',\n",
       " 450: u'attend',\n",
       " 451: u'attendees',\n",
       " 452: u'attends',\n",
       " 453: u'attention',\n",
       " 454: u'attentive',\n",
       " 455: u'attitude',\n",
       " 456: u'attributes',\n",
       " 457: u'attributesethics',\n",
       " 458: u'attribution',\n",
       " 459: u'audience',\n",
       " 460: u'audiences',\n",
       " 461: u'audit',\n",
       " 462: u'auditing',\n",
       " 463: u'auditors',\n",
       " 464: u'aug',\n",
       " 465: u'austin',\n",
       " 466: u'australia',\n",
       " 467: u'authentication',\n",
       " 468: u'authoring',\n",
       " 469: u'authorization',\n",
       " 470: u'authorized',\n",
       " 471: u'authorizes',\n",
       " 472: u'auto',\n",
       " 473: u'automated',\n",
       " 474: u'automation',\n",
       " 475: u'autonomic',\n",
       " 476: u'autonomousservice',\n",
       " 477: u'autonomy',\n",
       " 478: u'av',\n",
       " 479: u'availability',\n",
       " 480: u'available',\n",
       " 481: u'average',\n",
       " 482: u'avoidance',\n",
       " 483: u'award',\n",
       " 484: u'awards',\n",
       " 485: u'aware',\n",
       " 486: u'awareness',\n",
       " 487: u'away',\n",
       " 488: u'aways',\n",
       " 489: u'awesome',\n",
       " 490: u'aws',\n",
       " 491: u'azure',\n",
       " 492: u'b2b',\n",
       " 493: u'b2c',\n",
       " 494: u'ba',\n",
       " 495: u'bachelor',\n",
       " 496: u'bachelors',\n",
       " 497: u'backbone',\n",
       " 498: u'backed',\n",
       " 499: u'backend',\n",
       " 500: u'background',\n",
       " 501: u'backgrounds',\n",
       " 502: u'backlog',\n",
       " 503: u'backseat',\n",
       " 504: u'baidu',\n",
       " 505: u'bajob',\n",
       " 506: u'balance',\n",
       " 507: u'balanced',\n",
       " 508: u'balancers',\n",
       " 509: u'balancing',\n",
       " 510: u'bank',\n",
       " 511: u'banking',\n",
       " 512: u'bar',\n",
       " 513: u'barclaycard',\n",
       " 514: u'barclays',\n",
       " 515: u'base',\n",
       " 516: u'based',\n",
       " 517: u'basedon',\n",
       " 518: u'baseline',\n",
       " 519: u'bash',\n",
       " 520: u'bashes',\n",
       " 521: u'basic',\n",
       " 522: u'basis',\n",
       " 523: u'basketball',\n",
       " 524: u'batch',\n",
       " 525: u'bbc',\n",
       " 526: u'beach',\n",
       " 527: u'beautiful',\n",
       " 528: u'beauty',\n",
       " 529: u'bed',\n",
       " 530: u'beer',\n",
       " 531: u'begin',\n",
       " 532: u'beginning',\n",
       " 533: u'behavior',\n",
       " 534: u'behaviors',\n",
       " 535: u'behaviour',\n",
       " 536: u'beijing',\n",
       " 537: u'belief',\n",
       " 538: u'beliefs',\n",
       " 539: u'believe',\n",
       " 540: u'believes',\n",
       " 541: u'believing',\n",
       " 542: u'benchmarks',\n",
       " 543: u'beneficial',\n",
       " 544: u'benefit',\n",
       " 545: u'benefits',\n",
       " 546: u'bespoke',\n",
       " 547: u'best',\n",
       " 548: u'better',\n",
       " 549: u'bettering',\n",
       " 550: u'bgp',\n",
       " 551: u'bi',\n",
       " 552: u'biased',\n",
       " 553: u'bibme',\n",
       " 554: u'bidding',\n",
       " 555: u'bids',\n",
       " 556: u'big',\n",
       " 557: u'bigger',\n",
       " 558: u'billboard',\n",
       " 559: u'billion',\n",
       " 560: u'billions',\n",
       " 561: u'bim',\n",
       " 562: u'bind',\n",
       " 563: u'biomedical',\n",
       " 564: u'biostatistical',\n",
       " 565: u'biostatistics',\n",
       " 566: u'birmingham',\n",
       " 567: u'bitbucket',\n",
       " 568: u'bits',\n",
       " 569: u'blackstone',\n",
       " 570: u'blasts',\n",
       " 571: u'blend',\n",
       " 572: u'blocks',\n",
       " 573: u'blog',\n",
       " 574: u'blogs',\n",
       " 575: u'bloomberg',\n",
       " 576: u'blue',\n",
       " 577: u'blueprint',\n",
       " 578: u'board',\n",
       " 579: u'boarding',\n",
       " 580: u'boat',\n",
       " 581: u'boc',\n",
       " 582: u'bold',\n",
       " 583: u'bom',\n",
       " 584: u'bonus',\n",
       " 585: u'bonuses',\n",
       " 586: u'booking',\n",
       " 587: u'boolean',\n",
       " 588: u'bootstrap',\n",
       " 589: u'borders',\n",
       " 590: u'boring',\n",
       " 591: u'boston',\n",
       " 592: u'bothell',\n",
       " 593: u'boundaries',\n",
       " 594: u'boutique',\n",
       " 595: u'brainstorming',\n",
       " 596: u'brand',\n",
       " 597: u'branded',\n",
       " 598: u'brands',\n",
       " 599: u'brandsoutline',\n",
       " 600: u'brazil',\n",
       " 601: u'breadth',\n",
       " 602: u'break',\n",
       " 603: u'breakfast',\n",
       " 604: u'breakfasts',\n",
       " 605: u'breath',\n",
       " 606: u'breathe',\n",
       " 607: u'bribery',\n",
       " 608: u'bridge',\n",
       " 609: u'bridging',\n",
       " 610: u'briefings',\n",
       " 611: u'bright',\n",
       " 612: u'brightest',\n",
       " 613: u'bring',\n",
       " 614: u'bringing',\n",
       " 615: u'broad',\n",
       " 616: u'broader',\n",
       " 617: u'broadly',\n",
       " 618: u'brochures',\n",
       " 619: u'brokerages',\n",
       " 620: u'broking',\n",
       " 621: u'brook',\n",
       " 622: u'browse',\n",
       " 623: u'browser',\n",
       " 624: u'browsers',\n",
       " 625: u'bs',\n",
       " 626: u'bsa',\n",
       " 627: u'bscs',\n",
       " 628: u'bsn',\n",
       " 629: u'buddies',\n",
       " 630: u'budget',\n",
       " 631: u'budgeting',\n",
       " 632: u'budgets',\n",
       " 633: u'buffs',\n",
       " 634: u'bugs',\n",
       " 635: u'build',\n",
       " 636: u'builders',\n",
       " 637: u'building',\n",
       " 638: u'builds',\n",
       " 639: u'built',\n",
       " 640: u'bundle',\n",
       " 641: u'burdens',\n",
       " 642: u'burlington',\n",
       " 643: u'bus',\n",
       " 644: u'business',\n",
       " 645: u'businesses',\n",
       " 646: u'businessworking',\n",
       " 647: u'buy',\n",
       " 648: u'buyer',\n",
       " 649: u'buying',\n",
       " 650: u'bw',\n",
       " 651: u'c4isr',\n",
       " 652: u'ca',\n",
       " 653: u'caching',\n",
       " 654: u'cacs',\n",
       " 655: u'cadence',\n",
       " 656: u'calc',\n",
       " 657: u'calculating',\n",
       " 658: u'calculus',\n",
       " 659: u'calendar',\n",
       " 660: u'calgary',\n",
       " 661: u'caliber',\n",
       " 662: u'california',\n",
       " 663: u'callminer',\n",
       " 664: u'calls',\n",
       " 665: u'calm',\n",
       " 666: u'caltrain',\n",
       " 667: u'cambridge',\n",
       " 668: u'campaign',\n",
       " 669: u'campaignother',\n",
       " 670: u'campaigns',\n",
       " 671: u'campaignsensure',\n",
       " 672: u'campaignsmanage',\n",
       " 673: u'campaignsworking',\n",
       " 674: u'campus',\n",
       " 675: u'campuses',\n",
       " 676: u'canada',\n",
       " 677: u'canadian',\n",
       " 678: u'candidate',\n",
       " 679: u'candidates',\n",
       " 680: u'cap',\n",
       " 681: u'capabilities',\n",
       " 682: u'capability',\n",
       " 683: u'capable',\n",
       " 684: u'capacity',\n",
       " 685: u'capital',\n",
       " 686: u'capitalize',\n",
       " 687: u'capture',\n",
       " 688: u'car',\n",
       " 689: u'carat',\n",
       " 690: u'card',\n",
       " 691: u'cardiology',\n",
       " 692: u'care',\n",
       " 693: u'career',\n",
       " 694: u'careers',\n",
       " 695: u'careful',\n",
       " 696: u'carefully',\n",
       " 697: u'cargurus',\n",
       " 698: u'caring',\n",
       " 699: u'carried',\n",
       " 700: u'carrier',\n",
       " 701: u'carriers',\n",
       " 702: u'carrying',\n",
       " 703: u'cas',\n",
       " 704: u'case',\n",
       " 705: u'cases',\n",
       " 706: u'cash',\n",
       " 707: u'casl',\n",
       " 708: u'cassandra',\n",
       " 709: u'casual',\n",
       " 710: u'categories',\n",
       " 711: u'categorization',\n",
       " 712: u'category',\n",
       " 713: u'catered',\n",
       " 714: u'catman',\n",
       " 715: u'cause',\n",
       " 716: u'causes',\n",
       " 717: u'ccie',\n",
       " 718: u'ccl',\n",
       " 719: u'ccn',\n",
       " 720: u'ccnp',\n",
       " 721: u'cdh',\n",
       " 722: u'cdn',\n",
       " 723: u'cdns',\n",
       " 724: u'celebrate',\n",
       " 725: u'celebrating',\n",
       " 726: u'censecfor',\n",
       " 727: u'census',\n",
       " 728: u'center',\n",
       " 729: u'centered',\n",
       " 730: u'centers',\n",
       " 731: u'central',\n",
       " 732: u'centric',\n",
       " 733: u'century',\n",
       " 734: u'ceo',\n",
       " 735: u'cerner',\n",
       " 736: u'certification',\n",
       " 737: u'certifications',\n",
       " 738: u'certified',\n",
       " 739: u'cfr',\n",
       " 740: u'cgi',\n",
       " 741: u'chain',\n",
       " 742: u'chains',\n",
       " 743: u'challenge',\n",
       " 744: u'challenged',\n",
       " 745: u'challenges',\n",
       " 746: u'challengesassess',\n",
       " 747: u'challengesdevelop',\n",
       " 748: u'challenging',\n",
       " 749: u'champion',\n",
       " 750: u'champs',\n",
       " 751: u'chance',\n",
       " 752: u'change',\n",
       " 753: u'changes',\n",
       " 754: u'changing',\n",
       " 755: u'channel',\n",
       " 756: u'channels',\n",
       " 757: u'characteristic',\n",
       " 758: u'characteristics',\n",
       " 759: u'charge',\n",
       " 760: u'charged',\n",
       " 761: u'charles',\n",
       " 762: u'charters',\n",
       " 763: u'charting',\n",
       " 764: u'chat',\n",
       " 765: u'check',\n",
       " 766: u'checklists',\n",
       " 767: u'checkpoint',\n",
       " 768: u'checks',\n",
       " 769: u'checkstyle',\n",
       " 770: u'chef',\n",
       " 771: u'chegg',\n",
       " 772: u'chgg',\n",
       " 773: u'chicago',\n",
       " 774: u'children',\n",
       " 775: u'chinese',\n",
       " 776: u'choice',\n",
       " 777: u'choices',\n",
       " 778: u'choose',\n",
       " 779: u'christ',\n",
       " 780: u'church',\n",
       " 781: u'ci',\n",
       " 782: u'cioreview',\n",
       " 783: u'cisco',\n",
       " 784: u'cision',\n",
       " 785: u'citationmachine',\n",
       " 786: u'citizen',\n",
       " 787: u'citizenship',\n",
       " 788: u'citizensip',\n",
       " 789: u'citrix',\n",
       " 790: u'city',\n",
       " 791: u'civil',\n",
       " 792: u'claim',\n",
       " 793: u'claims',\n",
       " 794: u'clara',\n",
       " 795: u'clarify',\n",
       " 796: u'clarita',\n",
       " 797: u'clarity',\n",
       " 798: u'class',\n",
       " 799: u'classes',\n",
       " 800: u'classification',\n",
       " 801: u'classifications',\n",
       " 802: u'classified',\n",
       " 803: u'clean',\n",
       " 804: u'cleansing',\n",
       " 805: u'clear',\n",
       " 806: u'clearance',\n",
       " 807: u'clearancepreferred',\n",
       " 808: u'clearing',\n",
       " 809: u'clearly',\n",
       " 810: u'clears',\n",
       " 811: u'click',\n",
       " 812: u'clicking',\n",
       " 813: u'client',\n",
       " 814: u'clientpersonnel',\n",
       " 815: u'clients',\n",
       " 816: u'clientsrun',\n",
       " 817: u'climate',\n",
       " 818: u'climb',\n",
       " 819: u'clinic',\n",
       " 820: u'clinical',\n",
       " 821: u'clinician',\n",
       " 822: u'clinicians',\n",
       " 823: u'close',\n",
       " 824: u'closed',\n",
       " 825: u'closely',\n",
       " 826: u'cloud',\n",
       " 827: u'cloudera',\n",
       " 828: u'cloudfront',\n",
       " 829: u'cluster',\n",
       " 830: u'clustering',\n",
       " 831: u'clusters',\n",
       " 832: u'cms',\n",
       " 833: u'cnc',\n",
       " 834: u'coach',\n",
       " 835: u'coaching',\n",
       " 836: u'cocoapods',\n",
       " 837: u'code',\n",
       " 838: u'codebase',\n",
       " 839: u'coded',\n",
       " 840: u'coding',\n",
       " 841: u'cognitive',\n",
       " 842: u'cognos',\n",
       " 843: u'coherent',\n",
       " 844: u'cohesive',\n",
       " 845: u'collaborate',\n",
       " 846: u'collaborates',\n",
       " 847: u'collaborating',\n",
       " 848: u'collaboration',\n",
       " 849: u'collaborative',\n",
       " 850: u'collaboratively',\n",
       " 851: u'collaborator',\n",
       " 852: u'collateral',\n",
       " 853: u'collateralanalytics',\n",
       " 854: u'colleagues',\n",
       " 855: u'collect',\n",
       " 856: u'collection',\n",
       " 857: u'collections',\n",
       " 858: u'collects',\n",
       " 859: u'college',\n",
       " 860: u'collegial',\n",
       " 861: u'color',\n",
       " 862: u'colors',\n",
       " 863: u'com',\n",
       " 864: u'combination',\n",
       " 865: u'combinations',\n",
       " 866: u'combine',\n",
       " 867: u'combined',\n",
       " 868: u'combines',\n",
       " 869: u'combining',\n",
       " 870: u'come',\n",
       " 871: u'comfort',\n",
       " 872: u'comfortable',\n",
       " 873: u'coming',\n",
       " 874: u'command',\n",
       " 875: u'commands',\n",
       " 876: u'commensurate',\n",
       " 877: u'commerce',\n",
       " 878: u'commercial',\n",
       " 879: u'commercially',\n",
       " 880: u'commit',\n",
       " 881: u'commitment',\n",
       " 882: u'commitments',\n",
       " 883: u'committed',\n",
       " 884: u'committee',\n",
       " 885: u'committees',\n",
       " 886: u'common',\n",
       " 887: u'commonly',\n",
       " 888: u'communicate',\n",
       " 889: u'communicated',\n",
       " 890: u'communicates',\n",
       " 891: u'communicating',\n",
       " 892: u'communication',\n",
       " 893: u'communications',\n",
       " 894: u'communicator',\n",
       " 895: u'communities',\n",
       " 896: u'community',\n",
       " 897: u'commuter',\n",
       " 898: u'companies',\n",
       " 899: u'company',\n",
       " 900: u'comparable',\n",
       " 901: u'compared',\n",
       " 902: u'comparing',\n",
       " 903: u'compassionate',\n",
       " 904: u'compatibility',\n",
       " 905: u'compelling',\n",
       " 906: u'compensation',\n",
       " 907: u'compete',\n",
       " 908: u'competencies',\n",
       " 909: u'competency',\n",
       " 910: u'competing',\n",
       " 911: u'competition',\n",
       " 912: u'competitive',\n",
       " 913: u'competitor',\n",
       " 914: u'competitors',\n",
       " 915: u'compile',\n",
       " 916: u'complaint',\n",
       " 917: u'complete',\n",
       " 918: u'completed',\n",
       " 919: u'completemake',\n",
       " 920: u'completeness',\n",
       " 921: u'completes',\n",
       " 922: u'completing',\n",
       " 923: u'completion',\n",
       " 924: u'completionpassionate',\n",
       " 925: u'complex',\n",
       " 926: u'complexity',\n",
       " 927: u'compliance',\n",
       " 928: u'complianceefficiency',\n",
       " 929: u'complimentary',\n",
       " 930: u'component',\n",
       " 931: u'components',\n",
       " 932: u'compose',\n",
       " 933: u'composition',\n",
       " 934: u'comprehensive',\n",
       " 935: u'comprehensively',\n",
       " 936: u'comprised',\n",
       " 937: u'comprises',\n",
       " 938: u'compromise',\n",
       " 939: u'computational',\n",
       " 940: u'computer',\n",
       " 941: u'computers',\n",
       " 942: u'computing',\n",
       " 943: u'comscore',\n",
       " 944: u'concentration3',\n",
       " 945: u'concept',\n",
       " 946: u'conception',\n",
       " 947: u'concepts',\n",
       " 948: u'conceptualization',\n",
       " 949: u'concerning',\n",
       " 950: u'concerns',\n",
       " 951: u'concise',\n",
       " 952: u'concisely',\n",
       " 953: u'conclusions',\n",
       " 954: u'concrete',\n",
       " 955: u'concurrency',\n",
       " 956: u'concurrent',\n",
       " 957: u'condition',\n",
       " 958: u'conditions',\n",
       " 959: u'conditionsthe',\n",
       " 960: u'conduct',\n",
       " 961: u'conducted',\n",
       " 962: u'conducting',\n",
       " 963: u'conducts',\n",
       " 964: u'conference',\n",
       " 965: u'confidence',\n",
       " 966: u'confident',\n",
       " 967: u'confidential',\n",
       " 968: u'confidentiality',\n",
       " 969: u'configuration',\n",
       " 970: u'configurations',\n",
       " 971: u'configure',\n",
       " 972: u'confirm',\n",
       " 973: u'confirming',\n",
       " 974: u'conflict',\n",
       " 975: u'conformation',\n",
       " 976: u'confront',\n",
       " 977: u'conjunction',\n",
       " 978: u'connect',\n",
       " 979: u'connected',\n",
       " 980: u'connectivity',\n",
       " 981: u'consecutive',\n",
       " 982: u'consensus',\n",
       " 983: u'consequently',\n",
       " 984: u'consider',\n",
       " 985: u'consideration',\n",
       " 986: u'considerationsqualifications2',\n",
       " 987: u'considered',\n",
       " 988: u'considers',\n",
       " 989: u'consist',\n",
       " 990: u'consistency',\n",
       " 991: u'consistent',\n",
       " 992: u'consistently',\n",
       " 993: u'constant',\n",
       " 994: u'constantly',\n",
       " 995: u'constituency',\n",
       " 996: u'constraints',\n",
       " 997: u'construct',\n",
       " 998: u'constructing',\n",
       " 999: u'construction',\n",
       " ...}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform corpus into a reference list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 309 367 187\n"
     ]
    }
   ],
   "source": [
    "list_corpus = []\n",
    "for x in corpus:\n",
    "    list_corpus.append(x)\n",
    "    \n",
    "print len(list_corpus), len(list_corpus[0]), len(list_corpus[1]), len(list_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LDA Model for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-24697a7bdb61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpasses\u001b[0m      \u001b[0;34m=\u001b[0m  \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mid2word\u001b[0m     \u001b[0;34m=\u001b[0m  \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# or use the gensim dictionary object!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# id2word     =  dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "topic_ct = 3\n",
    "start = datetime.datetime.now()\n",
    "lda = models.LdaModel(\n",
    "    corpus,\n",
    "    # or use the corpus object created with the dictionary in the previous frame!\n",
    "    # corpus, \n",
    "    num_topics  =  topic_ct,\n",
    "    passes      =  20,\n",
    "    id2word     =  vocab,\n",
    "    verbose = True\n",
    "    # or use the gensim dictionary object!\n",
    "    # id2word     =  dictionary\n",
    ")\n",
    "print datetime.datetime.now()-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "topic number: 0\n",
      "\t0.019*data \n",
      "\t 0.013*experience \n",
      "\t 0.009*business \n",
      "\t 0.008*work \n",
      "\t 0.007*team \n",
      "\t 0.006*skills \n",
      "\t 0.006*development \n",
      "\t 0.005*ability \n",
      "\t 0.005*analytics \n",
      "\t 0.005*management\n",
      "========================================\n",
      "topic number: 1\n",
      "\t0.007*team \n",
      "\t 0.007*experience \n",
      "\t 0.007*product \n",
      "\t 0.007*solutions \n",
      "\t 0.006*skills \n",
      "\t 0.005*business \n",
      "\t 0.005*data \n",
      "\t 0.005*marketing \n",
      "\t 0.005*new \n",
      "\t 0.004*sales\n",
      "========================================\n",
      "topic number: 2\n",
      "\t0.013*business \n",
      "\t 0.009*experience \n",
      "\t 0.007*work \n",
      "\t 0.007*skills \n",
      "\t 0.006*ability \n",
      "\t 0.005*marketing \n",
      "\t 0.005*data \n",
      "\t 0.005*management \n",
      "\t 0.005*team \n",
      "\t 0.005*strong\n"
     ]
    }
   ],
   "source": [
    "topic_vectors = lda.print_topics(num_topics=3, num_words=10)\n",
    "for topic in topic_vectors:\n",
    "    print '========================================'\n",
    "    print 'topic number:', topic[0]\n",
    "    for y in topic[1].split('+'):\n",
    "        print '\\t',y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform  Topics into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "          0         1         2       sum\n",
      "0  0.000000  0.998348  0.000000  0.998348\n",
      "1  0.998646  0.000000  0.000000  0.998646\n",
      "2  0.098688  0.000000  0.900039  0.998727\n",
      "3  0.994124  0.000000  0.000000  0.994124\n",
      "4  0.358134  0.640354  0.000000  0.998489\n",
      "5  0.995060  0.000000  0.000000  0.995060\n",
      "6  0.997292  0.000000  0.000000  0.997292\n",
      "7  0.148182  0.000000  0.850116  0.998297\n",
      "8  0.997983  0.000000  0.000000  0.997983\n",
      "9  0.996809  0.000000  0.000000  0.996809\n"
     ]
    }
   ],
   "source": [
    "topic_proba = []\n",
    "for x in corpus:\n",
    "    local = lda.get_document_topics(x)\n",
    "    row = { x:float(0) for x in range(topic_ct)}\n",
    "    for y in local:\n",
    "        row[y[0]] = y[1]\n",
    "    topic_proba.append(row)\n",
    "    \n",
    "\n",
    "topic_proba_df = pd.DataFrame(topic_proba)\n",
    "topic_proba_df['sum'] = topic_proba_df.apply(np.sum,axis=1)\n",
    "print topic_proba_df.shape\n",
    "print topic_proba_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.9986444788586617, 1: 0.0, 2: 0.0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that probabilities add up:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class topicModeling(object):\n",
    "    margin = '\\t\\t'\n",
    "    def verboseMsg(self, msg):\n",
    "        if self.verbose:\n",
    "            print self.margin, msg\n",
    "        \n",
    "    def __init__(self, alltext, verbose=0):\n",
    "        self.sometext = alltext\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def loadText(self,alltext):\n",
    "        self.sometext = alltext\n",
    "    \n",
    "    def setVectorizer(self,vectorizer):\n",
    "        self.vectorizer = vectorizer\n",
    "    \n",
    "    def setCvec(self,ngram_range):\n",
    "        self.vectorizer = CountVectorizer(stop_words='english', ngram_range=ngram_range)\n",
    "    \n",
    "    def setTvec(self,ngram_range):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', ngram_range=ngram_range)\n",
    "        \n",
    "    def fitText(self):\n",
    "        self.verboseMsg('text===>word start: fitting text of size %d documents' % (len(self.sometext)))\n",
    "            \n",
    "        self.X = self.vectorizer.fit_transform(self.sometext)\n",
    "        \n",
    "        self.verboseMsg('text===>word summarizing vocab and creating corpus ')\n",
    "        \n",
    "        #prepping summary\n",
    "        self.df_X = pd.DataFrame(self.X.toarray(), columns=self.vectorizer.get_feature_names())\n",
    "        self.wordFreq = self.df_X.sum().sort_values(ascending = False)        \n",
    "    \n",
    "        self.vocab = {v: k for k, v in self.vectorizer.vocabulary_.iteritems()}\n",
    "        self.corpus = matutils.Sparse2Corpus(self.X, documents_columns=False)    \n",
    "        \n",
    "        self.verboseMsg('text===>word complete. ')\n",
    "            \n",
    "    def fitTopics(self,topic_ct,passes):\n",
    "        self.topic_ct = topic_ct\n",
    "        self.passes = passes\n",
    "        \n",
    "        self.verboseMsg('worp===>%d topics, %d passes: start ' %(topic_ct,passes))\n",
    "        self.lda = models.LdaModel(\n",
    "            self.corpus,\n",
    "            num_topics  =  self.topic_ct,\n",
    "            passes      =  20,\n",
    "            id2word     =  self.vocab\n",
    "        )\n",
    "        self.verboseMsg('worp===>%d topics, %d passes: lda model complete ' %(topic_ct,passes))\n",
    "        \n",
    "        self.topic_vectors = self.lda.print_topics(num_topics=self.topic_ct, num_words=5)\n",
    "\n",
    "        self.topic_proba = []\n",
    "        for x in self.corpus:\n",
    "            local = self.lda.get_document_topics(x)\n",
    "            row = { x:float(0) for x in range(self.topic_ct)}\n",
    "            for y in local:\n",
    "                row[y[0]] = y[1]\n",
    "            self.topic_proba.append(row)\n",
    "\n",
    "        self.verboseMsg('worp===>%d topics, %d passes: creating probabilities in dataframe ' %(topic_ct,passes))\n",
    "        \n",
    "        self.topic_proba_df = pd.DataFrame(self.topic_proba)\n",
    "    \n",
    "        self.verboseMsg('worp===>%d topics, %d passes: complete ' %(topic_ct,passes))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\ttext===>word start: fitting text of size 100 documents\n",
      "\t\ttext===>word summarizing vocab and creating corpus \n",
      "\t\ttext===>word complete. \n",
      "\t\tworp===>3 topics, 20 passes: start \n",
      "\t\tworp===>3 topics, 20 passes: lda model complete \n",
      "\t\tworp===>3 topics, 20 passes: creating probabilities in dataframe \n",
      "\t\tworp===>3 topics, 20 passes: complete \n",
      "\n",
      "============ word frequencies ==================\n",
      "data           478\n",
      "experience     406\n",
      "business       322\n",
      "work           238\n",
      "team           227\n",
      "skills         224\n",
      "ability        172\n",
      "analytics      158\n",
      "development    157\n",
      "management     152\n",
      "marketing      150\n",
      "solutions      148\n",
      "support        145\n",
      "years          132\n",
      "strong         128\n",
      "technical      125\n",
      "systems        122\n",
      "knowledge      119\n",
      "new            118\n",
      "design         113\n",
      "dtype: int64\n",
      "\n",
      "============ topics found === ==================\n",
      "(0, u'0.016*experience + 0.016*data + 0.012*business + 0.010*team + 0.008*software')\n",
      "(1, u'0.013*business + 0.013*marketing + 0.011*experience + 0.009*work + 0.009*data')\n",
      "(2, u'0.027*data + 0.013*experience + 0.007*skills + 0.006*systems + 0.006*solutions')\n",
      "\t\ttext===>word start: fitting text of size 100 documents\n",
      "\t\ttext===>word summarizing vocab and creating corpus \n",
      "\t\ttext===>word complete. \n",
      "\t\tworp===>3 topics, 20 passes: start \n",
      "\t\tworp===>3 topics, 20 passes: lda model complete \n",
      "\t\tworp===>3 topics, 20 passes: creating probabilities in dataframe \n",
      "\t\tworp===>3 topics, 20 passes: complete \n",
      "\n",
      "============ word frequencies ==================\n",
      "data           478\n",
      "experience     406\n",
      "business       322\n",
      "work           238\n",
      "team           227\n",
      "skills         224\n",
      "ability        172\n",
      "analytics      158\n",
      "development    157\n",
      "management     152\n",
      "marketing      150\n",
      "solutions      148\n",
      "support        145\n",
      "years          132\n",
      "strong         128\n",
      "technical      125\n",
      "systems        122\n",
      "knowledge      119\n",
      "new            118\n",
      "design         113\n",
      "dtype: int64\n",
      "\n",
      "============ topics found === ==================\n",
      "(0, u'0.001*profiles + 0.001*primarily + 0.001*timerequired + 0.000*type + 0.000*marketing')\n",
      "(1, u'0.000*business + 0.000*data + 0.000*profiles + 0.000*referrals + 0.000*analyst')\n",
      "(2, u'0.005*data + 0.004*experience + 0.003*business + 0.003*marketing + 0.002*work')\n",
      "\t\ttext===>word start: fitting text of size 1000 documents\n",
      "\t\ttext===>word summarizing vocab and creating corpus \n",
      "\t\ttext===>word complete. \n",
      "\t\tworp===>3 topics, 20 passes: start \n",
      "\t\tworp===>3 topics, 20 passes: lda model complete \n",
      "\t\tworp===>3 topics, 20 passes: creating probabilities in dataframe \n",
      "\t\tworp===>3 topics, 20 passes: complete \n",
      "\n",
      "============ word frequencies ==================\n",
      "data           478\n",
      "experience     406\n",
      "business       322\n",
      "work           238\n",
      "team           227\n",
      "skills         224\n",
      "ability        172\n",
      "analytics      158\n",
      "development    157\n",
      "management     152\n",
      "marketing      150\n",
      "solutions      148\n",
      "support        145\n",
      "years          132\n",
      "strong         128\n",
      "technical      125\n",
      "systems        122\n",
      "knowledge      119\n",
      "new            118\n",
      "design         113\n",
      "dtype: int64\n",
      "\n",
      "============ topics found === ==================\n",
      "(0, u'0.015*business + 0.011*experience + 0.009*management + 0.008*data + 0.008*skills')\n",
      "(1, u'0.013*experience + 0.012*data + 0.008*business + 0.006*skills + 0.005*work')\n",
      "(2, u'0.021*data + 0.016*experience + 0.009*team + 0.008*work + 0.008*business')\n"
     ]
    }
   ],
   "source": [
    "with open('master_total_df.p','rb') as f:\n",
    "    master_total_df = pickle.load(f)\n",
    "master_total_df.head(2)\n",
    "alltext = master_total_df['jobdesc'].values\n",
    "\n",
    "\n",
    "\n",
    "tM = topicModeling(alltext[:100],verbose=1)\n",
    "vec = CountVectorizer(stop_words='english', ngram_range=(1,1), min_df =2,max_features=50000)\n",
    "tM.setVectorizer(vec)\n",
    "tM.fitText()\n",
    "tM.fitTopics(topic_ct=3,passes=20)\n",
    "\n",
    "print '\\n============ word frequencies =================='\n",
    "print tM.wordFreq[:20]\n",
    "print '\\n============ topics found === =================='\n",
    "for y in tM.topic_vectors:\n",
    "    print y\n",
    "    \n",
    "    \n",
    "tM = topicModeling(alltext[:100],verbose=1)\n",
    "vec = TfidfVectorizer(stop_words='english', ngram_range=(1,1), min_df =2,max_features=50000)\n",
    "tM.setVectorizer(vec)\n",
    "tM.fitText()\n",
    "tM.fitTopics(topic_ct=3,passes=20)\n",
    "\n",
    "print '\\n============ word frequencies =================='\n",
    "print tM.wordFreq[:20]\n",
    "print '\\n============ topics found === =================='\n",
    "for y in tM.topic_vectors:\n",
    "    print y\n",
    "\n",
    "\n",
    "tM = topicModeling(alltext[:1000],verbose=1)\n",
    "tM.setCvec((1,1))\n",
    "tM.fitText()\n",
    "tM.fitTopics(topic_ct=3,passes=20)\n",
    "\n",
    "print '\\n============ word frequencies =================='\n",
    "print tM.wordFreq[:20]\n",
    "print '\\n============ topics found === =================='\n",
    "for y in tM.topic_vectors:\n",
    "    print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
