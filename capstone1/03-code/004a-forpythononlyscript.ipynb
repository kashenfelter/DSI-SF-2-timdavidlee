{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin add in NLP features ========= \n",
      "9. merging the top 10,000 most common words; merging with X\n",
      "(8038, 14047)\n",
      "0:00:10.595376  of  0:00:10.595414\n",
      "10. using enhanced data with Ridge and Gridsearch\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] alpha=0.001 .....................................................\n",
      "[CV] alpha=0.001 .....................................................\n",
      "[CV] alpha=0.001 .....................................................\n",
      "[CV] alpha=0.001 .....................................................\n"
     ]
    }
   ],
   "source": [
    "#load libraries\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import patsy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, Ridge\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import unidecode\n",
    "\n",
    "global_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "def printProgress():\n",
    "    global start\n",
    "    currentTime = datetime.datetime.now()\n",
    "    print currentTime - start, ' of ', currentTime - global_start \n",
    "    start = currentTime\n",
    "\n",
    "\n",
    "#loading data from the pickled data frame\n",
    "\n",
    "# ======================================================================================\n",
    "banner = '======================================================================================\\n======================================================================================'\n",
    "print banner\n",
    "print '1. loading data'\n",
    "with open('../jNotebooks/master_total_df.p','rb') as f:\n",
    "    master_total_df = pickle.load(f)\n",
    "    \n",
    "printProgress()\n",
    "\n",
    "# ======================================================================================\n",
    "print banner\n",
    "print '2. cleaning data'\n",
    "print master_total_df.shape\n",
    "# only one data source has view data, will sub-set here.\n",
    "jobview_data = master_total_df[master_total_df.sourcesite =='lnk'].copy()\n",
    "print jobview_data.shape\n",
    "jobview_data.reset_index(inplace=True)\n",
    "\n",
    "jobview_data['expanded_title'].fillna('other',inplace=True)\n",
    "jobview_data['prefix_title'].fillna('other',inplace=True)\n",
    "jobview_data['state'].fillna('other',inplace=True)\n",
    "jobview_data['base_title'].replace(to_replace='', value='other',inplace=True)\n",
    "\n",
    "jobview_data['post_month'] = jobview_data['post_start_date'].map(lambda x : x[:3] )\n",
    "jobview_data['days_posted'] = jobview_data['days_posted'].map(lambda x: x.split(' ')[1])\n",
    "jobview_data['description_length'] = jobview_data['jobdesc'].map(lambda x: len(x))\n",
    "\n",
    "text = jobview_data['jobdesc'].map(lambda x : unidecode.unidecode(x).replace('\\n',' '))\n",
    "\n",
    "printProgress()\n",
    "\n",
    "# ======================================================================================\n",
    "print banner\n",
    "print '3. scaling data data'\n",
    "# initialize the scaler and create a normalized dataset\n",
    "sclr = StandardScaler()\n",
    "jobview_data_norm = jobview_data.copy()\n",
    "\n",
    "# convert to float and remove any words in the column\n",
    "jobview_data_norm['days_posted'].replace(to_replace=unicode('less'), value=0. , inplace=True)\n",
    "jobview_data_norm['days_posted'] = jobview_data_norm['days_posted'].astype(float)\n",
    "\n",
    "jobview_data_norm['description_length'].replace(to_replace='less', value=0. , inplace=True)\n",
    "jobview_data_norm['description_length'] = jobview_data_norm['description_length'].astype(float)\n",
    "\n",
    "\n",
    "# normalize the two number features\n",
    "norm_cols = ['days_posted','description_length']\n",
    "jobview_data_norm[norm_cols].mean()\n",
    "jobview_data_norm[norm_cols] = sclr.fit_transform(jobview_data_norm[norm_cols])\n",
    "\n",
    "printProgress()\n",
    "\n",
    "\n",
    "job_columns = ['company','city','state','views','days_posted','base_title','expanded_title', 'post_month','description_length']\n",
    "# ======================================================================================\n",
    "print banner\n",
    "print '4. splitting X and Y datasets'\n",
    "formula = 'views ~ ' + ' + '.join([x for x in jobview_data_norm[job_columns].columns if x != 'views']) + '-1'\n",
    "y, X = patsy.dmatrices(formula, jobview_data_norm,return_type='dataframe')\n",
    "#X = X.merge(cdf_for_merge, how='left', left_index=True, right_index=True)\n",
    "y = np.ravel(y)\n",
    "print X.shape, y.shape\n",
    "\n",
    "printProgress()\n",
    "# ======================================================================================\n",
    "print banner\n",
    "print 'begin basic and baseline regressions (without NLP additions) ========= '\n",
    "print '5.Performing lasso'\n",
    "lm = LassoCV(cv=5, n_alphas=200, n_jobs=4, iter)\n",
    "lm.fit(X,y)\n",
    "score = lm.score(X,y)\n",
    "print score, lm.alpha_\n",
    "\n",
    "printProgress()\n",
    "# ======================================================================================\n",
    "print banner\n",
    "print '6. Performing Ridge'\n",
    "rm = RidgeCV(cv=5, alphas=[0.001,0.01,0.1,1.0,3,10,30])\n",
    "rm.fit(X,y)\n",
    "score = rm.score(X,y)\n",
    "print score, rm.alpha_\n",
    "\n",
    "printProgress()\n",
    "# ======================================================================================\n",
    "print banner\n",
    "print '7. Performing Decision Tree Regressor'\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtc = DecisionTreeRegressor(max_depth=10, min_samples_split=20)\n",
    "dtc.fit(X,y)\n",
    "mscore = dtc.score(X,y)\n",
    "print mscore\n",
    "\n",
    "printProgress()\n",
    "# ======================================================================================\n",
    "print banner\n",
    "print '8. prepping the word data'\n",
    "cvec = CountVectorizer(stop_words='english', lowercase=True,ngram_range=(1,1))\n",
    "cvec.fit(text)\n",
    "\n",
    "printProgress()\n",
    "\n",
    "cdf  = pd.DataFrame(cvec.transform(text).todense(),\n",
    "             columns=cvec.get_feature_names())\n",
    "\n",
    "printProgress()\n",
    "# ======================================================================================\n",
    "print banner\n",
    "print 'begin add in NLP features ========= '\n",
    "print '9. merging the top 10,000 most common words; merging with X'\n",
    "summary = cdf.sum().sort_values(ascending=False)\n",
    "word_features = summary[:10000].index\n",
    "word_features\n",
    "\n",
    "cdf_for_merge = cdf[word_features].copy()\n",
    "cdf_for_merge.columns = ['nlp_'+x for x in cdf_for_merge.columns]\n",
    "\n",
    "X_plus = X.copy()\n",
    "X_plus = X_plus.merge(cdf_for_merge, how='left', left_index=True, right_index=True)\n",
    "print X_plus.shape\n",
    "\n",
    "printProgress()\n",
    "# ======================================================================================\n",
    "print banner\n",
    "print '10. using enhanced data with Ridge'\n",
    "\n",
    "rm = RidgeCV(cv=5, alphas=[0.001,0.01,0.1,1.0,3,10,30])\n",
    "rm.fit(X_plus,y)\n",
    "\n",
    "score = rm.score(X_plus,y)\n",
    "print score, rm.alpha_\n",
    "\n",
    "\n",
    "printProgress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
