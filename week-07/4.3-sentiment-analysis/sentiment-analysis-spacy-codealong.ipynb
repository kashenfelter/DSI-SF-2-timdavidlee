{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis codealong using spacy and movie reviews\n",
    "\n",
    "Sentiment analysis is one of the more popular topics in NLP. It is concerned with finding some kind of valence to written text. This could be positivity, negativity, subjectivity and many others. In this lesson we will just be looking at those three. \n",
    "\n",
    "First we will load in a dataset of pre-coded sentiment scores for positivity and negativity on words. These words are also divided up by their part of speech in the sentence.\n",
    "\n",
    "Then we will load snippets of rottentomatoes reviews and explore the sentiment of the writing.\n",
    "\n",
    "---\n",
    "\n",
    "### Load packages and sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22-caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22-calibre</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22_caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adj</td>\n",
       "      <td>.22_calibre</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adj</td>\n",
       "      <td>.38-caliber</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos         word  pos_score  neg_score\n",
       "0  adj  .22-caliber        0.0        0.0\n",
       "1  adj  .22-calibre        0.0        0.0\n",
       "2  adj  .22_caliber        0.0        0.0\n",
       "3  adj  .22_calibre        0.0        0.0\n",
       "4  adj  .38-caliber        0.0        0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen = pd.read_csv('/Users/tlee010/desktop//DSI-SF-2-timdavidlee/datasets/sentiment_words/sentiment_words_simple.csv')\n",
    "\n",
    "sen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create a sentiment dataset that does not take into account part of speech tags\n",
    "\n",
    "This will be what we use first, not knowing the part of speech a word is in. Later when we use spacy we will be able to determine the part of speech of each word and pair the scores accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'hood</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'s_gravenhage</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'tween</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'tween_decks</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.22</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  pos_score  neg_score\n",
       "0          'hood      0.000      0.375\n",
       "1  's_gravenhage      0.000      0.000\n",
       "2         'tween      0.000      0.000\n",
       "3   'tween_decks      0.000      0.000\n",
       "4            .22      0.125      0.000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_agg = sen[['word','pos_score','neg_score']].groupby('word').agg(np.mean).reset_index()\n",
    "sen_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create a dictionary version of the sentiment data for both the part of speech and aggregate\n",
    "\n",
    "The dictionary format of the data will be much easier to index into in our functions later. If we don't do this it's much harder to make those functions run quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n"
     ]
    }
   ],
   "source": [
    "sen_dict = {\n",
    "    'ADJ':{}\n",
    "    ,'NOUN':{}\n",
    "    ,'VERB':{}\n",
    "    , 'ADV':{}\n",
    "}\n",
    "for i, row in enumerate(sen.itertuples()): #makes rows into tuples as it goes (much faster)\n",
    "    if (i % 10000)==0:\n",
    "        print i\n",
    "    sen_dict[row[1].upper()][row[2]] = {'pos_score':row[3], 'neg_score':row[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_score': 0.625, 'pos_score': 0.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_dict['ADJ']['horrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen_agg_dict={}\n",
    "for row in sen_agg.itertuples():\n",
    "    sen_agg_dict[row[1]] = {'pos_score':row[2], 'neg_score':row[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_score': 0.75, 'pos_score': 0.25}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_dict['ADJ']['worst']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load the rotten tomatoes dataset\n",
    "\n",
    "This dataset has:\n",
    "    \n",
    "    critic: critic's name\n",
    "    fresh: fresh vs. rotten rating\n",
    "    imdb: code for imdb\n",
    "    publication: where the review was published\n",
    "    quote: the review snippet\n",
    "    review_date: date of review\n",
    "    rtid: rottentomatoes id\n",
    "    title: name of movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt = pd.read_csv('/Users/tlee010/desktop/DSI-SF-2-timdavidlee/datasets/rottentomatoes_critics/rt_critics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            critic  fresh      imdb    publication  \\\n",
       "0      Derek Adams  fresh  114709.0       Time Out   \n",
       "1  Richard Corliss  fresh  114709.0  TIME Magazine   \n",
       "\n",
       "                                               quote review_date    rtid  \\\n",
       "0  So ingenious in concept, design and execution ...  2009-10-04  9559.0   \n",
       "1                  The year's most inventive comedy.  2008-08-31  9559.0   \n",
       "\n",
       "       title  \n",
       "0  Toy story  \n",
       "1  Toy story  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Restrict data to reviews with valid ratings and reviews over 10 words long\n",
    "\n",
    "Clean up the reviews, making a column with the case and punctuation removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fresh     8613\n",
       "rotten    5436\n",
       "none        23\n",
       "Name: fresh, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.fresh.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove none reviews\n",
    "# reviews with 10 words or fewer\n",
    "\n",
    "rt = rt[rt.fresh.isin(['fresh','rotten'])]\n",
    "rt.fresh = rt.fresh.map(lambda x : 1 if x=='fresh' else 0)\n",
    "rt['quote_len'] = rt.quote.map(lambda x : len(x.split()))\n",
    "rt = rt[rt.quote_len > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11215, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So ingenious in concept, design and execution that you could watch it on a postage stamp-sized screen and still be engulfed by its charm.\n",
      "A winning animated feature that has something for everyone on the age spectrum.\n",
      "The film sports a provocative and appealing story that's every bit the equal of this technical achievement.\n",
      "An entertaining computer-generated, hyperrealist animation feature (1995) that's also in effect a toy catalog.\n"
     ]
    }
   ],
   "source": [
    "for q in rt.quote.values[0:4]:\n",
    "    print q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so ingenious in concept design and execution that you could watch it on a postage stamp-sized screen and still be engulfed by its charm\n",
      "a winning animated feature that has something for everyone on the age spectrum\n",
      "the film sports a provocative and appealing story that's every bit the equal of this technical achievement\n",
      "an entertaining computer-generated hyperrealist animation feature  that's also in effect a toy catalog\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "      <th>quote_len</th>\n",
       "      <th>qt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>1</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>24</td>\n",
       "      <td>so ingenious in concept design and execution t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>1</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>13</td>\n",
       "      <td>a winning animated feature that has something ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>1</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>17</td>\n",
       "      <td>the film sports a provocative and appealing st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>1</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>14</td>\n",
       "      <td>an entertaining computer-generated hyperrealis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Michael Booth</td>\n",
       "      <td>1</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Denver Post</td>\n",
       "      <td>As Lion King did before it, Toy Story revived ...</td>\n",
       "      <td>2007-05-03</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>40</td>\n",
       "      <td>as lion king did before it toy story revived t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh      imdb     publication  \\\n",
       "0         Derek Adams      1  114709.0        Time Out   \n",
       "2         David Ansen      1  114709.0        Newsweek   \n",
       "3       Leonard Klady      1  114709.0         Variety   \n",
       "4  Jonathan Rosenbaum      1  114709.0  Chicago Reader   \n",
       "5       Michael Booth      1  114709.0     Denver Post   \n",
       "\n",
       "                                               quote review_date    rtid  \\\n",
       "0  So ingenious in concept, design and execution ...  2009-10-04  9559.0   \n",
       "2  A winning animated feature that has something ...  2008-08-18  9559.0   \n",
       "3  The film sports a provocative and appealing st...  2008-06-09  9559.0   \n",
       "4  An entertaining computer-generated, hyperreali...  2008-03-10  9559.0   \n",
       "5  As Lion King did before it, Toy Story revived ...  2007-05-03  9559.0   \n",
       "\n",
       "       title  quote_len                                                 qt  \n",
       "0  Toy story         24  so ingenious in concept design and execution t...  \n",
       "2  Toy story         13  a winning animated feature that has something ...  \n",
       "3  Toy story         17  the film sports a provocative and appealing st...  \n",
       "4  Toy story         14  an entertaining computer-generated hyperrealis...  \n",
       "5  Toy story         40  as lion king did before it toy story revived t...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "string.ascii_lowercase\n",
    "\n",
    "rt['qt'] = rt['quote'].map(lambda x : unicode(''.join([i for i in x.lower() if i in string.ascii_lowercase+\" -'\"])))\n",
    "\n",
    "for q in rt.qt.values[0:4]:\n",
    "    print q\n",
    "\n",
    "rt.head()\n",
    "#spacy will find all these marks and turn into puncuation, but we will remove for practice sake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Write a function to assign positive rating, negative, and objective based on words in review\n",
    "\n",
    "We'll use the dictionary we constructed above (without the part of speech tags). \n",
    "\n",
    "Objectivity is calculated: \n",
    "\n",
    "    1. - (positive_score + negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def agg_scorer(x):\n",
    "    x = x.split()\n",
    "    pos_scores , neg_scores, obj_scores = [],[],[]\n",
    "    for word in x:\n",
    "        try:\n",
    "            pos_scores.append(sen_agg_dict[word]['pos_score'])\n",
    "            neg_scores.append(sen_agg_dict[word]['neg_score'])\n",
    "            obj_scores.append(1. - (pos_scores[-1] + neg_scores[-1]))\n",
    "        except:\n",
    "            pos_scores.append(0.)\n",
    "            neg_scores.append(0.)\n",
    "            obj_scores.append(1.)\n",
    "            continue\n",
    "    return [pos_scores, neg_scores, obj_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'children will enjoy a new take on the irresistible idea of toys coming to life adults will marvel at a witty script and utterly brilliant anthropomorphism'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev = rt.qt[7]\n",
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p, n ,o = agg_scorer(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children 0.0\n",
      "will 0.0208333333333\n",
      "enjoy 0.475\n",
      "a 0.0178571428571\n",
      "new 0.068181818182\n",
      "take 0.0193452380952\n",
      "on 0.0208333333333\n",
      "the 0.0\n",
      "irresistible 0.3125\n",
      "idea 0.05\n",
      "of 0.0\n",
      "toys 0.0\n",
      "coming 0.09375\n",
      "to 0.0\n",
      "life 0.0178571428571\n",
      "adults 0.0\n",
      "will 0.0208333333333\n",
      "marvel 0.34375\n",
      "at 0.0\n",
      "a 0.0178571428571\n",
      "witty 0.5\n",
      "script 0.0\n",
      "and 0.0\n",
      "utterly 0.5\n",
      "brilliant 0.4375\n",
      "anthropomorphism 0.0\n"
     ]
    }
   ],
   "source": [
    "for word, p_ in zip(rev.split(),p):\n",
    "    print word, p_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Calculate the sum and average ratings for positive, negative, and objective for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_scores = map(agg_scorer,rt.qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt['pos_avg'] = [np.mean(x[0]) for x in agg_scores]\n",
    "rt['neg_avg'] = [np.mean(x[1]) for x in agg_scores]\n",
    "rt['obj_avg'] = [np.mean(x[2]) for x in agg_scores]\n",
    "\n",
    "rt['pos_sum'] = [np.sum(x[0]) for x in agg_scores]\n",
    "rt['neg_sum'] = [np.sum(x[1]) for x in agg_scores]\n",
    "rt['obj_sum'] = [np.sum(x[2]) for x in agg_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "      <th>quote_len</th>\n",
       "      <th>qt</th>\n",
       "      <th>pos_avg</th>\n",
       "      <th>neg_avg</th>\n",
       "      <th>obj_avg</th>\n",
       "      <th>pos_sum</th>\n",
       "      <th>neg_sum</th>\n",
       "      <th>obj_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>1</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>24</td>\n",
       "      <td>so ingenious in concept design and execution t...</td>\n",
       "      <td>0.045647</td>\n",
       "      <td>0.024706</td>\n",
       "      <td>0.929647</td>\n",
       "      <td>1.095524</td>\n",
       "      <td>0.592949</td>\n",
       "      <td>22.311527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>1</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>13</td>\n",
       "      <td>a winning animated feature that has something ...</td>\n",
       "      <td>0.062271</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.915751</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>11.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>1</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>17</td>\n",
       "      <td>the film sports a provocative and appealing st...</td>\n",
       "      <td>0.057831</td>\n",
       "      <td>0.024271</td>\n",
       "      <td>0.917897</td>\n",
       "      <td>0.983135</td>\n",
       "      <td>0.412608</td>\n",
       "      <td>15.604257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>1</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>14</td>\n",
       "      <td>an entertaining computer-generated hyperrealis...</td>\n",
       "      <td>0.072688</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.884982</td>\n",
       "      <td>0.944940</td>\n",
       "      <td>0.550298</td>\n",
       "      <td>11.504762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Michael Booth</td>\n",
       "      <td>1</td>\n",
       "      <td>114709.0</td>\n",
       "      <td>Denver Post</td>\n",
       "      <td>As Lion King did before it, Toy Story revived ...</td>\n",
       "      <td>2007-05-03</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>Toy story</td>\n",
       "      <td>40</td>\n",
       "      <td>as lion king did before it toy story revived t...</td>\n",
       "      <td>0.028408</td>\n",
       "      <td>0.021935</td>\n",
       "      <td>0.949657</td>\n",
       "      <td>1.136316</td>\n",
       "      <td>0.877397</td>\n",
       "      <td>37.986287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh      imdb     publication  \\\n",
       "0         Derek Adams      1  114709.0        Time Out   \n",
       "2         David Ansen      1  114709.0        Newsweek   \n",
       "3       Leonard Klady      1  114709.0         Variety   \n",
       "4  Jonathan Rosenbaum      1  114709.0  Chicago Reader   \n",
       "5       Michael Booth      1  114709.0     Denver Post   \n",
       "\n",
       "                                               quote review_date    rtid  \\\n",
       "0  So ingenious in concept, design and execution ...  2009-10-04  9559.0   \n",
       "2  A winning animated feature that has something ...  2008-08-18  9559.0   \n",
       "3  The film sports a provocative and appealing st...  2008-06-09  9559.0   \n",
       "4  An entertaining computer-generated, hyperreali...  2008-03-10  9559.0   \n",
       "5  As Lion King did before it, Toy Story revived ...  2007-05-03  9559.0   \n",
       "\n",
       "       title  quote_len                                                 qt  \\\n",
       "0  Toy story         24  so ingenious in concept design and execution t...   \n",
       "2  Toy story         13  a winning animated feature that has something ...   \n",
       "3  Toy story         17  the film sports a provocative and appealing st...   \n",
       "4  Toy story         14  an entertaining computer-generated hyperrealis...   \n",
       "5  Toy story         40  as lion king did before it toy story revived t...   \n",
       "\n",
       "    pos_avg   neg_avg   obj_avg   pos_sum   neg_sum    obj_sum  \n",
       "0  0.045647  0.024706  0.929647  1.095524  0.592949  22.311527  \n",
       "2  0.062271  0.021978  0.915751  0.809524  0.285714  11.904762  \n",
       "3  0.057831  0.024271  0.917897  0.983135  0.412608  15.604257  \n",
       "4  0.072688  0.042331  0.884982  0.944940  0.550298  11.504762  \n",
       "5  0.028408  0.021935  0.949657  1.136316  0.877397  37.986287  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate predictive ability using the sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.624431608768 0.615069103879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "X = rt[['pos_avg','neg_avg','obj_avg','quote_len']]\n",
    "y = rt.fresh.values\n",
    "\n",
    "lr_scores = cross_val_score(LogisticRegression(), X, y, cv=10)\n",
    "print np.mean(lr_scores), np.mean(y)\n",
    "lr = LogisticRegression().fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_avg 9.0934243294\n",
      "neg_avg -7.52380496811\n",
      "obj_avg -0.776342845174\n",
      "quote_len 0.011594988458\n"
     ]
    }
   ],
   "source": [
    "for predictor, coef in zip(X.columns, lr.coef_[0]):\n",
    "    print predictor, coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp = pd.DataFrame({\n",
    "        'prob_fresh': lr.predict_proba(X)[:,1]\n",
    "        ,'prob_rotten': lr.predict_proba(X)[:,0]\n",
    "        ,'quote':rt.quote.values\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_fresh</th>\n",
       "      <th>prob_rotten</th>\n",
       "      <th>quote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>0.893040</td>\n",
       "      <td>0.106960</td>\n",
       "      <td>Appropriately operatic, Chen's visually specta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9701</th>\n",
       "      <td>0.875425</td>\n",
       "      <td>0.124575</td>\n",
       "      <td>From Russia with Love is a preposterous, skill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7889</th>\n",
       "      <td>0.859373</td>\n",
       "      <td>0.140627</td>\n",
       "      <td>A very good film with some dazzling moments an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prob_fresh  prob_rotten  \\\n",
       "1837    0.893040     0.106960   \n",
       "9701    0.875425     0.124575   \n",
       "7889    0.859373     0.140627   \n",
       "\n",
       "                                                  quote  \n",
       "1837  Appropriately operatic, Chen's visually specta...  \n",
       "9701  From Russia with Love is a preposterous, skill...  \n",
       "7889  A very good film with some dazzling moments an...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appropriately operatic, Chen's visually spectacular epic is sumptuous in every respect. Intelligent, enthralling, rhapsodic.\n",
      "============================================================\n",
      "From Russia with Love is a preposterous, skillful slab of hardhitting, sexy hokum.\n",
      "============================================================\n",
      "A very good film with some dazzling moments and one truly outstanding performance!\n",
      "============================================================\n",
      "Remains a beautiful, deftly directed and superbly acted version of a witty and poignant drama.\n",
      "============================================================\n",
      "An ingenious script, excellent special effects and photography, and superior acting, make it an endearing winner.\n",
      "============================================================\n",
      "Part homage, part spoof, the deft balancing act is a clever, engaging adaption.\n",
      "============================================================\n",
      "The Karate Kid exhibits warmth and friendly, predictable humor, its greatest assets.\n",
      "============================================================\n",
      "Improbabilities and all, Simpatico still boasts wonderful scenes and a cast that is truly superb.\n",
      "============================================================\n",
      "An inspiring translation of biblical grandeur, turning the story of one of history's greatest heroes into an entertaining, visually dazzling cartoon.\n",
      "============================================================\n",
      "High Noon combines its points about good citizenship with some excellent picturemaking.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "pp.sort_values('prob_fresh', ascending=False, inplace = True)\n",
    "for x in pp.quote.values[0:10]:\n",
    "    print x\n",
    "    print '='*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_fresh</th>\n",
       "      <th>prob_rotten</th>\n",
       "      <th>quote</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>0.893040</td>\n",
       "      <td>0.106960</td>\n",
       "      <td>Appropriately operatic, Chen's visually specta...</td>\n",
       "      <td>0.786080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9701</th>\n",
       "      <td>0.875425</td>\n",
       "      <td>0.124575</td>\n",
       "      <td>From Russia with Love is a preposterous, skill...</td>\n",
       "      <td>0.750850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7889</th>\n",
       "      <td>0.859373</td>\n",
       "      <td>0.140627</td>\n",
       "      <td>A very good film with some dazzling moments an...</td>\n",
       "      <td>0.718746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prob_fresh  prob_rotten  \\\n",
       "1837    0.893040     0.106960   \n",
       "9701    0.875425     0.124575   \n",
       "7889    0.859373     0.140627   \n",
       "\n",
       "                                                  quote  difference  \n",
       "1837  Appropriately operatic, Chen's visually specta...    0.786080  \n",
       "9701  From Russia with Love is a preposterous, skill...    0.750850  \n",
       "7889  A very good film with some dazzling moments an...    0.718746  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp['difference'] = abs(pp.prob_fresh - pp.prob_rotten)\n",
    "pp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shambolic, deafening, intelligence-insulting mess, a crushing failure on almost all counts.\n",
      "============================================================\n",
      "It's like watching the dreckiest of teen puppy courtships trying to pass itself off as 'Annie Hall.\n",
      "============================================================\n",
      "This cockamamy action flick is excruciatingly formulaic -- brimming with spy movie cliches but devoid of the genre's fun, upper-class pretensions.\n",
      "============================================================\n",
      "The film, for all its mayhem and fury, is too distant to be truly disturbing; it treats everything with an impatient, born-too-late shrug.\n",
      "============================================================\n",
      "The story is no more than a thread stitching set pieces of increasing implausibility and ineptitude.\n",
      "============================================================\n",
      "This may work for you if you settle at the outset for a nostalgic, all-American mood piece.\n",
      "============================================================\n",
      "Notorious has a fine time along the way, with Woolard channeling the rapper's sweetness and wit as comfortably as his pathos.\n",
      "============================================================\n",
      "Never manages more than a glib, TV movie-of- the-week glance at their lives.\n",
      "============================================================\n",
      "An old hand at this sort of thing, Pakula goes through the motions, but not much more.\n",
      "============================================================\n",
      "Another soulless, by-the-numbers attempt to resurrect a genre that made money for the studios in the past.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "pp.sort_values('difference', ascending=True, inplace = True)\n",
    "for x in pp.quote.values[0:10]:\n",
    "    print x\n",
    "    print '='*60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Import spacy\n",
    "\n",
    "The spacy package is the current gold standard for parsing text. We are going to use it to find the part of speech tags for the review words. \n",
    "\n",
    "Once we have parsed the tags with spacey, we can assign sentiment scores at a more granular level, using the correct part of speech version of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = en_nlp(rt.qt.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV\n",
      "ADJ\n",
      "ADP\n",
      "NOUN\n",
      "NOUN\n",
      "CONJ\n",
      "NOUN\n",
      "ADJ\n",
      "PRON\n",
      "VERB\n",
      "VERB\n",
      "PRON\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "NOUN\n",
      "PUNCT\n",
      "ADJ\n",
      "NOUN\n",
      "CONJ\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADP\n",
      "ADJ\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "for x in txt:\n",
    "    print x.pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "so"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = txt[0]\n",
    "token1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(token1)=='so'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Parse the quotes using spacey's multithreaded parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "parsed_quotes = []\n",
    "for i, parsed in enumerate(en_nlp.pipe(rt.qt.values, batch_size=50, n_threads=4)):\n",
    "    if (i%1000)==0:\n",
    "        print i\n",
    "    parsed_quotes.append(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create columns for part of speech proportions\n",
    "\n",
    "For each of the part of speech tags, create a column in the dataset that records the proportion of words in the quote that have that part of speech tag. We can try using these as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ','ADP','ADV','CONJ','DET','INTJ','NOUN','NUM','PART','PRON','PROPN','PUNCT','SPACE','SYM','VERB','X\n"
     ]
    }
   ],
   "source": [
    "unique_pos =[]\n",
    "for parsed in parsed_quotes:\n",
    "    unique_pos.extend([t.pos_ for t in parsed])\n",
    "unique_pos = np.unique(unique_pos)\n",
    "print \"','\".join(unique_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_grammar = 'ADJ','ADP','ADV','CONJ','DET','INTJ','NOUN','NUM','PART','PRON','PROPN','PUNCT','SPACE','SYM','VERB'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pos in useful_grammar:\n",
    "    rt[pos+'_prop'] =0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n"
     ]
    }
   ],
   "source": [
    "rt = rt.reset_index(drop=True)\n",
    "for i, parsed in enumerate(parsed_quotes):\n",
    "    if (i%500) ==0:\n",
    "        print i\n",
    "    parsed_len = len(parsed)\n",
    "    for pos in useful_grammar:\n",
    "        prop = len([x for x in parsed if x.pos_ == pos]) / float(parsed_len)\n",
    "        rt.ix[i,pos+'_prop'] = prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate a model with the new part of speech predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Print out the most likely fresh and most likely rotten reviews\n",
    "\n",
    "Using the predicted probabilities from our model, we can see which reviews are most likely to be fresh or rotten. We can easily validate that our model is doing something that makes sense by looking at these (one of the benefits of doing NLP work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Assign sentiment scores using the correct part of speech tag\n",
    "\n",
    "We need to write another function that will take into account the part of speech tags using the parsed quotes we created earlier and the original sentiment data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scorer(parsed):\n",
    "    pos_scores, neg_scores, obj_scores =[],[],[]\n",
    "    for token in [t for t in parsed if t.pos_ in ['NOUN','VERB','ADV','ADJ']]:\n",
    "        try:\n",
    "            pos_scores.append(sen_dict[token.pos_][str(token)]['pos_score'])\n",
    "            neg_scores.append(sen_dict[token.pos_][str(token)]['pos_score'])\n",
    "            obj_scores.append(1. - (pos_scores[-1] + obj_scores[-1]))\n",
    "        except:\n",
    "            pos_scores.append(0.)\n",
    "            neg_scores.append(0.)\n",
    "            obj_scores.append(0.)\n",
    "    return [pos_scores, neg_scores, obj_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = map(scorer, parsed_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rt['pos_part_avg'] = [np.mean(x[0]) for x in scores]\n",
    "rt['neg_part_avg'] = [np.mean(x[1]) for x in scores]\n",
    "rt['obj_part_avg'] = [np.mean(x[2]) for x in scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_avg</th>\n",
       "      <th>neg_avg</th>\n",
       "      <th>obj_avg</th>\n",
       "      <th>pos_part_avg</th>\n",
       "      <th>neg_part_avg</th>\n",
       "      <th>obj_part_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045647</td>\n",
       "      <td>0.024706</td>\n",
       "      <td>0.929647</td>\n",
       "      <td>0.065343</td>\n",
       "      <td>0.065343</td>\n",
       "      <td>0.388009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062271</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.915751</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.057831</td>\n",
       "      <td>0.024271</td>\n",
       "      <td>0.917897</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.306818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.072688</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.884982</td>\n",
       "      <td>0.065972</td>\n",
       "      <td>0.065972</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028408</td>\n",
       "      <td>0.021935</td>\n",
       "      <td>0.949657</td>\n",
       "      <td>0.036564</td>\n",
       "      <td>0.036564</td>\n",
       "      <td>0.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.119091</td>\n",
       "      <td>0.045225</td>\n",
       "      <td>0.835684</td>\n",
       "      <td>0.097178</td>\n",
       "      <td>0.097178</td>\n",
       "      <td>0.368575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.112158</td>\n",
       "      <td>0.028341</td>\n",
       "      <td>0.859501</td>\n",
       "      <td>0.140578</td>\n",
       "      <td>0.140578</td>\n",
       "      <td>0.356581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.055437</td>\n",
       "      <td>0.019369</td>\n",
       "      <td>0.925193</td>\n",
       "      <td>0.067751</td>\n",
       "      <td>0.067751</td>\n",
       "      <td>0.321453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.025202</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>0.962352</td>\n",
       "      <td>0.082465</td>\n",
       "      <td>0.082465</td>\n",
       "      <td>0.394965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.097777</td>\n",
       "      <td>0.011065</td>\n",
       "      <td>0.891158</td>\n",
       "      <td>0.163904</td>\n",
       "      <td>0.163904</td>\n",
       "      <td>0.396875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pos_avg   neg_avg   obj_avg  pos_part_avg  neg_part_avg  obj_part_avg\n",
       "0  0.045647  0.024706  0.929647      0.065343      0.065343      0.388009\n",
       "1  0.062271  0.021978  0.915751      0.020833      0.020833      0.222222\n",
       "2  0.057831  0.024271  0.917897      0.078125      0.078125      0.306818\n",
       "3  0.072688  0.042331  0.884982      0.065972      0.065972      0.363636\n",
       "4  0.028408  0.021935  0.949657      0.036564      0.036564      0.258700\n",
       "5  0.119091  0.045225  0.835684      0.097178      0.097178      0.368575\n",
       "6  0.112158  0.028341  0.859501      0.140578      0.140578      0.356581\n",
       "7  0.055437  0.019369  0.925193      0.067751      0.067751      0.321453\n",
       "8  0.025202  0.012446  0.962352      0.082465      0.082465      0.394965\n",
       "9  0.097777  0.011065  0.891158      0.163904      0.163904      0.396875"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt[[col for col in rt.columns if col.endswith('_avg')]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluate the new predictors with different models.\n",
    "\n",
    "Does regularization help? Decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = rt[['quote_len'] + [c for c in rt.columns if c.endswith('_avg')] + [c for c in rt.columns if c.endswith('_prop')]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote_len</th>\n",
       "      <th>pos_avg</th>\n",
       "      <th>neg_avg</th>\n",
       "      <th>obj_avg</th>\n",
       "      <th>pos_part_avg</th>\n",
       "      <th>neg_part_avg</th>\n",
       "      <th>obj_part_avg</th>\n",
       "      <th>ADJ_prop</th>\n",
       "      <th>ADP_prop</th>\n",
       "      <th>ADV_prop</th>\n",
       "      <th>...</th>\n",
       "      <th>INTJ_prop</th>\n",
       "      <th>NOUN_prop</th>\n",
       "      <th>NUM_prop</th>\n",
       "      <th>PART_prop</th>\n",
       "      <th>PRON_prop</th>\n",
       "      <th>PROPN_prop</th>\n",
       "      <th>PUNCT_prop</th>\n",
       "      <th>SPACE_prop</th>\n",
       "      <th>SYM_prop</th>\n",
       "      <th>VERB_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>0.045647</td>\n",
       "      <td>0.024706</td>\n",
       "      <td>0.929647</td>\n",
       "      <td>0.065343</td>\n",
       "      <td>0.065343</td>\n",
       "      <td>0.388009</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.062271</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.915751</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   quote_len   pos_avg   neg_avg   obj_avg  pos_part_avg  neg_part_avg  \\\n",
       "0         24  0.045647  0.024706  0.929647      0.065343      0.065343   \n",
       "1         13  0.062271  0.021978  0.915751      0.020833      0.020833   \n",
       "\n",
       "   obj_part_avg  ADJ_prop  ADP_prop  ADV_prop    ...      INTJ_prop  \\\n",
       "0      0.388009  0.153846  0.115385  0.076923    ...            0.0   \n",
       "1      0.222222  0.153846  0.153846  0.000000    ...            0.0   \n",
       "\n",
       "   NOUN_prop  NUM_prop  PART_prop  PRON_prop  PROPN_prop  PUNCT_prop  \\\n",
       "0   0.269231       0.0        0.0   0.076923         0.0    0.038462   \n",
       "1   0.384615       0.0        0.0   0.000000         0.0    0.000000   \n",
       "\n",
       "   SPACE_prop  SYM_prop  VERB_prop  \n",
       "0         0.0       0.0   0.153846  \n",
       "1         0.0       0.0   0.153846  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xn = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1500 candidates, totalling 7500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.8s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:    3.3s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:    7.4s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:   13.2s\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:   20.7s\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:   29.9s\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed:   40.5s\n",
      "[Parallel(n_jobs=1)]: Done 3199 tasks       | elapsed:   54.1s\n",
      "[Parallel(n_jobs=1)]: Done 4049 tasks       | elapsed:  1.2min\n",
      "[Parallel(n_jobs=1)]: Done 4999 tasks       | elapsed:  1.4min\n",
      "[Parallel(n_jobs=1)]: Done 6049 tasks       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=1)]: Done 7199 tasks       | elapsed:  2.2min\n",
      "[Parallel(n_jobs=1)]: Done 7500 out of 7500 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['elasticnet'], 'loss': ['log'], 'l1_ratio': array([ 0.01   ,  0.06211,  0.11421,  0.16632,  0.21842,  0.27053,\n",
       "        0.32263,  0.37474,  0.42684,  0.47895,  0.53105,  0.58316,\n",
       "        0.63526,  0.68737,  0.73947,  0.79158,  0.84368,  0.89579,\n",
       "        0.94789,  1.     ]), 'alpha': array([  1.00000e-04,   1.20526e-04, ...,   8.29696e+01,   1.00000e+02])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_params = {\n",
    "    'loss':['log']\n",
    "    , 'penalty': ['elasticnet']\n",
    "    , 'alpha':np.logspace(-4,2,75)\n",
    "    , 'l1_ratio':np.linspace(0.01,1.0,20)\n",
    "}\n",
    "\n",
    "sgd_gs = GridSearchCV(SGDClassifier(), sgd_params, cv=5, verbose=1)\n",
    "sgd_gs.fit(Xn,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64752563531\n"
     ]
    }
   ],
   "source": [
    "print sgd_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quote_len 0.0902213767003\n",
      "pos_avg 0.178256757629\n",
      "neg_avg -0.273272538607\n",
      "obj_avg 0.0\n",
      "pos_part_avg 0.160807152371\n",
      "neg_part_avg 0.160807152371\n",
      "obj_part_avg 0.0\n",
      "ADJ_prop 0.0\n",
      "ADP_prop 0.0\n",
      "ADV_prop -0.13516635068\n",
      "CONJ_prop 0.021514940718\n",
      "DET_prop -0.0241481810788\n",
      "INTJ_prop 0.0\n",
      "NOUN_prop 0.114989536061\n",
      "NUM_prop 0.0\n",
      "PART_prop -0.0603798853488\n",
      "PRON_prop 0.0\n",
      "PROPN_prop 0.0566200422861\n",
      "PUNCT_prop -0.0650161194037\n",
      "SPACE_prop 0.0\n",
      "SYM_prop 0.0\n",
      "VERB_prop -0.163907482009\n"
     ]
    }
   ],
   "source": [
    "for var, coef in zip(X.columns, sgd_gs.best_estimator_.coef_[0]):\n",
    "    print var, coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
