{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare logistic regression and kNN\n",
    "\n",
    "This is an open-ended lab.\n",
    "\n",
    "You will:\n",
    "\n",
    "1. Load in the wine dataset (create target, concatenate, normalize predictors)\n",
    "- Do EDA on predictors\n",
    "- Select predictors of interest\n",
    "- Load KNeighborsClassifier and LogisticRegression from sklearn\n",
    "- Compare performance between the two using stratified cross-validation\n",
    "- [Optional bonus] Plot the results of kNN vs. Logistic regression using the plotting functions I wrote yesterday and today. You may have to modify the functions to work for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Humor styles questions encoding reference\n",
    "\n",
    "### 32 questions:\n",
    "\n",
    "Subjects answered **32** different questions outlined below:\n",
    "\n",
    "1. I usually don't laugh or joke with other people.\n",
    "2. If I feel depressed, I can cheer myself up with humor.\n",
    "3. If someone makes a mistake, I will tease them about it.\n",
    "4. I let people laugh at me or make fun of me at my expense more than I should.\n",
    "5. I don't have to work very hard to make other people laugh. I am a naturally humorous person.\n",
    "6. Even when I'm alone, I am often amused by the absurdities of life.\n",
    "7. People are never offended or hurt by my sense of humor.\n",
    "8. I will often get carried away in putting myself down if it makes family or friends laugh.\n",
    "9. I rarely make other people laugh by telling funny stories about myself.\n",
    "10. If I am feeling upset or unhappy I usually try to think of something funny about the situation to make myself feel better.\n",
    "11. When telling jokes or saying funny things, I am usually not concerned about how other people are taking it.\n",
    "12. I often try to make people like or accept me more by saying something funny about my own weaknesses, blunders, or faults.\n",
    "13. I laugh and joke a lot with my closest friends.\n",
    "14. My humorous outlook on life keeps me from getting overly upset or depressed about things.\n",
    "15. I do not like it when people use humor as a way of criticizing or putting someone down.\n",
    "16. I don't often say funny things to put myself down.\n",
    "17. I usually don't like to tell jokes or amuse people.\n",
    "18. If I'm by myself and I'm feeling unhappy, I make an effort to think of something funny to cheer myself up.\n",
    "19. Sometimes I think of something that is so funny that I can't stop myself from saying it, even if it is not appropriate for the situation.\n",
    "20. I often go overboard in putting myself down when I am making jokes or trying to be funny.\n",
    "21. I enjoy making people laugh.\n",
    "22. If I am feeling sad or upset, I usually lose my sense of humor.\n",
    "23. I never participate in laughing at others even if all my friends are doing it.\n",
    "24. When I am with friends or family, I often seem to be the one that other people make fun of or joke about.\n",
    "25. I donít often joke around with my friends.\n",
    "26. It is my experience that thinking about some amusing aspect of a situation is often a very effective way of coping with problems.\n",
    "27. If I don't like someone, I often use humor or teasing to put them down.\n",
    "28. If I am having problems or feeling unhappy, I often cover it up by joking around, so that even my closest friends don't know how I really feel.\n",
    "29. I usually can't think of witty things to say when I'm with other people.\n",
    "30. I don't need to be with other people to feel amused. I can usually find things to laugh about even when I'm by myself.\n",
    "31. Even if something is really funny to me, I will not laugh or joke about it if someone will be offended.\n",
    "32. Letting others laugh at me is my way of keeping my friends and family in good spirits.\n",
    "\n",
    "---\n",
    "\n",
    "### Response scale:\n",
    "\n",
    "For each question, there are 5 possible response codes (\"likert scale\") that correspond to different answers. There is also a code that indicates there is no response for that subject.\n",
    "\n",
    "    1 == \"Never or very rarely true\"\n",
    "    2 == \"Rarely true\"\n",
    "    3 == \"Sometimes true\"\n",
    "    4 == \"Often true\"\n",
    "    5 == \"Very often or always true\n",
    "    [-1 == Did not select an answer]\n",
    "    \n",
    "---\n",
    "\n",
    "### Demographics:\n",
    "\n",
    "    age: entered as as text then parsed to an interger.\n",
    "    gender: chosen from drop down list (1=male, 2=female, 3=other, 0=declined)\n",
    "    accuracy: How accurate they thought their answers were on a scale from 0 to 100, answers were entered as text and parsed to an integer. They were instructed to enter a 0 if they did not want to be included in research.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Load humor styles dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hsq = pd.read_csv('/Users/tlee010/desktop/DSI-SF-2-timdavidlee/datasets/humor_styles/hsq_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "affiliative      0\n",
       "selfenhancing    0\n",
       "agressive        0\n",
       "selfdefeating    0\n",
       "accuracy         0\n",
       "clean_gender     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsq['clean_gender'] =hsq['gender'].map(lambda x: 1 if x == 1 else 0) \n",
    "hsq.head()\n",
    "\n",
    "subset = hsq[['affiliative','selfenhancing','agressive','selfdefeating','accuracy','clean_gender']]\n",
    "subset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Create a target and predictor matrix\n",
    "\n",
    "Target and predictors are up to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1071, 5) <class 'patsy.design_info.DesignMatrix'> (1071, 1) <class 'patsy.design_info.DesignMatrix'>\n",
      "1    581\n",
      "0    490\n",
      "Name: clean_gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#personality types\n",
    "#gender\n",
    "formula = 'clean_gender ~ affiliative + selfenhancing + agressive + selfdefeating + accuracy-1'\n",
    "y, X = patsy.dmatrices(formula, subset)\n",
    "print X.shape, type(X), y.shape, type(y)\n",
    "\n",
    "print subset.clean_gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Perform any EDA you deem relevant on your predictors and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Perform stratified cross-validation on a KNN classifier and logisitic regression.\n",
    "\n",
    "1. Gridsearch the best KNN parameters.\n",
    "\n",
    "Note: cross_val_score conveniently does stratification for you when you have a categorical target. :/ So much for forcing you to practice StratifiedKFold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 35, 'weights': 'uniform'}\n",
      "0.566760037348\n"
     ]
    }
   ],
   "source": [
    "knn_blank = KNeighborsClassifier()\n",
    "\n",
    "params = {\n",
    "    'n_neighbors': range(1,50)\n",
    "    ,'weights': ['uniform','distance']\n",
    "}\n",
    "\n",
    "estimator = GridSearchCV(knn_blank, params,cv = 5)\n",
    "results = estimator.fit(X,np.ravel(y))\n",
    "print results.best_params_\n",
    "print results.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.566781134536\n",
      "0.535018474245\n"
     ]
    }
   ],
   "source": [
    "knnC = KNeighborsClassifier(n_neighbors = 35, weights='uniform')\n",
    "scores = cross_val_score(knnC,X,np.ravel(y), cv=5)\n",
    "\n",
    "print scores.mean()\n",
    "\n",
    "\n",
    "knnC = KNeighborsClassifier(n_neighbors = 3, weights='uniform')\n",
    "scores = cross_val_score(knnC,X,np.ravel(y), cv=5)\n",
    "\n",
    "print scores.mean()\n",
    "#logres_model = logres.fit(X,np.ravel(y))\n",
    "#scores = cross_val_score(logres_model,X,y, cv=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Regularization with logistic regression\n",
    "\n",
    "Since logistic regression _is_ a regression, it can use the Lasso and Ridge penalties.\n",
    "\n",
    "The `penalty` keyword argument can be set to `l2` for Ridge and `l1` for Lasso. \n",
    "\n",
    "Note: you must set `solver='liblinear'` if you're going to use the Lasso penalty!\n",
    "\n",
    "**`C` is the regularization strength for LogisticRegression, but IT IS THE INVERSE OF ALPHA: 1/alpha. I don't know why they did this – it's stupid.**\n",
    "\n",
    "1. Select everything but your target to be predictors.\n",
    "- Normalize the predictors!\n",
    "- Gridsearch the LogisticRegression with regularization.\n",
    "- Gridsearch the KNN.\n",
    "- Compare their cross-validated accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q20', 'Q21', 'Q22', 'Q23', 'Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30', 'Q31', 'Q32', 'affiliative', 'selfenhancing', 'agressive', 'selfdefeating', 'age', 'accuracy']\n",
      "{'penalty': 'l2', 'C': 1e-05}\n",
      "{'n_neighbors': 42, 'weights': 'uniform'}\n",
      "0.588250380352\n",
      "0.605020647685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns = [x for x in hsq.columns.values if x not in ('gender','clean_gender')]\n",
    "print columns\n",
    "formula = 'clean_gender ~ ' + ' + '.join(columns)\n",
    "\n",
    "y, X = patsy.dmatrices(formula, hsq)\n",
    "\n",
    "#normalize the predictors\n",
    "scalar = StandardScaler()\n",
    "X_norm = scalar.fit_transform(X)\n",
    "y_norm = np.ravel(y)\n",
    "\n",
    "#grid search\n",
    "logres = LogisticRegression(solver='liblinear')\n",
    "\n",
    "params = {\n",
    "    'C': [0.0000001, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "    , 'penalty': ['l1','l2']\n",
    "}\n",
    "\n",
    "estimator = GridSearchCV(logres, params, cv=5)\n",
    "results = estimator.fit(X_norm,y_norm)\n",
    "print results.best_params_\n",
    "#{'penalty': 'l2', 'C': 1e-05}\n",
    "\n",
    "\n",
    "knn_blank = KNeighborsClassifier()\n",
    "\n",
    "params = {\n",
    "    'n_neighbors': range(1,50)\n",
    "    ,'weights': ['uniform','distance']\n",
    "}\n",
    "\n",
    "estimator = GridSearchCV(knn_blank, params, cv=5)\n",
    "results = estimator.fit(X_norm,y_norm)\n",
    "print results.best_params_\n",
    "\n",
    "#{'n_neighbors': 42, 'weights': 'uniform'}\n",
    "\n",
    "logres = LogisticRegression(solver='liblinear',penalty='l2', C=0.00001)\n",
    "scores = cross_val_score(logres,X_norm,y_norm, cv=5)\n",
    "print scores.mean()\n",
    "\n",
    "knnC = KNeighborsClassifier(n_neighbors=42, weights ='uniform')\n",
    "scores = cross_val_score(knnC,X_norm,y_norm, cv=5)\n",
    "print scores.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Explain why that regularization for logistic regression may have been chosen. Print out the most important variables for predicting your target from logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The not-strong ridge indicates to me that the variables are decently independent (but some multicollinearity) and that they are all reasonably useful for out-of-sample prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Re-run a (non-regularized) logistic regression with only centered coefficients (not normalized). Interperet the baseline probability and the effect of one of your predictors.\n",
    "\n",
    "**sklearn's LogisticRegression actually uses l2 Ridge regularization by default with `C=1`! To \"turn it off\" set `C=1e10`.**\n",
    "\n",
    "1. Fit the logistic regression using centered predictors.\n",
    "- Write a function to turn coefficient results to probability (logistic transformation).\n",
    "- Describe the baseline probability.\n",
    "- Plot the distribution of one of your predictors.\n",
    "- Describe based on the coefficient of the predictor the effect on probability of your target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the baseline probability is different than the mean of the target. This can happen! It's not wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
